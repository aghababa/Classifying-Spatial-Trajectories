# -*- coding: utf-8 -*-
"""Car-Bus_Experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TR_eH_e7PZrXbqD1EUf1FavyyVMIgDyC
"""

#pip install similaritymeasures

#pip install tslearn

#pip install frechetdist

pip install trjtrypy

"""# Libraries"""

import glob
import numpy as np 
import time
import math
import random
from scipy import linalg as LA
import pandas as pd
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.svm import NuSVC
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import Perceptron
import statsmodels.api as sm
#from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
from termcolor import colored
import matplotlib as mpl
from scipy import optimize
from sklearn.svm import LinearSVC
from termcolor import colored
from scipy.stats import entropy
import os
import ast
import csv
import json 
import scipy.io
import trjtrypy as tt
from trjtrypy.featureMappings import curve2vec
from trjtrypy.distances import d_Q_pi
#import similaritymeasures
#import tslearn
#from tslearn.metrics import dtw
#from scipy.spatial.distance import directed_hausdorff
#from frechetdist import frdist

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import sampler
import torchvision.transforms as T

#%matplotlib inline

from google.colab import drive
drive.mount("/content/gdrive")

if torch.cuda.is_available():
    device = torch.device("cuda:0")
    print("GPU")
else:
    device = torch.device("cpu")
    print("CPU")

# RAM
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""# Read data"""

def read_file(file_name):
    data = []
    with open(file_name, "r") as f:
        for line in f:
            item = line.strip().split(",")
            data.append(np.array(item))
    return np.array(data)

data1 = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_tracks.csv')[1:,:-1]
data2 = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_trackspoints.csv')[1:,1:4]

float1 = np.vectorize(float)
int1 = np.vectorize(int)
track_id = int1(data1[:,0])
labels = np.where(int1(data1[:,-1]) < 1.5, int1(data1[:,-1]), -1) 
traj = float1(data2)

sum(labels==1), sum(labels==-1)

trajec = [0] * 163

for i in range(163):
    trajec[i] = []
    I = np.where(traj[:,2] == track_id[i])
    trajec[i] = np.array([labels[i], traj[I]], dtype = 'object')

trajec = np.array(trajec)

trajectory = [0] * 163
trajectory_label_id = [0] * 163

for i in range(163):
    trajectory[i] = trajec[i][1][:,:2]
    trajectory_label_id[i] = np.array([trajec[i][1][:,:2], trajec[i][0], 
                                       trajec[i][1][:,2][0]], dtype = 'object')
    
trajectory_label_id = np.array(trajectory_label_id, dtype = 'object')
trajectory = np.array(trajectory, dtype = 'object')

min_length = 10
max_length = 1000 #160 for balance data
l = 0
index = [] 
for i in range(163):
    if len(trajectory[i]) < min_length or len(trajectory[i]) > max_length:
        l = l + 1
    else:
        index.append(i)
        
l, 163-l

trajectories = [0] * (163-l)
trajectories_label_id = [0] * (163-l)

j = 0
for i in range(163):
    if len(trajectory[i]) >= min_length and len(trajectory[i]) <= max_length:
        trajectories[j] = np.array(trajectory[i])
        trajectories_label_id[j] = trajectory_label_id[i]
        j = j + 1

trajectories_label_id = np.array(trajectories_label_id, dtype = 'object')
trajectories = np.array(trajectories, dtype = 'object')

cars = trajectories_label_id[np.where(trajectories_label_id[:,1] == 1)][:,:2][:,0]
buses = trajectories_label_id[np.where(trajectories_label_id[:,1] == -1)][:,:2][:,0]

cars_copy = cars.copy()
buses_copy = buses.copy()
len(cars), len(buses)

def remove_segments(traj): # removes stationary points
    p2 = traj[1:]
    p1 = traj[:-1]
    L = ((p2-p1)*(p2-p1)).sum(axis =1)
    I = np.where(L>1e-16)[0]
    return traj[I]

cars = np.array(list(map(remove_segments, cars)), dtype='object')
buses = np.array(list(map(remove_segments, buses)), dtype='object')

I = np.where(np.array([len(cars[i]) for i in range(len(cars))]) > 1)
J = np.where(np.array([len(buses[i]) for i in range(len(buses))]) > 1)

cars = cars[I]
buses = buses[J]

cars_copy = cars.copy()
buses_copy = buses.copy()
print("len(cars), len(buses)=", len(cars), len(buses))

"""# Removing 2 outliers"""

a = np.arange(len(cars))
I = np.where((a != 28) & (a != 29))
cars = cars[I]
buses = buses[:-1]
len(cars), len(buses)

a, c = np.min((np.min([np.min(cars[i], axis=0) for i in range(len(cars))], axis=0), 
       np.min([np.min(buses[i], axis=0) for i in range(len(buses))], axis=0)), axis=0)
  
b, d = np.max((np.max([np.max(cars[i], axis=0) for i in range(len(cars))], axis=0), 
               np.max([np.max(buses[i], axis=0) for i in range(len(buses))], axis=0)), axis=0)

m = 20
Q = np.ones((m,2))

Q[:,0] = 0.2 * (b - a ) * np.random.random_sample(m) + a - 0.01
Q[:,1] = 0.2 *(d - c ) * np.random.random_sample(m) + c - 0.02

for i in range(len(cars)):
    plt.plot(cars[i][:,0], cars[i][:,1], color = "steelblue");
for i in range(len(buses)):
    plt.plot(buses[i][:,0], buses[i][:,1], color = "r");
plt.scatter(Q[:,0], Q[:,1], color = "black")
print(colored(f'Original car-bus', 'yellow'))
plt.show()

"""## Plot data"""

for i in range(len(cars)):
    if i != 11 and i != 12:
        plt.plot(cars[i][:,0], cars[i][:,1], color='blue')
for i in range(len(buses)):
    plt.plot(buses[i][:,0], buses[i][:,1], color='red')
plt.title('Car-Bus')
plt.savefig(f'/Users/hasan/Desktop/Anaconda/Research/Pictures for 2ed paper/Car-Bus data.png', 
            bbox_inches='tight', dpi=200)
plt.show()

"""# Classifiers and get_mu function"""

CC = [100, 100, 10]
number_estimators = [50, 50]


clf0 = [make_pipeline(LinearSVC(dual=False, C=CC[0], tol=1e-5, 
                               class_weight ='balanced', max_iter=1000)), 
        "SVM, LinearSVC, C = "+str(CC[0])]
clf1 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[1], kernel='rbf', gamma='auto', max_iter=200000)),
        "Gaussian SVM, C="+str(CC[1])+", gamma=auto"]
clf2 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[2], kernel='poly', degree=3, max_iter=400000)),
        "Poly kernel SVM, C="+str(CC[2])+", deg=auto"]
clf3 = [DecisionTreeClassifier(), "Decision Tree"]
clf4 = [RandomForestClassifier(n_estimators=number_estimators[0]), 
         "RandomForestClassifier, n="+str(number_estimators[0])]
clf5 = [KNeighborsClassifier(n_neighbors=5), "KNN"]
clf6 = [LogisticRegression(solver='newton-cg'), "Logistic Regression"]

clf = [clf0, clf1, clf2, clf3, clf4, clf5, clf6]
classifs = [item[0] for item in clf]
keys = [item[1] for item in clf]

def get_mu(data_1, data_2):
    a = np.mean([np.mean(data_1[i], 0) for i in range(len(data_1))], 0)
    b = np.mean([np.mean(data_2[i], 0) for i in range(len(data_2))], 0)
    c = abs(a-b)
    return max(c)

"""# |Q| = 20 (done)

## Classification with feature mappings $v_Q$, $v_Q^{\exp}$, $v_Q^{\varsigma}$ with 20 random landmarks, and endpoints
"""

from google.colab import files
files.upload()

import v_Q_mu_endpoints_classification
from v_Q_mu_endpoints_classification import binaryClassificationAverageMajority

"""### Boost($v_Q$), Boost($v_Q^{\varsigma}$), Boost($v_Q^{\exp}$) with epoch=50 and num_trials_maj=11"""

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=20, epoch=50, 
                                    num_trials_maj=11, classifiers=clf,
                                    version='unsigned', test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=20, 
                            epoch=50, num_trials_maj=11, classifiers=clf,
                            version='signed', sigma=1, test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=20, epoch=50, 
                                num_trials_maj=11, classifiers=clf, test_size=0.3)
A = classifs.classification_v_Q_mu()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

"""### Rand $v_Q$, Rand $v_Q^{\varsigma}$, Rand $v_Q^{\exp}$ with epoch=50 and num_trials_maj=1"""

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=20, 
                                    epoch=50, num_trials_maj=1, classifiers=clf, 
                                    version='unsigned', test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=20, 
                                    epoch=50, num_trials_maj=1, classifiers=clf, 
                                    version='signed', sigma=1, test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=20, 
                    epoch=50, num_trials_maj=1, classifiers=clf, test_size=0.3)
A = classifs.classification_v_Q_mu()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=20, 
                                               epoch=50, num_trials_maj=1, 
                                               classifiers=clf, test_size=0.3)
A = classifs.endpoint_classification()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

"""# Classification with Perceptron-Like algorithm |Q|=20 (done)"""

from google.colab import files
files.upload()

import Perceptron_Like_Algo_Class
from Perceptron_Like_Algo_Class import classification

"""## Boost(MD $v_Q^{\exp}$) with epoch=50, maj_num=11 and init_iter=3 (done)"""

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT']:
    classif = classification(cars, buses, Q_size=20, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

for Model in ['RF', 'KNN', 'LR']:
    classif = classification(cars, buses, Q_size=20, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

"""## MD $v_Q^{\exp}$ with epoch=50, maj_num=1 and init_iter=3 (done)"""

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
    classif = classification(cars, buses, Q_size=20, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=1, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

"""# KNN with $d_Q^{\pi}$ (with |Q|=20) and DTW (done)

## KNN with $d_Q^{\pi}$ distance with matrix storing method  (|Q|=20)

### Calculate distance matrix
"""

def calculate_dists_d_Q_pi(data1, data2, p, path): 
    start_time = time.time() 
    data = np.concatenate((data1, data2), 0)
    n = len(data)
    A = []
    for i in range(n-1):
        for j in range(i+1, n):
            A.append(d_Q_pi(Q, data[i], data[j], p=p))
    A = np.array(A)
    tri = np.zeros((n, n))
    tri[np.triu_indices(n, 1)] = A
    for i in range(1, n):
        for j in range(i):
            tri[i][j] = tri[j][i]
    np.savetxt(path, tri, delimiter=',')

    total_time = time.time() - start_time
    return total_time

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/d_Q_pi-76-44.csv'
calculate_dists_d_Q_pi(cars, buses, p=1, path=path)

def KNN_with_dists_d_Q_pi(n_1, n_2, path_to_dists):
    '''path example: '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/d_Q_pi-76-44.csv'
       path_to_dists: the path to the corresponding distance matrix
       n_1: len(data_1)
       n_2: len(data_2)'''

    I_1, J_1, y_train_1, y_test_1 = train_test_split(np.arange(n_1), 
                                                np.ones(n_1), test_size=0.3)
    I_2, J_2, y_train_2, y_test_2 = train_test_split(np.arange(n_1, n_1+n_2), 
                                                np.ones(n_2), test_size=0.3)
    labels = np.array([1] * n_1 + [0] * n_2)
    I = np.concatenate((I_1, I_2), 0)
    np.random.shuffle(I)
    J = np.concatenate((J_1, J_2), 0)
    np.random.shuffle(J)

    dist_matrix = np.array(pd.read_csv(path_to_dists,  header=None))

    D_train = dist_matrix[I][:, I]
    D_test = dist_matrix[J][:,I]
    train_labels = labels[I]
    test_labels = labels[J]

    clf = KNeighborsClassifier(n_neighbors=5, metric='precomputed')
    
    #Train the model using the training sets
    clf.fit(D_train, list(train_labels))

    #Predict labels for train dataset
    train_pred = clf.predict(D_train)
    train_error = sum(train_labels != train_pred)/len(I)
    
    #Predict labels for test dataset
    test_pred = clf.predict(D_test)
    test_error = sum((test_labels != test_pred))/len(J)
        
    return train_error, test_error

def KNN_average_error_d_Q_pi(data1, data2, num_trials, path_to_dists):

    '''path_to_dists: the path to the corresponding distance matrix'''

    Start_time = time.time()

    train_errors = np.zeros(num_trials)
    test_errors = np.zeros(num_trials)

    for i in range(num_trials):
        train_errors[i], test_errors[i] = KNN_with_dists_d_Q_pi(len(data1), len(data2), path_to_dists)

    Dict = {}
    Dict[1] = [f"KNN with d_Q_pi", 
                    np.round(np.mean(train_errors), decimals=4), 
                    np.round(np.mean(test_errors), decimals=4), 
                    np.round(np.std(test_errors), decimals=4)]

    df = pd.DataFrame.from_dict(Dict, orient='index', columns=['Classifier',
                                'Train Error', 'Test Error', 'std'])
    print(colored(f"num_trials = {num_trials}", "blue"))
    print(colored(f'total time = {time.time() - Start_time}', 'green'))

    return (df, np.mean(train_errors), np.mean(test_errors), np.std(test_errors))

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/d_Q_pi-76-44.csv'
KNN_average_error_d_Q_pi(cars, buses, num_trials=50, path_to_dists=path)

"""## KNN with DTW from tslearn by saving matrx method

### Calculate distance matrix
"""

def calculate_dists_dtw_tslearn(data1, data2, path): 
    start_time = time.time() 
    data = np.concatenate((data1, data2), 0)
    n = len(data)
    A = []
    for i in range(n-1):
        for j in range(i+1, n):
            A.append(tslearn.metrics.dtw(data[i], data[j]))
    A = np.array(A)
    tri = np.zeros((n, n))
    tri[np.triu_indices(n, 1)] = A
    for i in range(1, n):
        for j in range(i):
            tri[i][j] = tri[j][i]
    np.savetxt(path, tri, delimiter=',')

    total_time = time.time() - start_time
    return total_time

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/dtw-tslearn-76-44.csv'
calculate_dists_dtw_tslearn(cars, buses, path=path)

def KNN_with_dists_dtw_tslearn(n_1, n_2, path_to_dists):
    '''path example: '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/sspd-76-44.csv'
       path_to_dists: the path to the corresponding distance matrix
       n_1: len(data_1)
       n_2: len(data_2)'''

    I_1, J_1, y_train_1, y_test_1 = train_test_split(np.arange(n_1), 
                                                np.ones(n_1), test_size=0.3)
    I_2, J_2, y_train_2, y_test_2 = train_test_split(np.arange(n_1, n_1+n_2), 
                                                np.ones(n_2), test_size=0.3)
    labels = np.array([1] * n_1 + [0] * n_2)
    I = np.concatenate((I_1, I_2), 0)
    np.random.shuffle(I)
    J = np.concatenate((J_1, J_2), 0)
    np.random.shuffle(J)

    dist_matrix = np.array(pd.read_csv(path_to_dists,  header=None))

    D_train = dist_matrix[I][:, I]
    D_test = dist_matrix[J][:,I]
    train_labels = labels[I]
    test_labels = labels[J]

    clf = KNeighborsClassifier(n_neighbors=5, metric='precomputed')
    
    #Train the model using the training sets
    clf.fit(D_train, list(train_labels))

    #Predict labels for train dataset
    train_pred = clf.predict(D_train)
    train_error = sum(train_labels != train_pred)/len(I)
    
    #Predict labels for test dataset
    test_pred = clf.predict(D_test)
    test_error = sum((test_labels != test_pred))/len(J)
        
    return train_error, test_error

def KNN_average_error_dtw_tslearn(data1, data2, num_trials, path_to_dists):

    '''path_to_dists: the path to the corresponding distance matrix'''

    Start_time = time.time()

    train_errors = np.zeros(num_trials)
    test_errors = np.zeros(num_trials)

    for i in range(num_trials):
        train_errors[i], test_errors[i] = KNN_with_dists_dtw_tslearn(len(data1), len(data2), path_to_dists)

    Dict = {}
    Dict[1] = [f"KNN with dtw from tslearn", 
                    np.round(np.mean(train_errors), decimals = 4), 
                    np.round(np.mean(test_errors), decimals = 4), 
                    np.round(np.std(test_errors), decimals = 4)]

    df = pd.DataFrame.from_dict(Dict, orient='index', columns=['Classifier',
                                'Train Error', 'Test Error', 'std'])
    print(colored(f"num_trials = {num_trials}", "blue"))
    print(colored(f'total time = {time.time() - Start_time}', 'green'))

    return (df, np.mean(train_errors), np.mean(test_errors), np.std(test_errors))

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/dtw-tslearn-76-44.csv'

F = KNN_average_error_dtw_tslearn(cars, buses, num_trials=50, path_to_dists=path)
F[0]

"""# Using physical and geometric features together (done)

## Length, speed, acceleration and jerk functions
"""

# average speed from data
#A = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_tracks.csv')[1:,:]
A = read_file('GPS Trajectory/go_track_tracks.csv')[1:,:]

I = np.where(np.array(list(map(int, A[1:,-2]))) == 1)[0]
print(np.mean(list(map(float, A[I][:,2]))))

J = np.where(np.array(list(map(int, A[1:,-2]))) == 2)[0]
np.mean(list(map(float, A[J][:,2])))

# length calculator 
def length(x):
    p1 = x[:,:2][:-1]
    p2 = x[:,:2][1:]
    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))
    Length = [sum(L)/len(L)]
    return Length

# speed calculator 
def speed(x):
    t = x[:,2][1:] - x[:,2][:-1] + 1e-10
    p1 = x[:,:2][:-1]
    p2 = x[:,:2][1:]
    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))
    speeds = L/t
    s = [sum(speeds)/len(speeds)]
    return s

# acceleration calculator
def acceleration(x):
    t = x[:,2][1:] - x[:,2][:-1] + 1e-10
    p1 = x[:,:2][:-1]
    p2 = x[:,:2][1:]
    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))
    accelerations = L/t**2
    a = [1e-5 * sum(accelerations)/len(accelerations)]
    return a

# jerk calculator 
def jerk(x):
    t = x[:,2][1:] - x[:,2][:-1] + 1e-10
    p1 = x[:,:2][:-1]
    p2 = x[:,:2][1:]
    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))
    jerks = L/t**3
    a = [1e-5 * sum(jerks)/len(jerks)]
    return a

def train_test(data1, data2, test_size):
    
    n_1 = len(data1)
    n_2 = len(data2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 = \
            train_test_split(np.arange(n_1), np.ones(n_1), test_size=test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 = \
    train_test_split(np.arange(n_2), -np.ones(n_2), test_size=test_size)
    
    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
            train_idx_2, test_idx_2, train_label_2, test_label_2

"""## Prepairing data to get physical features """

# create 'daysDate' function to convert start and end time to a float number of days
from datetime import datetime
def days_date(time_str):
    date_format = "%Y-%m-%d %H:%M:%S"
    current = datetime.strptime(time_str, date_format)
    date_format = "%Y-%m-%d"
    bench = datetime.strptime('2006-12-30', date_format)
    no_days = current - bench
    delta_time_days = no_days.days + current.hour / 24.0 + current.minute / (24. * 60.) + current.second / (24. * 3600.)
    return delta_time_days

days_date = np.vectorize(days_date)
float1 = np.vectorize(float)

#A = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_tracks.csv')#[1:,:]
A = read_file('GPS Trajectory/go_track_tracks.csv')

print(A[:2])
#traj1 = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_trackspoints.csv')[1:,1:5]
traj1 = read_file('GPS Trajectory/go_track_trackspoints.csv')[1:,1:5]
traj1[:, -1] = [traj1[:, -1][i][1:-1] for i in range(len(traj1))]
traj1[:, -1] = days_date(traj1[:, -1])
traj1 = float1(traj1)
traj1[:2]
# B is the data with time in last dimension

trajec1 = [0] * 163

for i in range(163):
    trajec1[i] = []
    I = np.where(traj[:,2] == track_id[i])
    trajec1[i] = np.array([labels[i], traj1[:,[0,1,3]][I]], dtype='object')

trajec1 = np.array(trajec1)

trajectory1 = [0] * 163
trajectory_label_id_1 = [0] * 163

for i in range(163):
    trajectory1[i] = trajec1[i][1]
    trajectory_label_id_1[i] = np.array([trajec1[i][1], trajec1[i][0], 
                                         trajec1[i][1][0]], dtype='object')

trajectory_label_id_1 = np.array(trajectory_label_id_1, dtype='object')
trajectory1 = np.array(trajectory1, dtype='object')

min_length = 10
max_length = 1000 #160 for balance data
l = 0
index = [] 
for i in range(163):
    if len(trajectory1[i]) < min_length or len(trajectory1[i]) > max_length:
        l = l + 1
    else:
        index.append(i)
        
l, 163-l

trajectories1 = [0] * (163-l)
trajectories_label_id_1 = [0] * (163-l)

j = 0
for i in range(163):
    if len(trajectory1[i]) >= min_length and len(trajectory1[i]) <= max_length:
        trajectories1[j] = np.array(trajectory1[i])
        trajectories_label_id_1[j] = trajectory_label_id_1[i]
        j = j + 1

trajectories_label_id_1 = np.array(trajectories_label_id_1, dtype='object')
trajectories1 = np.array(trajectories1, dtype='object')

cars_time = trajectories_label_id_1[np.where(trajectories_label_id_1[:,1] == 1)][:,:2][:,0]
buses_time = trajectories_label_id_1[np.where(trajectories_label_id_1[:,1] == -1)][:,:2][:,0]
cars_time_copy = cars_time.copy()
buses_time_copy = buses_time.copy()
len(cars_time), len(buses_time)

a = np.arange(len(cars_time))
I = np.where((a != 28) & (a != 29))
cars_time = cars_time[I]
buses_time = buses_time[:-1]
len(cars_time), len(buses_time)

def remove_segments_time(traj): # removes stationary points
    p2 = traj[:,:2][1:]
    p1 = traj[:,:2][:-1]
    L = ((p2-p1)*(p2-p1)).sum(axis =1)
    I = np.where(L>1e-16)[0]
    return traj[I]

cars_time = np.array(list(map(remove_segments, cars_time)), dtype='object')
buses_time = np.array(list(map(remove_segments, buses_time)), dtype='object')

cars_time_copy = cars_time.copy()
buses_time_copy = buses_time.copy()
print("len(cars), len(buses)=", len(cars_time), len(buses_time))

"""## Classification with only physical features"""

def get_features(data_1, data_2, Leng=True, spd=True, accn=True, jrk=True):

    data = np.concatenate((data_1, data_2), 0)
    n = len(data)
    data_traj_1 = np.array([data_1[i][:,:2] for i in range(len(data_1))])
    data_traj_2 = np.array([data_2[i][:,:2] for i in range(len(data_2))])
    data_traj = np.array([data[i][:,:2] for i in range(n)])
    data_labels = np.array([1] * len(data_1) + [-1] * len(data_2))
    A = data_labels.copy().reshape(-1,1)

    if Leng == True:
        length_ = np.array([length(data_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(data[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(data[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(data[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A[:, 1:]

def classification_physical_features(data_1, data_2, epoch, classifiers, 
                                       Leng=True, spd=True, accn=True, jrk=True, 
                                       test_size=0.3):
    start_time = time.time()
    models = [item[0] for item in classifiers]
    keys = [item[1] for item in classifiers]
    r = len(classifiers)

    train_error_mean = np.ones(r)
    test_error_mean = np.ones(r)
    test_error_std = np.ones(r)
    
    train_errors = np.ones((r, epoch)) 
    test_errors = np.ones((r, epoch))

    data = get_features(data_1, data_2, Leng=Leng, spd=spd, accn=accn, jrk=jrk)
    data1 = data[:len(data_1)]
    data2 = data[len(data_1):]

    for s in range(epoch):
        train_idx_1, test_idx_1, train_labels_1, test_labels_1, \
            train_idx_2, test_idx_2, train_labels_2, test_labels_2 \
                = train_test(data1, data2, test_size=test_size)

        train1 = data1[train_idx_1]
        train2 = data2[train_idx_2]
        test1 = data1[test_idx_1]
        test2 = data2[test_idx_2]

        I = np.arange(len(train1)+len(train2))
        np.random.shuffle(I)
        J = np.arange(len(test1)+len(test2))
        np.random.shuffle(J)

        train = np.concatenate((train1, train2), 0)[I]
        test = np.concatenate((test1, test2), 0)[J]
        train_labels = np.concatenate((train_labels_1, train_labels_2), 0)[I]
        test_labels = np.concatenate((test_labels_1, test_labels_2), 0)[J]

        x_preds = np.zeros((r, len(train)))
        y_preds = np.zeros((r, len(test)))

        for k in range(r): 
            Model = models[k]
            Model.fit(train, train_labels)
            x_preds[k] = Model.predict(train)                
            y_preds[k] = Model.predict(test)

        for k in range(r):
            train_errors[k][s] = sum(train_labels != x_preds[k])/len(train_labels)
            test_errors[k][s] = sum(test_labels != y_preds[k])/len(test_labels)

    for k in range(r):
        train_error_mean[k] = np.mean(train_errors[k])
        test_error_mean[k] = np.mean(test_errors[k])
        test_error_std[k] = np.std(test_errors[k])

    Dict = {}

    for k in range(len(keys)): 
        Dict[k+1] = [keys[k], np.round(train_error_mean[k], decimals=4), 
                     np.round(test_error_mean[k], decimals=4),
                     np.round(test_error_std[k], decimals=4)]

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                columns=['Classifier','Train Error', 'Test Error', 'Std Error'])
    
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf, train_error_mean, test_error_mean

classification_physical_features(cars_time, buses_time, epoch=50, classifiers=clf, 
                                 Leng=True, spd=True, accn=True, jrk=True, 
                                 test_size=0.3)[0]

"""## Classification with $v_Q^+$ which is random Q and physical features"""

def get_features_Q(train_1, train_2, version='unsigned', sigma=1, Q_size=20, 
                   Leng=True, spd=True, accn=True, jrk=True):

    train = np.concatenate((train_1, train_2), 0)
    n = len(train)
    train_traj_1 = np.array([train_1[i][:,:2] for i in range(len(train_1))])
    train_traj_2 = np.array([train_2[i][:,:2] for i in range(len(train_2))])
    train_traj = np.array([train[i][:,:2] for i in range(n)])

    Mean = np.mean([np.mean(train_traj[i], 0) for i in range(n)], 0)
    Std = np.std([np.std(train_traj[i], 0) for i in range(n)], 0)
    Q = np.ones((Q_size,2))
    Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], Q_size)
    Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], Q_size)

    A = np.array(curve2vec(Q, train_traj, version=version, sigma=sigma))
    
    if Leng == True:
        length_ = np.array([length(train_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A, Q

def get_test_data_Q(test_1, test_2, Q, version='unsigned', sigma=1, Leng=True, 
                    spd=True, accn=True, jrk=True):

    test = np.concatenate((test_1, test_2), 0)
    n = len(test)
    test_traj_1 = np.array([test_1[i][:,:2] for i in range(len(test_1))])
    test_traj_2 = np.array([test_2[i][:,:2] for i in range(len(test_2))])
    test_traj = np.array([test[i][:,:2] for i in range(n)])

    A = np.array(curve2vec(Q, test_traj, version=version, sigma=sigma))
    
    if Leng == True:
        length_ = np.array([length(test_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A

def classification_Q_physics(data1, data2, version='unsigned', sigma=1, Q_size=20, 
                     Leng=True, spd=True, accn=True, jrk=True, num_trials=10, 
                     classifiers=clf):

    Start_time = time.time()
    models = [item[0] for item in classifiers]
    keys = [item[1] for item in classifiers]
    r = len(classifiers)
    train_error_mean = np.zeros(r)
    test_error_mean = np.zeros(r)
    test_error_std = np.zeros(r)
    train_error_list = np.zeros((r, num_trials)) 
    test_error_list = np.zeros((r, num_trials))

    for s in range(num_trials):

        train_idx_1, test_idx_1, train_labels_1, test_labels_1, \
            train_idx_2, test_idx_2, train_labels_2, test_labels_2 \
            = train_test(data1, data2, test_size=0.3)

        train1 = data1[train_idx_1]
        train2 = data2[train_idx_2]
        test1 = data1[test_idx_1]
        test2 = data2[test_idx_2]

        train, Q = get_features_Q(train1, train2, version, sigma,
                                  Q_size, Leng, spd, accn, jrk)
        #train = (train-np.mean(train,0))/np.std(train,0)
        train_labels = np.concatenate((train_labels_1, train_labels_2), 0)

        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        train_labels = train_labels[I]

        J = np.arange(len(test1)+len(test2))
        np.random.shuffle(J)

        test = get_test_data_Q(test1, test2, Q, version=version, sigma=sigma, 
                               Leng=Leng, spd=spd, accn=accn, jrk=jrk)[J]
        test_labels = np.concatenate((test_labels_1, test_labels_2), 0)[J]
        #test = (test-np.mean(test,0))/np.std(test,0)

        for k in range(r):            
            model = models[k]

            #Train the model using the training sets
            model.fit(train, train_labels)

            #Predict train labels
            train_pred = model.predict(train)
            err = sum(train_labels != train_pred)/len(train_labels)
            train_error_list[k][s] = err
            
            #Predict test labels
            test_pred = model.predict(test)
            er = sum(test_labels != test_pred)/len(test_labels)
            test_error_list[k][s] = er
            
    for k in range(r):
        train_error_mean[k] = np.mean(train_error_list[k])
        test_error_mean[k] = np.mean(test_error_list[k])
        test_error_std[k] = np.std(test_error_list[k])
    
    Dic = {}

    for k in range(len(keys)): 
        Dic[k] = [keys[k], np.round(train_error_mean[k], decimals = 4), 
                    np.round(test_error_mean[k], decimals = 4),
                    np.round(test_error_std[k], decimals = 4)]

    pdf = pd.DataFrame.from_dict(Dic, orient='index', columns=['Classifier','Train Error', 
                                                               'Test Error', 'Std Error'])
    print(colored(f"Total time: {time.time() - Start_time}", 'red'))
    return pdf

"""### With only adding length"""

classification_Q_physics(cars_time, buses_time, version='unsigned',  
                        Q_size=20, Leng=True, spd=False, accn=False, jrk=False, 
                        num_trials=50, classifiers=clf)

#
classification_Q_physics(cars_time, buses_time, version='signed', sigma=1, 
                        Q_size=20, Leng=True, spd=False, accn=False, jrk=False, 
                        num_trials=50, classifiers=clf)

"""### With adding all physical pheatures

#### $v_Q^+$
"""

classification_Q_physics(cars_time, buses_time, version='unsigned',  
                         Q_size=20, Leng=True, spd=True, accn=True, jrk=True, 
                         num_trials=50, classifiers=clf)

"""#### $v_Q^{\varsigma +}$"""

classification_Q_physics(cars_time, buses_time, version='signed', sigma=1, 
                        Q_size=20, Leng=True, spd=True, accn=True, jrk=True, 
                        num_trials=50, classifiers=clf)

"""## Using Perceptron-like algorithm with physical features"""

import trjtrypy as tt
from trjtrypy.featureMappings import curve2vec

def ExpCurve2Vec(points,curves,mu):
    D=tt.distsbase.DistsBase()
    return [np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves]

def initialize_Q(train_1, train_2, C, gamma, Q_size, mu_coeff, model, Leng=True,
                 spd=True, accn=True, jrk=True, n_estimators=50):
    #Start_time = time.time()
    train = np.concatenate((train_1, train_2), 0)
    n = len(train)
    train_traj_1 = np.array([train_1[i][:,:2] for i in range(len(train_1))])
    train_traj_2 = np.array([train_2[i][:,:2] for i in range(len(train_2))])
    train_traj = np.array([train[i][:,:2] for i in range(n)])
    train_labels = np.array([1] * len(train_1) + [-1] * len(train_2))
    
    A = train_labels.copy().reshape(-1,1)
    if Leng == True:
        length_ = np.array([length(train_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    #print((A-np.mean(A,0))/np.std(A,0))

# Make the classifier
    if model == 'LSVM':
        Model = make_pipeline(LinearSVC(dual=False, C=C, tol=1e-5, 
                                        class_weight='balanced', max_iter=1000))
    elif model == 'GSVM':
        Model = make_pipeline(StandardScaler(), svm.SVC(C=C, kernel='rbf', 
                                                gamma=gamma, max_iter=200000))
    elif model == 'PSVM':
        Model = make_pipeline(StandardScaler(), svm.SVC(C=C, kernel='poly', 
                                                        degree=3, max_iter=400000))
    elif model == 'DT':
        Model = DecisionTreeClassifier()
    elif model == 'RF':
        Model = RandomForestClassifier(n_estimators=n_estimators)
    elif model == 'KNN':
        Model = KNeighborsClassifier(n_neighbors=5)
    elif model == 'LR':
        Model = LogisticRegression(solver='newton-cg')
    elif model == 'Prn':
        Model = Perceptron(tol=1e-5, validation_fraction=0.01, class_weight="balanced")
    else:
        print("model is not supported")
        
    mu = get_mu(train_traj_1, train_traj_2) * mu_coeff
    std = mu/2
    errors = []
    Q = []

    if sum([Leng, spd, accn, jrk]) > 0:
        Train_0 = A[:,1:]
        #Train_0 = (A[:,1:] - np.mean(A[:,1:] , 0))/np.std(A[:,1:],0)
        Model.fit(Train_0, train_labels)
        train_pred = Model.predict(Train_0) 
        error = 1 - metrics.accuracy_score(train_labels, train_pred)
        errors.append(error) 
        if model in ['LSVM', 'GSVM', 'PSVM', 'LR', 'Prn']:
            index = np.argmax(np.max(Model.decision_function(Train_0)))
        else:
            probs = Model.predict_proba(Train_0)
            index = np.argmax(entropy(probs, axis=1))
    else:
        index = np.random.randint(0, high=n) 
    k = np.random.randint(0, high=len(train_traj[index]))
    q = train_traj[index][k] + np.random.normal(0, std, 2)
    Q.append(q)

# Iteratively choose landmarks
    for i in range(1, Q_size):
        if sum([Leng, spd, accn, jrk]) > 0:
            Train_0 = A[:,1:]
            Train = np.concatenate((Train_0, np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))), 1)
        else:
            Train = np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))

        Model.fit(Train, train_labels)
        train_pred = Model.predict(Train)
        error = 1 - metrics.accuracy_score(train_labels, train_pred)
        errors.append(error)
        if model in ['LSVM', 'GSVM', 'PSVM', 'LR', 'Prn']:
            index = np.argmax(np.max(Model.decision_function(Train)))
        else:
            probs = Model.predict_proba(Train)
            index = np.argmax(entropy(probs, axis=1))

        k = np.random.randint(0, high=len(train_traj[index]))
        q = train_traj[index][k] + np.random.normal(0, std, 2)
        Q.append(q)
    if sum([Leng, spd, accn, jrk]) > 0:
        Train = np.concatenate((Train_0, np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))), 1)
    else: 
        Train = ExpCurve2Vec(np.array(Q), train_traj, mu)
    Model.fit(Train, train_labels)
    train_pred = Model.predict(Train)

    error = 1 - metrics.accuracy_score(train_labels, train_pred)
    errors.append(error)

    #print(colored(f"Total time for mapping row data: {time.time() - Start_time}", 'green'))

    return np.array(Q), mu, np.array(errors), error

def train_test(data1, data2, test_size):
    
    n_1 = len(data1)
    n_2 = len(data2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 = \
            train_test_split(np.arange(n_1), np.ones(n_1), test_size=test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 = \
    train_test_split(np.arange(n_2), -np.ones(n_2), test_size=test_size)
    
    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
            train_idx_2, test_idx_2, train_label_2, test_label_2

def classification_init_Q(data1, data2, C, gamma, Q_size, mu_coeff, model, epoch, 
                          init_iter, classifiers, Leng=True, spd=True, accn=True, 
                          jrk=True, test_size=0.3, normal=False, n_estimators=50):

    start_time = time.time()
    models = [item[0] for item in classifiers]
    keys = [item[1] for item in classifiers]

    r = len(classifiers)

    train_error_mean = np.zeros(r)
    test_error_mean = np.zeros(r)
    test_error_std = np.zeros(r)
    
    train_errors = np.zeros((r, epoch)) 
    test_errors = np.zeros((r, epoch))

    data = np.concatenate((data1, data2), 0)
    n1 = len(data1) 
    n2 = len(data2) 
    n = len(data) 
    data_traj1 = np.array([data1[i][:,:2] for i in range(len(data1))])
    data_traj2 = np.array([data2[i][:,:2] for i in range(len(data2))])
    data_traj = np.array([data[i][:,:2] for i in range(n)])
    labels = np.array([1] * len(data1) + [-1] * len(data2))
    #length1 = np.array([length(data_traj1[i]) for i in range(n1)])
    #length2 = np.array([length(data_traj2[i]) for i in range(n2)])

    for s in range(epoch):
        train_idx_1, test_idx_1, train_label_1, test_label_1, \
            train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test(data1, data2, test_size)
        
        train1 = data1[train_idx_1]
        train2 = data2[train_idx_2]
        train_traj1 = data_traj1[train_idx_1]
        train_traj2 = data_traj2[train_idx_2]
        train_labels1 = np.ones(len(train1))
        train_labels2 = np.zeros(len(train2))

        test1 = data1[test_idx_1]
        test2 = data2[test_idx_2]
        test_traj1 = data_traj1[test_idx_1]
        test_traj2 = data_traj2[test_idx_2]
        test_labels1 = np.ones(len(test1))
        test_labels2 = np.zeros(len(test2))

        train = np.concatenate((train1, train2), 0)
        test = np.concatenate((test1, test2), 0)
        train_traj = np.concatenate((train_traj1, train_traj2), 0)
        test_traj = np.concatenate((test_traj1, test_traj2), 0)
        train_labels = np.concatenate((train_labels1, train_labels2), 0)
        test_labels = np.concatenate((test_labels1, test_labels2), 0)

        x_preds = np.zeros((r, len(train)))
        y_preds = np.zeros((r, len(test)))

        Q_list = []
        temp_errors = []
        mu_list = []

        for j in range(init_iter):
            B = initialize_Q(train1, train2, C=C, gamma=gamma, Q_size=Q_size, 
                             mu_coeff=mu_coeff, model=model, Leng=Leng, spd=spd, 
                             accn=accn, jrk=jrk, n_estimators=n_estimators)
            Q_list.append(B[0])
            temp_errors.append(B[-1])
            mu_list.append(B[1])

        h = np.argmin(temp_errors)
        Q = Q_list[h]
        mu = mu_list[h]
        print("mu =", mu)
        if sum([Leng, spd, accn, jrk]) > 0:
            A = train_labels.copy().reshape(-1,1)
            B = test_labels.copy().reshape(-1,1)
            if Leng == True:
                length_train = np.array([length(train_traj[i]) for i in range(len(train))]).reshape(-1,1)
                A = np.concatenate((A, length_train),1)
                length_test = np.array([length(test_traj[i]) for i in range(len(test))]).reshape(-1,1)
                B = np.concatenate((B, length_test),1)
            if spd == True:
                speed_train = np.array([speed(train[i]) for i in range(len(train))]).reshape(-1,1)
                A = np.concatenate((A, speed_train),1)
                speed_test = np.array([speed(test[i]) for i in range(len(test))]).reshape(-1,1)
                B = np.concatenate((B, speed_test),1)
            if accn == True:
                acceleration_train = np.array([acceleration(train[i]) for i in range(len(train))]).reshape(-1,1)
                A = np.concatenate((A, acceleration_train),1)
                acceleration_test = np.array([acceleration(test[i]) for i in range(len(test))]).reshape(-1,1)
                B = np.concatenate((B, acceleration_test),1)
            if jrk == True:
                jerk_train = np.array([jerk(train[i]) for i in range(len(train))]).reshape(-1,1)
                A = np.concatenate((A, jerk_train),1)
                jerk_test = np.array([jerk(test[i]) for i in range(len(test))]).reshape(-1,1)
                B = np.concatenate((B, jerk_test),1)
            if normal == True:
                A[:,1:] = (A[:,1:] - np.mean(A[:,1:] , 0))/(np.std(A[:,1:],0) + 1e-10)
                B[:,1:] = (B[:,1:] - np.mean(B[:,1:] , 0))/(np.std(B[:,1:],0) + 1e-10)
            train_data = np.concatenate((A[:,1:], np.array(ExpCurve2Vec(Q, train_traj, mu))), 1)
            test_data = np.concatenate((B[:,1:], np.array(ExpCurve2Vec(Q, test_traj, mu))), 1)
            #train_data = (train_data - np.mean(train_data))/(np.std(train_data) + 1e-10)
            #test_data = (test_data - np.mean(test_data))/(np.std(test_data) + 1e-10)
        else:
            train_data = np.array(ExpCurve2Vec(Q, train_traj, mu))
            test_data = test_length, np.array(ExpCurve2Vec(Q, test_traj, mu))

        for k in range(r): 
            Model = models[k]
            Model.fit(train_data, train_labels)
            x_preds[k] = Model.predict(train_data)                
            y_preds[k] = Model.predict(test_data)

        for k in range(r):
            train_errors[k][s] = 1 - metrics.accuracy_score(train_labels, x_preds[k])
            test_errors[k][s] = 1 - metrics.accuracy_score(test_labels, y_preds[k])

    for k in range(r):
        train_error_mean[k] = np.mean(train_errors[k])
        test_error_mean[k] = np.mean(test_errors[k])
        test_error_std[k] = np.std(test_errors[k])

    Dict = {}

    for k in range(len(keys)): 
        Dict[k+1] = [keys[k], np.round(train_error_mean[k], decimals = 4), 
                     np.round(test_error_mean[k], decimals = 4),
                     np.round(test_error_std[k], decimals = 4)]

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                columns=['Classifier','Train Error', 'Test Error', 'Std Error'])
    
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf, train_error_mean, test_error_mean

C = 100

clf_L = [make_pipeline(LinearSVC(dual=False, C=C, tol=1e-5, 
                               class_weight ='balanced', max_iter=1000)), 
        "SVM, LinearSVC, C="+str(C)]

L = classification_init_Q(cars_time, buses_time, C=C, gamma=1, Q_size=20, 
                          mu_coeff=1, model='LSVM', epoch=50, init_iter=3, 
                          classifiers=[clf_L], Leng=True, spd=True, accn=True, 
                          jrk=True, test_size=0.3, normal=False)
L[0]

C = 100
g = 'auto'
clf_GSVM = [make_pipeline(StandardScaler(), svm.SVC(C=C, kernel='rbf', gamma=g, 
                                                    max_iter=200000)),
                                    "GSVM, C="+str(C)+", gamma="+str(g)]

G = classification_init_Q(cars_time, buses_time, C=C, gamma=g, Q_size=20, 
                          mu_coeff=1, model='GSVM', epoch=50, init_iter=3, 
                          classifiers=[clf_GSVM], Leng=True, spd=True, 
                          accn=True, jrk=True, test_size=0.3, normal=False)
G[0]

C = 1000
clf_PSVM = [make_pipeline(StandardScaler(), svm.SVC(C=C, kernel='poly', degree=5, 
                                                    max_iter=400000)),
            "Poly kernel SVM, C="+str(C)+", deg=auto"]

P = classification_init_Q(cars_time, buses_time, C=C, gamma=g, Q_size=20, 
                          mu_coeff=1, model='PSVM', epoch=50, init_iter=3, 
                          classifiers=[clf_PSVM], Leng=True, spd=True, 
                          accn=True, jrk=True, test_size=0.3, normal=False)
P[0]

clf_DT = [DecisionTreeClassifier(), "Decision Tree"]

DT = classification_init_Q(cars_time, buses_time, C=1, gamma=1, Q_size=20, 
                          mu_coeff=1, model='DT', epoch=50, init_iter=3, 
                          classifiers=[clf_DT], Leng=True, spd=True, accn=True, 
                          jrk=True, test_size=0.3, normal=False)
DT[0]

n_estimators = 50

clf_RF = [RandomForestClassifier(n_estimators=n_estimators), 
         "RandomForestClassifier, n="+str(n_estimators)]

RF = classification_init_Q(cars_time, buses_time, C=1, gamma=1, Q_size=20, 
                          mu_coeff=1, model='RF', epoch=50, init_iter=3, 
                          classifiers=[clf_RF], Leng=True, spd=True, accn=True, 
                          jrk=True, test_size=0.3, normal=False, 
                           n_estimators=n_estimators)
RF[0]

n_neighbors = 5

clf_KNN = [KNeighborsClassifier(n_neighbors=n_neighbors), "KNN"]

K = classification_init_Q(cars_time, buses_time, C=1, gamma=1, Q_size=20, 
                          mu_coeff=1, model='KNN', epoch=50, init_iter=3, 
                          classifiers=[clf_KNN], Leng=True, spd=True, accn=True, 
                          jrk=True, test_size=0.3, normal=False)
K[0]

clf_LR = [LogisticRegression(solver='newton-cg'), "Logistic Regression"]

LR = classification_init_Q(cars_time, buses_time, C=100, gamma=1, Q_size=20, 
                          mu_coeff=1, model='LR', epoch=50, init_iter=3, 
                          classifiers=[clf_LR], Leng=True, spd=True, 
                          accn=True, jrk=True, test_size=0.3, normal=False)
LR[0]

"""# Run time Analysis (done)

## $v_Q$, $v_Q^{\varsigma}$, $v_Q^{\exp}$, endpoints
"""

from google.colab import files
files.upload()

import v_Q_mu_endpoints_classification_runtime
from v_Q_mu_endpoints_classification_runtime import runTime

"""## Class"""

import numpy as np 
import pandas as pd
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.svm import NuSVC
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import Perceptron
from termcolor import colored
from sklearn.svm import LinearSVC
import trjtrypy as tt
from trjtrypy.distances import d_Q
from trjtrypy.distances import d_Q_pi
from trjtrypy.featureMappings import curve2vec
from scipy.spatial import distance
from collections import Counter
import time
import timeit



def ExpCurve2Vec(points, curves, mu):
    D = tt.distsbase.DistsBase()
    a = np.array([np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves])
    return a


'''The following class includes 3 functions:
        1. According to v_Q classification (classification_v_Q() function)
            a) Average
            b) Average-Majority
        2. According to v_Q^mu classification (classification_v_Q_mu() function)
            a) Average
            b) Average-Majority
        3. Endpoint classification (endpoint_classification() function)
            a) Only average'''

class runTime():
    def __init__(self, data_1, data_2, Q_size, classifiers, std_coeff=1, version='unsigned', sigma=1):
        self.data_1 = data_1
        self.data_2 = data_2
        self.Q_size = Q_size
        self.classifiers = classifiers
        self.version = version
        self.sigma = sigma
        self.std_coeff = std_coeff

    
        
    '''mu calculator function'''
        
    def get_mu(self, data_1, data_2):
        a = np.mean([np.mean(data_1[i], 0) for i in range(len(data_1))], 0)
        b = np.mean([np.mean(data_2[i], 0) for i in range(len(data_2))], 0)
        return max(abs(a-b)) * self.std_coeff


    def generate_Q_runtime(self):

        start_time = time.time()
        data = np.concatenate((self.data_1, self.data_2), 0)

        Mean = np.mean([np.mean(data[i], 0) for i in range(len(data))], 0)
        Std = np.std([np.std(data[i], 0) for i in range(len(data))], 0)

        Q = np.ones((self.Q_size, 2))
        Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], self.Q_size)
        Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], self.Q_size)

        stop_time = time.time()
        total_time = stop_time - start_time

        return total_time


    def train_classifier_v_Q_runtime(self):

        start_time = time.time()
        models = [item[0] for item in self.classifiers]
        keys = [item[1] for item in self.classifiers]

        r = len(self.classifiers)
        train_runtime = np.zeros(r)

        for k in range(r): 

            time_temp = time.time() #timeit.default_timer()

            n_1 = len(self.data_1)
            n_2 = len(self.data_2) 

            data = np.concatenate((self.data_1, self.data_2), 0)
            labels = np.array([1]*n_1 + [-1]*n_2)

            Mean = np.mean([np.mean(data[i], 0) for i in range(len(data))], 0)
            Std = np.std([np.std(data[i], 0) for i in range(len(data))], 0)
            
            Q = np.ones((self.Q_size, 2))
            Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], self.Q_size)
            Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], self.Q_size)

            Data = curve2vec(Q, data, version=self.version, sigma=self.sigma)

            model = models[k]
            model.fit(Data, labels)
            stop_time = time.time() #timeit.default_timer()

            train_runtime[k] = stop_time - time_temp
                    
        print(colored(f"run time: {time.time() - start_time}", "red"))
        
        Dict = {}
        for k in range(len(keys)): 
            Dict[k+1] = [keys[k], train_runtime[k]]

        pdf = pd.DataFrame.from_dict(Dict, orient='index', columns=['Classifier','Runtime'])

        return pdf, train_runtime


    def train_classifier_v_Q_exp_runtime(self):

        start_time = time.time()
        models = [item[0] for item in self.classifiers]
        keys = [item[1] for item in self.classifiers]

        r = len(self.classifiers)
        train_runtime = np.zeros(r)

        for k in range(r): 

            time_temp = time.time() #timeit.default_timer()

            n_1 = len(self.data_1)
            n_2 = len(self.data_2) 

            data = np.concatenate((self.data_1, self.data_2), 0)
            labels = np.array([1]*n_1 + [-1]*n_2)

            mu = self.get_mu(self.data_1, self.data_2)

            Mean = np.mean([np.mean(data[i], 0) for i in range(len(data))], 0)
            Std = np.std([np.std(data[i], 0) for i in range(len(data))], 0)
            
            Q = np.ones((self.Q_size, 2))
            Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], self.Q_size)
            Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], self.Q_size)

            Data = ExpCurve2Vec(Q, data, mu)

            model = models[k]
            model.fit(Data, labels)
            stop_time = time.time() #timeit.default_timer()

            train_runtime[k] = stop_time - time_temp
                    
        print(colored(f"run time: {time.time() - start_time}", "red"))
        
        Dict = {}
        for k in range(len(keys)): 
            Dict[k+1] = [keys[k], train_runtime[k]]

        pdf = pd.DataFrame.from_dict(Dict, orient='index', columns=['Classifier','Runtime'])

        return pdf, train_runtime

    '''Get the endpoints of all trajectories in data_1 and data_2'''
    def get_endpoints(self):
        n_1 = len(self.data_1)
        n_2 = len(self.data_2)
        data_1_endpoints = np.zeros((n_1, 4))
        data_2_endpoints = np.zeros((n_2, 4))
        for i in range(n_1):
            data_1_endpoints[i] = np.concatenate((self.data_1[i][0], self.data_1[i][-1]), 0)
        for i in range(n_2):
            data_2_endpoints[i] = np.concatenate((self.data_2[i][0], self.data_2[i][-1]), 0)
        return data_1_endpoints, data_2_endpoints
        

    '''Get the endpoint classification error
       parameters: 
            1. data_1, data_2
            2. epoch
            3. classifiers'''
    def train_classifier_endpoint_runtime(self):

        start_time = time.time()
        models = [item[0] for item in self.classifiers]
        keys = [item[1] for item in self.classifiers]
        r = len(self.classifiers)
        train_runtime = np.zeros(r)
        n_1 = len(self.data_1)
        n_2 = len(self.data_2) 

        data_1_endpoints, data_2_endpoints = self.get_endpoints()
        Data = np.concatenate((data_1_endpoints, data_2_endpoints), 0)
        labels = np.array([1]*n_1 + [-1]*n_2)

        for k in range(r):
            time_temp = time.time()
            model = models[k]
            model.fit(Data, labels)
            stop_time = time.time()
            train_runtime[k] = stop_time - time_temp
                    
        print(colored(f"run time: {time.time() - start_time}", "red"))
        
        Dict = {}
        for k in range(len(keys)): 
            Dict[k+1] = [keys[k], train_runtime[k]]

        pdf = pd.DataFrame.from_dict(Dict, orient='index', columns=['Classifier','Runtime'])

        return pdf, train_runtime

"""## Runtime of initializing random landmarks with different classifiers"""

# Commented out IPython magic to ensure Python compatibility.
# %timeit runTime(cars, buses, Q_size=20, classifiers=clf, version='unsigned')
Runtime = runTime(cars, buses, Q_size=20, classifiers=clf, version='unsigned')
A = Runtime.generate_Q_runtime()
print(A)
print(colored(f'runtimes for generating random Q: {A}', 'yellow'))

"""## Runtime of training classifiers"""

# Commented out IPython magic to ensure Python compatibility.
# %timeit runTime(cars, buses, Q_size=20, classifiers=clf, version='unsigned')
Runtime = runTime(cars, buses, Q_size=20, classifiers=clf, version='unsigned')
A = Runtime.train_classifier_v_Q_runtime()
print(A[0])
print(colored(f'runtimes for v_Q: {list(A[1])}', 'yellow'))

# Commented out IPython magic to ensure Python compatibility.
# %timeit runTime(cars, buses, Q_size=20, classifiers=clf, version='signed', sigma=1)
Runtime = runTime(cars, buses, Q_size=20, classifiers=clf, version='signed', sigma=1)
A = Runtime.train_classifier_v_Q_runtime()
print(A[0])
print(colored(f'runtimes for v_Q^varsigma: {list(A[1])}', 'yellow'))

# Commented out IPython magic to ensure Python compatibility.
# %timeit runTime(cars, buses, Q_size=20, classifiers=clf, version='unsigned')
Runtime = runTime(cars, buses, Q_size=20, classifiers=clf, version='unsigned')
A = Runtime.train_classifier_v_Q_exp_runtime()
print(A[0])
print(colored(f'runtimes for v_Q^exp: {list(A[1])}', 'yellow'))

# Commented out IPython magic to ensure Python compatibility.
# %timeit runTime(cars, buses, Q_size=20, classifiers=clf)
Runtime = runTime(cars, buses, Q_size=20, classifiers=clf)
A = Runtime.train_classifier_endpoint_runtime()
print(A[0])
print(colored(f'runtimes for v_Q: {list(A[1])}', 'yellow'))

"""## Mistake-driven"""

from google.colab import files
files.upload()

import Perceptron_Like_Algo_Runtime
from Perceptron_Like_Algo_Runtime import runTimeMD

"""## Class"""

import numpy as np 
import pandas as pd
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.svm import NuSVC
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import Perceptron
from termcolor import colored
from sklearn.svm import LinearSVC
import trjtrypy as tt
from trjtrypy.distances import d_Q
from trjtrypy.distances import d_Q_pi
from trjtrypy.featureMappings import curve2vec
from scipy.spatial import distance
from collections import Counter
import time
from scipy.stats import entropy




def ExpCurve2Vec(points, curves, mu):
    D = tt.distsbase.DistsBase()
    a = np.array([np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves])
    return a


# models = ['LSVM', 'GSVM', 'PSVM', 'LR', 'PERN', 'DT', 'RF', 'KNN']

class runTimeMD:
    def __init__(self, data_1, data_2, Q_size, model, C, gamma, std_coeff, 
                 n_neighbors, n_estimators):
        
        self.data_1 = data_1
        self.data_2 = data_2
        self.Q_size = Q_size
        self.model = model
        self.C = C
        self.gamma = gamma
        self.std_coeff = std_coeff
        self.n_neighbors = n_neighbors
        self.n_estimators = n_estimators
        
        
    
        
    '''mu calculator function'''
        
    def get_mu(self):
        a = np.mean([np.mean(self.data_1[i], 0) for i in range(len(self.data_1))], 0)
        b = np.mean([np.mean(self.data_2[i], 0) for i in range(len(self.data_2))], 0)
        return max(abs(a-b)) * self.std_coeff
       
        
    '''Perceptron-Like Algorithm'''
        
    def initialize_Q(self): 
        
        Q = []        

        mu = self.get_mu()
        std = mu * self.std_coeff

        trajectory_train_data = np.concatenate((self.data_1, self.data_2), axis = 0)
        train_labels = np.concatenate(([1] * len(self.data_1), [-1] * len(self.data_2)), 0)
        index = np.random.randint(0, high=len(trajectory_train_data)) 
        k = np.random.randint(0, high=len(trajectory_train_data[index]))
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)
        
        if self.model == "LSVM":
            clf = make_pipeline(LinearSVC(dual=False, C=self.C, tol=1e-5, 
                                        class_weight='balanced', max_iter=1000))
        elif self.model == "GSVM":
            clf = make_pipeline(StandardScaler(), svm.SVC(C=self.C, kernel='rbf', 
                                                gamma=self.gamma, max_iter=200000))
        elif self.model == 'PSVM':
            clf = make_pipeline(StandardScaler(), svm.SVC(C=self.C, kernel='poly', 
                                                    degree=3, max_iter = 400000))
        elif self.model == "LR":
            clf = LogisticRegression(solver='lbfgs')
        elif self.model == "Prn":
            clf = Perceptron(tol=1e-5, validation_fraction=0.01, class_weight="balanced")
        elif self.model == "DT":
            clf = DecisionTreeClassifier()
        elif self.model == "RF":
            clf = RandomForestClassifier(n_estimators=self.n_estimators)
        elif self.model == "KNN":
            clf = KNeighborsClassifier(n_neighbors=self.n_neighbors)
        else:
            print("error: model is not supported")
        
        for i in range(self.Q_size):
            train_data = ExpCurve2Vec(np.array(Q), trajectory_train_data, mu)
            clf.fit(train_data, train_labels)

            train_pred = clf.predict(train_data)
            error = sum(train_labels != train_pred)/len(train_labels)
            
            if self.model in ['LSVM', 'GSVM', 'PSVM', 'LR', 'Prn']:
                temp = train_labels * clf.decision_function(train_data)
                index = np.argmin(temp)
            elif self.model in ['DT', 'RF', 'KNN']:
                probs = clf.predict_proba(train_data)
                index = np.argmax(entropy(probs, axis=1))
            
            k = np.random.randint(0, high=len(trajectory_train_data[index]))
            q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
            Q.append(q)
        
        return np.array(Q), mu



    
    def train_classifier_runtime_Q(self):

        if self.model == 'LSVM':
            clf_L = [make_pipeline(LinearSVC(dual=False, C=self.C, tol=1e-5, 
                                       class_weight ='balanced', max_iter=1000)), 
                    "SVM, Linear SVC, C="+str(self.C)]
            clf = clf_L[0] 
        elif self.model == 'GSVM':
            clf_rbf = [make_pipeline(StandardScaler(), svm.SVC(C=self.C, kernel='rbf', 
                                                    gamma=self.gamma, max_iter=200000)), 
                       "GSVM, C="+str(self.C)+", gamma="+str(self.gamma)]
            clf = clf_rbf[0]
        elif self.model == 'PSVM':
            clf_PSVM = [make_pipeline(StandardScaler(), svm.SVC(C=self.C, kernel='poly', 
                                                        degree=3, max_iter = 400000)),
                        "Poly kernel SVM, C="+str(self.C)+", deg=auto"]
            clf = clf_PSVM[0]
        elif self.model == "LR":
            clf_LR = [LogisticRegression(solver='lbfgs'), "Logistic Regression"]
            clf = clf_LR[0]
        elif self.model == "Prn":
            clf_Prn = [Perceptron(tol=1e-5, validation_fraction=0.01, 
                               class_weight="balanced"), "Perceptron"]
            clf = clf_Prn [0]
        elif self.model == "DT":
            clf_DT = [DecisionTreeClassifier(), "Decision Tree"]
            clf = clf_DT[0]
        elif self.model == "RF":
            clf_RF = [RandomForestClassifier(n_estimators=self.n_estimators), 
                             "RandomForestClassifier, n="+str(self.n_estimators)]
            clf = clf_RF[0]
        elif self.model == "KNN":
            clf_KNN = [KNeighborsClassifier(n_neighbors=self.n_neighbors), "KNN"]
            clf = clf_KNN[0]
        else:
            print('model is not supported')

        start_time = time.time()

        n_1 = len(self.data_1)
        n_2 = len(self.data_2) 
        data = np.concatenate((self.data_1, self.data_2), axis = 0)
        labels = np.array([1]*n_1 + [-1]*n_2)
        
        I = np.arange(len(data))
        np.random.shuffle(I)
        data = data[I]
        labels = labels[I]

        init_Q_strat_time = time.time()
        B = self.initialize_Q()
        init_Q_end_time = time.time()
        init_total_time = init_Q_end_time - init_Q_strat_time
        
        Q = B[0]
        mu = B[1]
        Data = ExpCurve2Vec(Q, data, mu)
        clf.fit(Data, labels)

        train_runtime = time.time() - start_time

        return train_runtime, init_total_time

"""## Runtime of training classifiers"""

# Commented out IPython magic to ensure Python compatibility.
model = 'LSVM'
# %timeit runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', \
                  std_coeff=1, n_neighbors=5, n_estimators=50)
Runtime = runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', 
                    std_coeff=1, n_neighbors=5, n_estimators=50)
A = Runtime.train_classifier_runtime_Q()

print(colored(f'runtimes for initializing 20 landmarks with {model}: {np.round(A[1], decimals=4)}', 'magenta'))
print(colored(f'runtimes for {model}: {np.round(A[0], decimals=4)}', 'yellow'))

# Commented out IPython magic to ensure Python compatibility.
model = 'GSVM'
# %timeit runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', \
                  std_coeff=1, n_neighbors=5, n_estimators=50)
Runtime = runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', 
                    std_coeff=1, n_neighbors=5, n_estimators=50)
A = Runtime.train_classifier_runtime_Q()

print(colored(f'runtimes for initializing 20 landmarks with {model}: {np.round(A[1], decimals=4)}', 'magenta'))
print(colored(f'runtimes for {model}: {np.round(A[0], decimals=4)}', 'yellow'))

# Commented out IPython magic to ensure Python compatibility.
model = 'PSVM'
# %timeit runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', \
                  std_coeff=1, n_neighbors=5, n_estimators=50)
Runtime = runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', 
                    std_coeff=1, n_neighbors=5, n_estimators=50)
A = Runtime.train_classifier_runtime_Q()

print(colored(f'runtimes for initializing 20 landmarks with {model}: {np.round(A[1], decimals=4)}', 'magenta'))
print(colored(f'runtimes for {model}: {np.round(A[0], decimals=4)}', 'yellow'))

# Commented out IPython magic to ensure Python compatibility.
model = 'DT'
# %timeit runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', \
                  std_coeff=1, n_neighbors=5, n_estimators=50)
Runtime = runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', 
                    std_coeff=1, n_neighbors=5, n_estimators=50)
A = Runtime.train_classifier_runtime_Q()

print(colored(f'runtimes for initializing 20 landmarks with {model}: {np.round(A[1], decimals=4)}', 'magenta'))
print(colored(f'runtimes for {model}: {np.round(A[0], decimals=4)}', 'yellow'))

# Commented out IPython magic to ensure Python compatibility.
model = 'RF'
# %timeit runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', \
                  std_coeff=1, n_neighbors=5, n_estimators=50)
Runtime = runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', 
                    std_coeff=1, n_neighbors=5, n_estimators=50)
A = Runtime.train_classifier_runtime_Q()

print(colored(f'runtimes for initializing 20 landmarks with {model}: {np.round(A[1], decimals=4)}', 'magenta'))
print(colored(f'runtimes for {model}: {np.round(A[0], decimals=4)}', 'yellow'))

# Commented out IPython magic to ensure Python compatibility.
model = 'KNN'
# %timeit runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', \
                  std_coeff=1, n_neighbors=5, n_estimators=50)
Runtime = runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', 
                    std_coeff=1, n_neighbors=5, n_estimators=50)
A = Runtime.train_classifier_runtime_Q()

print(colored(f'runtimes for initializing 20 landmarks with {model}: {np.round(A[1], decimals=4)}', 'magenta'))
print(colored(f'runtimes for {model}: {np.round(A[0], decimals=4)}', 'yellow'))

# Commented out IPython magic to ensure Python compatibility.
model = 'LR'
# %timeit runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', \
                  std_coeff=1, n_neighbors=5, n_estimators=50)
Runtime = runTimeMD(cars, buses, Q_size=20, model=model, C=100, gamma='auto', 
                    std_coeff=1, n_neighbors=5, n_estimators=50)
A = Runtime.train_classifier_runtime_Q()

print(colored(f'runtimes for initializing 20 landmarks with {model}: {np.round(A[1], decimals=4)}', 'magenta'))
print(colored(f'runtimes for {model}: {np.round(A[0], decimals=4)}', 'yellow'))

"""## KNN runtime

### install package
"""

# Commented out IPython magic to ensure Python compatibility.
# to make a directory
# %cd 'gdrive/My Drive/traj-dist'

# to see in what directory we are in

pwd

# to install setup.py from the current directory
!python setup.py install

!pip install geohash2

pip install fastdtw

pip install tslearn

"""### KNN runtimes"""

from google.colab import files
files.upload()

import KNN_Class_Colab_runtime
from KNN_Class_Colab_runtime import KNN_runTime

'''This class handles all the metrics in "metrics array bellow" and is appropriate for using in Anaconda 
   for example, but not on Google Colab.'''

'''Requirements: (These are already installed in my computer)
        1. We need "pip install trjtrypy" in order to be able to use d_Q_pi
        2. We need "pip install tslearn" in order to be able to use dtw
        3. We need "pip install fastdtw" in order to be able to use fastdtw
        4. We need "pip install traj_dist" in order to be able to use the rest of metrics'''


import numpy as np
import time
import pandas as pd
import random
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw
import traj_dist.distance as tdist
import pickle
import tslearn
from tslearn.metrics import dtw as dtw_tslearn
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from trjtrypy.distances import d_Q_pi
from termcolor import colored

#metrics = ['discret_frechet', 'hausdorff', 'dtw', 'sspd', 'erp', 'edr', 'lcss',  
#           fastdtw, dtw, d_Q_pi]

# path example: 
#'Calculated Distance Matrices for KNN/Beijing-Pairs['+str(pairs_final[i])+']-d_Q_pi.csv'


class KNN_runTime:
    
    def __init__(self, data1, data2, metric, gamma=None, eps_edr=None, eps_lcss=None, 
                 Q_size=None, Q=None, p=2, n_neighbors=5, pair=None):
        '''data1 = data[pair[0]]
           data2 = data[pair[1]]'''
        self.data1 = data1
        self.data2 = data2
        self.metric = metric
        self.gamma = gamma
        self.eps_edr = eps_edr
        self.eps_lcss = eps_lcss
        self.Q_size = Q_size
        self.Q = Q
        self.p = p
        self.n_neighbors = n_neighbors
        self.pair = pair
    



    def calculate_dists_matrix(self):

        data = np.concatenate((self.data1, self.data2), 0)
        n = len(data)

        if self.metric == 'lcss':
            A = tdist.pdist(data, self.metric, type_d="euclidean", eps=self.eps_lcss)
        if self.metric == 'edr':
            A = tdist.pdist(data, self.metric, type_d="euclidean", eps=self.eps_edr)
        if self.metric in ['discret_frechet', 'hausdorff', 'dtw', 'sspd', 'erp']: 
            A = tdist.pdist(data, str(self.metric))
        if self.metric == fastdtw: 
            A = []
            for i in range(n-1):
                for j in range(i+1, n):
                    A.append(self.metric(data[i], data[j])[0])
        if self.metric == dtw_tslearn: 
            A = []
            for i in range(n-1):
                for j in range(i+1, n):
                    A.append(self.metric(data[i], data[j]))
        if self.metric == 'd_Q_pi':
            A = []
            if self.Q_size:
                Q = self.generate_random_Q()
                for i in range(n-1):
                    for j in range(i+1, n):
                        A.append(d_Q_pi(Q, data[i], data[j], p=self.p))
            elif len(self.Q):
                for i in range(n-1):
                    for j in range(i+1, n):
                        A.append(d_Q_pi(self.Q, data[i], data[j], p=self.p))

        tri = np.zeros((n, n))
        tri[np.triu_indices(n, 1)] = np.array(A)
        for i in range(1, n):
            for j in range(i):
                tri[i][j] = tri[j][i]
                
        return tri





    '''The following function is only used for d_Q_pi distance in order to 
       generate random landmarks
       Notice: in this pattern of coding we cannot use train1 and train2 to 
       get Q in the following function.'''
    def generate_random_Q(self):
        Q = np.zeros((self.Q_size, 2))
        data = np.concatenate((self.data1, self.data2), 0)
        Mean = np.mean([np.mean(data[i], 0) for i in range(len(data))], 0)
        Std = np.std([np.std(data[i], 0) for i in range(len(data))], 0)
        Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], self.Q_size)
        Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], self.Q_size)
        return Q




    def KNN_runtime(self):
        
        start_time = time.time()
        n_1 = len(self.data1)
        n_2 = len(self.data2) 
        
        labels = np.array([1] * n_1 + [-1] * n_2)
        data = np.concatenate((self.data1, self.data2), 0)
        dist_matrix = self.calculate_dists_matrix()

        clf = KNeighborsClassifier(n_neighbors=self.n_neighbors, metric='precomputed')
        clf.fit(dist_matrix, list(labels))
        stop_time = time.time()
        runtime = stop_time - start_time

        return runtime

# Commented out IPython magic to ensure Python compatibility.
metrics = ['discret_frechet', 'hausdorff', dtw_tslearn, fastdtw, 'lcss', 'sspd',
           'edr', 'erp', 'd_Q_pi']
runtimes = []
for metric in metrics:
#     %timeit KNN_runTime(cars, buses, metric=metric, gamma=None, eps_edr=0.02, \
                        eps_lcss=0.02, Q_size=20, Q=None, p=2, n_neighbors=5, pair=[0,1])

    Runtime = KNN_runTime(cars, buses, metric=metric, gamma=None, eps_edr=0.02, 
                      eps_lcss=0.02, Q_size=20, Q=None, p=2, n_neighbors=5, pair=[0,1])
    a = Runtime.KNN_runtime()
    runtimes.append(a)
    print(colored(f'runtimes for {metric}: {a}', 'yellow'))
    print(colored("===============================================================", 'red'))

print(colored(runtimes, 'blue'))

"""### KNN_LSH"""

from google.colab import files
files.upload()

import KNN_with_LSH_runtime
from KNN_with_LSH_runtime import KNN_LSH_runTime

# pip install trjtrypy

'''A clss for KNN with LSH distance with random circles in each iteration'''

import numpy as np 
import time
import random
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from trjtrypy.basedists import distance
from termcolor import colored




class KNN_LSH_runTime:
    def __init__(self, data1, data2, number_circles):
        self.data1 = data1
        self.data2 = data2
        self.number_circles = number_circles



    def get_circles(self):

        data = np.concatenate((self.data1, self.data2,), 0)
        n = len(data)
        Mean = np.mean([np.mean(data[i], 0) for i in range(n)], 0)
        Std = np.std([np.std(data[i], 0) for i in range(n)], 0)
        circles_centers = np.ones((self.number_circles,2))
        circles_centers[:,0] = np.random.normal(Mean[0], 4 * Std[0], self.number_circles)
        circles_centers[:,1] = np.random.normal(Mean[1], 4 * Std[1], self.number_circles)

        return circles_centers



    def get_radius(self):

        a = np.mean([np.mean(self.data1[i], 0) for i in range(len(self.data1))], 0)
        b = np.mean([np.mean(self.data2[i], 0) for i in range(len(self.data2))], 0)

        return max(abs(a-b))



    def LSH_sketch(self):

        radius = self.get_radius()
        #print("radius =", radius)
        data = np.concatenate((self.data1, self.data2), 0)
        circles_centers = self.get_circles()
        dists = distance(circles_centers, data) # shape = len(data) x number_circles
        LSH_array = np.zeros((len(data), self.number_circles))
        circules_cut_idx = np.where(dists < radius)
        LSH_array[circules_cut_idx] = 1

        return LSH_array


    def calculate_LSH_dists(self):

        LSH_array = self.LSH_sketch()
        data = np.concatenate((self.data1, self.data2), 0)
        dists = np.zeros((len(data), len(data)))
        for i in range(len(data)-1):
            dists[i, i+1:] = np.sum(abs(LSH_array[i+1:] - LSH_array[i]), 1)
        for i in range(len(data)-1):
            for j in range(i+1, len(data)):
                dists[j][i] = dists[i][j]
        
        return dists



    def KNN_LSH_runtime(self):

        start_time = time.time()
        n_1 = len(self.data1)
        n_2 = len(self.data2)

        radius = self.get_radius()
        dist_matrix = self.calculate_LSH_dists()

        labels = np.array([1]*n_1 + [-1] * n_2)

        clf = KNeighborsClassifier(n_neighbors=5, metric='precomputed')
        clf.fit(dist_matrix, list(labels))

        stop_time = time.time()
        runtime = stop_time - start_time
        
        return runtime

# Commented out IPython magic to ensure Python compatibility.
# %timeit KNN_LSH = KNN_LSH_runTime(cars, buses, number_circles=20)

KNN_LSH = KNN_LSH_runTime(cars, buses, number_circles=20)
t = KNN_LSH.KNN_LSH_runtime()
print(colored(f'runtime for KNN with LSH is: {t}', 'yellow'))

"""# |Q|=10 (done)

## Classification with feature mappings $v_Q$, $v_Q^{\exp}$, $v_Q^{\varsigma}$ with 10 random landmarks, and endpoints
"""

from google.colab import files
files.upload()

import v_Q_mu_endpoints_classification
from v_Q_mu_endpoints_classification import binaryClassificationAverageMajority

"""### Boost($v_Q$), Boost($v_Q^{\varsigma}$), Boost($v_Q^{\exp}$) with epoch=50 and num_trials_maj=11"""

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=10, epoch=50, 
                                    num_trials_maj=11, classifiers=clf,
                                    version='unsigned', test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=10, 
                            epoch=50, num_trials_maj=11, classifiers=clf,
                            version='signed', sigma=1, test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=10, epoch=50, 
                                num_trials_maj=11, classifiers=clf, test_size=0.3)
A = classifs.classification_v_Q_mu()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

"""### Rand $v_Q$, Rand $v_Q^{\varsigma}$, Rand $v_Q^{\exp}$ with epoch=50 and num_trials_maj=1"""

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=10, 
                                    epoch=50, num_trials_maj=1, classifiers=clf, 
                                    version='unsigned', test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=10, 
                                    epoch=50, num_trials_maj=1, classifiers=clf, 
                                    version='signed', sigma=1, test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=10, 
                    epoch=50, num_trials_maj=1, classifiers=clf, test_size=0.3)
A = classifs.classification_v_Q_mu()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

"""## KNN with $d_Q^{\pi}$ distance with |Q|=10"""

# Choose Q

a, c = np.min((np.min([np.min(cars[i], axis=0) for i in range(len(cars))], axis=0), 
       np.min([np.min(buses[i], axis=0) for i in range(len(buses))], axis=0)), axis=0)
  
b, d = np.max((np.max([np.max(cars[i], axis=0) for i in range(len(cars))], axis=0), 
               np.max([np.max(buses[i], axis=0) for i in range(len(buses))], axis=0)), axis=0)

m = 10
Q = np.ones((m,2))

Q[:,0] = 0.2 * (b - a ) * np.random.random_sample(m) + a - 0.01
Q[:,1] = 0.2 *(d - c ) * np.random.random_sample(m) + c - 0.02

for i in range(len(cars)):
    plt.plot(cars[i][:,0], cars[i][:,1], color = "steelblue");
for i in range(len(buses)):
    plt.plot(buses[i][:,0], buses[i][:,1], color = "r");
plt.scatter(Q[:,0], Q[:,1], color = "black")
print(colored(f'Original car-bus', 'yellow'))
plt.show()

Q

# Calculate distance matrix

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/d_Q_pi-76-44-|Q|=10.csv'
calculate_dists_d_Q_pi(cars, buses, p=1, path=path)

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/d_Q_pi-76-44-|Q|=10.csv'
KNN_average_error_d_Q_pi(cars, buses, num_trials=50, path_to_dists=path)

"""## Classification with Perceptron-Like algorithm (done)"""

from google.colab import files
files.upload()

import Perceptron_Like_Algo_Class
from Perceptron_Like_Algo_Class import classification

"""### Boost(MD $v_Q^{\exp}$) with epoch=50, maj_num=11 and init_iter=3 (done)"""

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT']:
    classif = classification(cars, buses, Q_size=10, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

for Model in ['RF', 'KNN', 'LR']:
    classif = classification(cars, buses, Q_size=10, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

"""### MD $v_Q^{\exp}$ with epoch=50, maj_num=1 and init_iter=3 (done)"""

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
    classif = classification(cars, buses, Q_size=10, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=1, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

"""# |Q|=50

## Classification with feature mappings $v_Q$, $v_Q^{\exp}$, $v_Q^{\varsigma}$ with 10 random landmarks, and endpoints
"""

from google.colab import files
files.upload()

import v_Q_mu_endpoints_classification
from v_Q_mu_endpoints_classification import binaryClassificationAverageMajority

"""### Boost($v_Q$), Boost($v_Q^{\varsigma}$), Boost($v_Q^{\exp}$) with epoch=50 and num_trials_maj=11"""

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=50, epoch=50, 
                                    num_trials_maj=11, classifiers=clf,
                                    version='unsigned', test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=50, 
                            epoch=50, num_trials_maj=11, classifiers=clf,
                            version='signed', sigma=1, test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=50, epoch=50, 
                                num_trials_maj=11, classifiers=clf, test_size=0.3)
A = classifs.classification_v_Q_mu()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

"""### Rand $v_Q$, Rand $v_Q^{\varsigma}$, Rand $v_Q^{\exp}$ with epoch=50 and num_trials_maj=1"""

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=50, 
                                    epoch=50, num_trials_maj=1, classifiers=clf, 
                                    version='unsigned', test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=50, 
                                    epoch=50, num_trials_maj=1, classifiers=clf, 
                                    version='signed', sigma=1, test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(cars, buses, Q_size=50, 
                    epoch=50, num_trials_maj=1, classifiers=clf, test_size=0.3)
A = classifs.classification_v_Q_mu()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

"""## KNN with $d_Q^{\pi}$ distance with |Q|=50"""

# Choose Q

a, c = np.min((np.min([np.min(cars[i], axis=0) for i in range(len(cars))], axis=0), 
       np.min([np.min(buses[i], axis=0) for i in range(len(buses))], axis=0)), axis=0)
  
b, d = np.max((np.max([np.max(cars[i], axis=0) for i in range(len(cars))], axis=0), 
               np.max([np.max(buses[i], axis=0) for i in range(len(buses))], axis=0)), axis=0)

m = 50
Q = np.ones((m,2))

Q[:,0] = 0.2 * (b - a ) * np.random.random_sample(m) + a - 0.01
Q[:,1] = 0.2 *(d - c ) * np.random.random_sample(m) + c - 0.02

for i in range(len(cars)):
    plt.plot(cars[i][:,0], cars[i][:,1], color = "steelblue");
for i in range(len(buses)):
    plt.plot(buses[i][:,0], buses[i][:,1], color = "r");
plt.scatter(Q[:,0], Q[:,1], color = "black")
print(colored(f'Original car-bus', 'yellow'))
plt.show()

Q

# Calculate distance matrix

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/d_Q_pi-76-44-|Q|=50.csv'
calculate_dists_d_Q_pi(cars, buses, p=1, path=path)

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/d_Q_pi-76-44-|Q|=50.csv'
KNN_average_error_d_Q_pi(cars, buses, num_trials=50, path_to_dists=path)

"""## Classification with Perceptron-Like algorithm (done)"""

from google.colab import files
files.upload()

import Perceptron_Like_Algo_Class
from Perceptron_Like_Algo_Class import classification

"""### Boost(MD $v_Q^{\exp}$) with epoch=50, maj_num=11 and init_iter=3 (done)"""

for Model in ['LSVM', 'GSVM']:
    classif = classification(cars, buses, Q_size=50, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

for Model in ['PSVM', 'DT']:
    classif = classification(cars, buses, Q_size=50, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

classif = classification(cars, buses, Q_size=50, model='RF', C=100, gamma='auto', 
                        classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

A = classif.classification_Q()
print(A[0])
print(colored(f"mu = {A[1]}", 'blue'))

for Model in ['KNN', 'LR']:
    classif = classification(cars, buses, Q_size=50, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

"""### MD $v_Q^{\exp}$ with epoch=50, maj_num=1 and init_iter=3 (done)"""

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
    classif = classification(cars, buses, Q_size=50, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=1, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

"""# Convolutional Neural Network For Random Choice of Q

## Helper functions
"""

from collections import Counter

def find_majority(votes):
    vote_count = Counter(votes)
    top = vote_count.most_common(1)
    return top[0][0]

def find_majority_array(A): # column-wise majority
    return list(map(find_majority, A.T))

def ExpCurve2Vec(points, curves, mu):
    D = tt.distsbase.DistsBase()
    a = np.array([np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves])
    return a

def get_endpoints(data):
    n = len(data)
    data_endpoints = np.zeros((n, 4))
    for i in range(n):
        data_endpoints[i] = np.concatenate((data[i][0], data[i][-1]), 0)
    return data_endpoints

def train_test(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size = test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size = test_size)

    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
                train_idx_2, test_idx_2, train_label_2, test_label_2

def train_test_mu(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size=test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size=test_size)

    train_1 = data_1[train_idx_1]
    train_2 = data_2[train_idx_2]
    test_1 = data_1[test_idx_1]
    test_2 = data_2[test_idx_2]

    arr1 = np.arange(len(train_1)+len(train_2))
    I_1 = np.random.shuffle(arr1)

    arr2 = np.arange(len(test_1)+len(test_2))
    I_2 = np.random.shuffle(arr2)
    
    train = np.concatenate((train_1, train_2), 0)[arr1[I_1]]
    train_labels = np.concatenate((train_label_1, train_label_2), 0)[arr1[I_1]]
    test = np.concatenate((test_1, test_2), 0)[arr2[I_2]]
    test_labels = np.concatenate((test_label_1, test_label_2), 0)[arr2[I_2]]

    a = np.mean([np.mean(train_1[i], 0) for i in range(len(train_1))], 0)
    b = np.mean([np.mean(train_2[i], 0) for i in range(len(train_2))], 0)
    mu = max(abs(a-b))
    
    return mu, train, test, train_labels, test_labels

def flatten(x):
    N = x.shape[0] 
    return x.view(N, -1)

class Flatten(nn.Module):
    def forward(self, x):
        return flatten(x)

"""## Neural Network with Random $Q$"""

def neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 1e-3, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 10, 
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3):
    
    """ 
    in_channels: the dimension of hidden layer
    D_out: output dimension
    version: 'signed' or 'unsigned' or 'exp' 
    stride: should be fixed to 1
    """

    start_time = time.time()

    train_errors = np.zeros(epoch)
    test_errors = np.zeros(epoch)

    losses = torch.zeros(epoch, num_trials_maj, Num_updates)
    
    for s in range(epoch):

        mu, train, test, train_labels, test_labels = train_test_mu(data_1, data_2, test_size)

        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()
        
        x_preds = torch.zeros(num_trials_maj, len(train[0]))
        y_preds = torch.zeros(num_trials_maj, len(test[0]))

        Min = np.min([np.min(train[0][i], 0) for i in range(len(train[0]))], 0)
        Max = np.max([np.max(train[0][i], 0) for i in range(len(train[0]))], 0)
        Mean = np.mean([np.mean(train[0][i], 0) for i in range(len(train[0]))], 0)
        Std = np.std([np.std(train[0][i], 0) for i in range(len(train[0]))], 0)
        
        for t in range(num_trials_maj):
            Q = np.ones((Q_size, 2))
            Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], Q_size)
            Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], Q_size)

            if (version == 'unsigned' or version == 'signed'):
                train_data = curve2vec(Q, train[0], version = version, sigma = sigma)
                test_data = curve2vec(Q, test[0], version = version, sigma = sigma)
            elif version == 'exp':
                train_data = ExpCurve2Vec(Q, train[0], mu)
                test_data = ExpCurve2Vec(Q, test[0], mu)
            elif version == 'endpoints':
                train_data = get_endpoints(train[0])
                test_data = get_endpoints(test[0])
            
            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                #nn.LeakyReLU(0.01),
                                #nn.Tanh(),
                                Flatten(),
                                nn.Linear(out_channels * (len(train_data[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )

            #if torch.cuda.is_available(): # or device == torch.device("cuda:0"):
            #    model.cuda()
    
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
            
            train_data = torch.from_numpy(train_data).float()
            test_data = torch.from_numpy(test_data).float()

            train_data = train_data.view(len(train_data), 1, len(train_data[0]))
            
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels[0])
                losses[s, t, k] = loss
                    
                if (k+1) % 100 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

                optimizer.zero_grad()

                loss.backward() # Backward pass

                optimizer.step()  # Calling the step function on the Optimizer 

            x_preds[t] = torch.argmax(model(train_data), axis=1)
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))        
            y_preds[t] = torch.argmax(model(test_data), axis=1)
        
        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))

        train_errors[s] = sum(abs(train_labels[0] - x_preds))/len(train_labels[0])
        test_errors[s] = sum(abs(test_labels[0] - y_preds))/len(test_labels[0])
            
    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    plt.plot((torch.mean(losses, dim=(0,1))).detach().numpy())
    plt.show()

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
               np.round(train_error_mean, decimals=4), 
                np.round(test_error_mean, decimals=4),
                np.round(test_error_std, decimals=4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                columns=['Channel 1', 'Learning Rate', 'Train Error', 
                         'Test Error', 'Std Error'])
    
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf

"""### Endpoints"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 1e-3, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 2,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'endpoints', 
                                   sigma = 1, test_size = 0.3)

"""### |Q|=10

### Majority = 11

#### Vote(Rand $v_Q$)
"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 10, lr_decay = 0.9, 
                                   learning_rate = 4e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Vote($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 10, lr_decay = 0.9, 
                                   learning_rate = 2e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'signed', 
                                   sigma = 1, test_size = 0.3)

"""#### Vote(Rand $v_Q^{exp}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 10, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'exp', 
                                   sigma = 1, test_size = 0.3)

"""### Majority = 1

#### Rand($v_Q$)
"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 10, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Rand($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 10, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Rand($v_Q^{exp}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 10, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'exp', 
                                   sigma = 1, test_size = 0.3)

"""### |Q|=20

### Majority = 11

#### Vote(Rand $v_Q$)
"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 4e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Vote($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 2e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'signed', 
                                   sigma = 1, test_size = 0.3)

"""#### Vote(Rand $v_Q^{exp}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'exp', 
                                   sigma = 1, test_size = 0.3)

"""### Majority = 1

#### Rand($v_Q$)
"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Rand($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Rand($v_Q^{exp}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'exp', 
                                   sigma = 1, test_size = 0.3)

"""### |Q|=30

### Majority = 11

#### Vote(Rand $v_Q$)
"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 30, lr_decay = 0.9, 
                                   learning_rate = 4e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Vote($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 30, lr_decay = 0.9, 
                                   learning_rate = 2e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'signed', 
                                   sigma = 1, test_size = 0.3)

"""#### Vote(Rand $v_Q^{exp}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 30, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'exp', 
                                   sigma = 1, test_size = 0.3)

"""### Majority = 1

#### Rand($v_Q$)
"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 30, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Rand($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 30, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Rand($v_Q^{exp}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 30, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'exp', 
                                   sigma = 1, test_size = 0.3)

"""### |Q|=40

### Majority = 11

#### Vote(Rand $v_Q$)
"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 40, lr_decay = 0.9, 
                                   learning_rate = 4e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Vote($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 40, lr_decay = 0.9, 
                                   learning_rate = 2e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'signed', 
                                   sigma = 1, test_size = 0.3)

"""#### Vote(Rand $v_Q^{exp}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 40, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'exp', 
                                   sigma = 1, test_size = 0.3)

"""### Majority = 1

#### Rand($v_Q$)
"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 40, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Rand($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 40, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Rand($v_Q^{exp}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 40, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'exp', 
                                   sigma = 1, test_size = 0.3)

"""### |Q| = 50

### Majority = 11

#### Vote(Rand $v_Q$)
"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 50, lr_decay = 0.9, 
                                   learning_rate = 4e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Vote($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 50, lr_decay = 0.9, 
                                   learning_rate = 2e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'signed', 
                                   sigma = 1, test_size = 0.3)

"""#### Vote(Rand $v_Q^{exp}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 50, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 500, 
                                   D_out = 2, epoch = 50, version = 'exp', 
                                   sigma = 1, test_size = 0.3)

"""### Majority = 1

#### Rand($v_Q$)
"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 50, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Rand($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 50, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3)

"""#### Rand($v_Q^{exp}$)"""

neuralNetworkClassificationCNN(cars, buses, Q_size = 50, lr_decay = 0.9, 
                                   learning_rate = 1e-2, num_trials_maj = 1,
                                   out_channels = 10, kernel_size = 5,  
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'exp', 
                                   sigma = 1, test_size = 0.3)

"""# Neural Network for Mistake Driven Method of Choosing Q

## Initialize Q
"""

def initialize_Q(train_1, train_2, std_coeff, out_channels, kernel_size, Q_size, 
                 padding, learning_rate = 1e-3, bias = True, D_out=2, lr_decay = 0.9, 
                 Num_updates = 100): 
        
    Q = []
    errors = []
    losses = np.zeros((Q_size - kernel_size, Num_updates))
    
    mu = get_mu(train_1, train_2)
    std = mu * std_coeff

    trajectory_train_data = np.concatenate((train_1, train_2), axis = 0)
    train_labels = np.concatenate(([1] * len(train_1), [0] * len(train_2)), 0)
    train_labels = torch.from_numpy(train_labels).long()
    index = np.random.randint(0, high=len(trajectory_train_data)) 
    k = np.random.randint(0, high=len(trajectory_train_data[index]))
    for i in range(kernel_size):
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)

    for i in range(Q_size - kernel_size):
        train_data = ExpCurve2Vec(np.array(Q), trajectory_train_data, mu)
        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                            out_channels = out_channels, 
                            kernel_size = kernel_size,
                            stride  = 1,
                            padding = padding,
                            bias = bias),
                nn.ReLU(),
                #nn.LeakyReLU(0.01),
                #nn.Tanh(),
                Flatten(),
                nn.Linear(out_channels * (len(train_data[0]) - kernel_size + 1 + 2 * padding), 
                            D_out)
                )
    
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
        train_data = torch.from_numpy(train_data).float()
        train_data = train_data.view(len(train_data), 1, len(train_data[0]))
        
        for k in range(Num_updates):
            x_pred = model(train_data) # of shape (N, D_out)
            loss = loss_fn(x_pred, train_labels)
            losses[i, k] = loss
            
            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
            optimizer.zero_grad()
            loss.backward() 
            optimizer.step() 
        
        train_pred = torch.argmax(model(train_data), axis=1)
        scores = model(train_data)
        I = np.where((train_labels == train_pred) == False)[0]

        temp_labels = 2 * train_labels.numpy().reshape(len(train_labels.numpy()), 1) - 1
        temp = temp_labels * scores.detach().numpy()
        temp = np.max(temp, axis=1)
        index = I[np.argmax(temp[I])]

        error = sum(train_labels != train_pred)/len(train_labels)
        errors.append(error.item())
        
        k = np.random.randint(0, high=len(trajectory_train_data[index]))
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)

    final_error = errors[-1]

    #plt.plot((np.mean(losses, axis=0)))
    #plt.show()

    return np.array(Q), np.array(errors), mu, final_error

# Example
initialize_Q(cars, buses, std_coeff=1, out_channels=10, kernel_size=5, 
             Q_size=20, padding=1, learning_rate = 1e-2, bias = True, D_out = 2, 
             lr_decay = 0.9, Num_updates = 100)

"""## Mistake Driven with CNN"""

def MD_NeuralNetworkClassificationCNN(data_1, data_2, maj_num, epoch, init_iter, 
                                      test_size, std_coeff, out_channels, kernel_size, 
                                      Q_size, padding, learning_rate = 1e-3, 
                                      bias = True, D_out=2, lr_decay = 0.9, 
                                      Num_updates = 100):
        
    start_time = time.time()

    train_errors = np.zeros(epoch) 
    test_errors = np.zeros(epoch)

    n_1 = len(data_1)
    n_2 = len(data_2) 

    for s in range(epoch):
        train_idx_1, test_idx_1, train_label_1, test_label_1, train_idx_2, \
        test_idx_2, train_label_2, test_label_2 = train_test(data_1, data_2, test_size)

        train = np.concatenate((data_1[train_idx_1], data_2[train_idx_2]), 0)
        test = np.concatenate((data_1[test_idx_1], data_2[test_idx_2]), 0)
        train_labels = np.concatenate((train_label_1, train_label_2), axis = 0)
        test_labels = np.concatenate((test_label_1, test_label_2), axis = 0)
        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()

        x_preds = np.zeros((maj_num, len(train)))
        y_preds = np.zeros((maj_num, len(test)))
        
        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        train_labels = train_labels[I]
        
        J = np.arange(len(test))
        np.random.shuffle(J)
        test = test[J]
        test_labels = test_labels[J]

        for t in range(maj_num):

            Q_list = []
            temp_errors = []
            mu_temp = []

            for j in range(init_iter):
                B = initialize_Q(data_1[train_idx_1], data_2[train_idx_2], 
                                 std_coeff, out_channels, kernel_size, Q_size, 
                                 padding, learning_rate = 1e-3, bias = True, 
                                 D_out=2, lr_decay = 0.9, Num_updates = 100)

                Q_list.append(B[0])
                mu_temp.append(B[2])
                temp_errors.append(B[-1])

            h = np.argmin(temp_errors)
            Q = Q_list[h]
            mu = mu_temp[h]

            train_data = torch.from_numpy(ExpCurve2Vec(Q, train, mu)).float()
            train_data = train_data.view(len(train_data), 1, len(train_data[0]))

            test_data = torch.from_numpy(ExpCurve2Vec(Q, test, mu)).float()
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))

            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                    nn.ReLU(),
                    #nn.LeakyReLU(0.01),
                    #nn.Tanh(),
                    Flatten(),
                    nn.Linear(out_channels * (len(train_data[0][0]) - kernel_size + 1 + 2 * padding), 
                              D_out)
                    )
        
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)

                if (k+1) % 10 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
                optimizer.zero_grad()
                loss.backward() 
                optimizer.step() 

            scores = model(train_data)
            
            x_preds[t] = torch.argmax(scores, axis=1).detach().numpy()
            y_preds[t] = torch.argmax(model(test_data), axis=1).detach().numpy()

        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))

        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
                            np.round(train_error_mean, decimals = 4), 
                            np.round(test_error_mean, decimals = 4),
                            np.round(test_error_std, decimals = 4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                                 columns=['Channel 1', 'Learning Rate', 
                                          'Train Error', 'Test Error', 'Std Error'])

    print(colored(f"total time = {time.time() - start_time}", "red"))
    print("mu =", mu)

    return pdf

"""## With num_maj =11"""

MD_NeuralNetworkClassificationCNN(cars, buses, maj_num=11, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 10, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 100)

MD_NeuralNetworkClassificationCNN(cars, buses, maj_num=11, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 20, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 100)

MD_NeuralNetworkClassificationCNN(cars, buses, maj_num=11, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 30, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 100)

MD_NeuralNetworkClassificationCNN(cars, buses, maj_num=11, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 40, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 100)

MD_NeuralNetworkClassificationCNN(cars, buses, maj_num=11, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 50, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 100)

"""## With num_maj =1"""

MD_NeuralNetworkClassificationCNN(cars, buses, maj_num=1, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 10, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 1000)

MD_NeuralNetworkClassificationCNN(cars, buses, maj_num=1, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 20, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 1000)

MD_NeuralNetworkClassificationCNN(cars, buses, maj_num=1, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 30, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 1000)

MD_NeuralNetworkClassificationCNN(cars, buses, maj_num=1, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 40, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 1000)

MD_NeuralNetworkClassificationCNN(cars, buses, maj_num=1, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 50, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 1000)

"""## Adding physical features

### Only Physical features
"""

def CNN_classification_physical_features(data_1, data_2, lr_decay = 0.9, 
                                learning_rate = 1e-3, out_channels = 10, kernel_size = 10,
                                padding = 1, bias = True, Num_updates = 1000, D_out = 2, 
                                epoch = 50, sigma = 1, Leng=True, spd=True, accn=True, 
                                jrk=True, test_size=0.3):
    """ 
    in_channels: the dimension of hidden layer
    D_out: output dimension
    version: 'signed' or 'unsigned' or 'exp' 
    stride: should be fixed to 1
    """
        
    start_time = time.time()
    
    train_errors = np.ones(epoch) 
    test_errors = np.ones(epoch)
    losses = torch.zeros(epoch, Num_updates)

    data = get_features(data_1, data_2, Leng=Leng, spd=spd, accn=accn, jrk=jrk)
    data1 = data[:len(data_1)]
    data2 = data[len(data_1):]

    for s in range(epoch):
        train_idx_1, test_idx_1, train_labels_1, test_labels_1, \
            train_idx_2, test_idx_2, train_labels_2, test_labels_2 \
                = train_test(data1, data2, test_size=test_size)

        train1 = data1[train_idx_1]
        train2 = data2[train_idx_2]
        test1 = data1[test_idx_1]
        test2 = data2[test_idx_2]

        I = np.arange(len(train1) + len(train2))
        np.random.shuffle(I)
        J = np.arange(len(test1) + len(test2))
        np.random.shuffle(J)

        train = np.concatenate((train1, train2), 0)[I]
        test = np.concatenate((test1, test2), 0)[J]
        train_labels = np.concatenate((train_labels_1, train_labels_2), 0)[I]
        test_labels = np.concatenate((test_labels_1, test_labels_2), 0)[J]
        
        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()

        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                Flatten(),
                                nn.Linear(out_channels * (len(train[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )
    
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

        train = torch.from_numpy(train).float()
        test = torch.from_numpy(test).float()

        train = train.view(len(train), 1, len(train[0]))
        
        for k in range(Num_updates):
            x_pred = model(train) # of shape (N, D_out)
            loss = loss_fn(x_pred, train_labels)
            losses[s, k] = loss

            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

            optimizer.zero_grad()

            loss.backward() # Backward pass

            optimizer.step()  # Calling the step function on the Optimizer 
        
        x_preds = torch.argmax(model(train), axis=1)
        test = test.view(len(test), 1, len(test[0]))        
        y_preds = torch.argmax(model(test), axis=1)
        
        train_errors[s] = sum(abs(train_labels - x_preds)).detach().numpy()/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds)).detach().numpy()/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN": [np.round(train_error_mean, decimals=4), 
                    np.round(test_error_mean, decimals=4),
                    np.round(test_error_std, decimals=4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index',
                                 columns = ['Train Error', 'Test Error', 'Std Error'])
    
    plt.plot(losses[-1].detach().numpy())
    plt.show()
        
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf

CNN_classification_physical_features(cars_time, buses_time, lr_decay = 0.9, 
                                learning_rate = 1e-3, out_channels = 10, kernel_size = 2,
                                padding = 1, bias = True, Num_updates = 1000, D_out = 2, 
                                epoch = 50, sigma = 1, Leng=True, spd=True, accn=True, 
                                jrk=True, test_size=0.3)

"""## CNN with $v_Q^+$, $v_Q^{\varsigma+}$, $v_Q^{exp+}$ which are random Q and physical features"""

def get_features_Q(train_1, train_2, version='unsigned', sigma=1, Q_size=20, 
                   Leng=True, spd=True, accn=True, jrk=True):

    train = np.concatenate((train_1, train_2), 0)
    n = len(train)
    train_traj_1 = np.array([train_1[i][:,:2] for i in range(len(train_1))])
    train_traj_2 = np.array([train_2[i][:,:2] for i in range(len(train_2))])
    train_traj = np.array([train[i][:,:2] for i in range(n)])

    Mean = np.mean([np.mean(train_traj[i], 0) for i in range(n)], 0)
    Std = np.std([np.std(train_traj[i], 0) for i in range(n)], 0)
    Q = np.ones((Q_size,2))
    Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], Q_size)
    Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], Q_size)

    if version == 'exp':
        mu = get_mu(train_traj_1, train_traj_2)
        A = ExpCurve2Vec(Q, train_traj, mu)
    else:
        A = np.array(curve2vec(Q, train_traj, version=version, sigma=sigma))
    
    if Leng == True:
        length_ = np.array([length(train_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A, Q

def get_test_data_Q(test_1, test_2, Q, version='unsigned', sigma=1, Leng=True, 
                    spd=True, accn=True, jrk=True):

    test = np.concatenate((test_1, test_2), 0)
    n = len(test)
    test_traj_1 = np.array([test_1[i][:,:2] for i in range(len(test_1))])
    test_traj_2 = np.array([test_2[i][:,:2] for i in range(len(test_2))])
    test_traj = np.array([test[i][:,:2] for i in range(n)])
    
    if version == 'exp':
        mu = get_mu(test_traj_1, test_traj_2)
        A = ExpCurve2Vec(Q, test_traj, mu)
    else:
        A = np.array(curve2vec(Q, test_traj, version=version, sigma=sigma))
    
    if Leng == True:
        length_ = np.array([length(test_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A

def CNN_classification_Q_physics(data1, data2, version='unsigned', sigma=1, Q_size=20, 
                             Leng=True, spd=True, accn=True, jrk=True, 
                             lr_decay = 0.9, learning_rate = 1e-3, out_channels = 10, 
                             kernel_size = 5, padding = 1, bias = True, 
                             Num_updates = 1000, D_out = 2, epoch = 50, test_size=0.3):

    start_time = time.time()
    train_errors = np.zeros(epoch)
    test_errors = np.zeros(epoch)
    losses = torch.zeros(epoch, Num_updates)

    for s in range(epoch):

        train_idx_1, test_idx_1, train_labels_1, test_labels_1, \
            train_idx_2, test_idx_2, train_labels_2, test_labels_2 \
            = train_test(data1, data2, test_size)

        train1 = data1[train_idx_1]
        train2 = data2[train_idx_2]
        test1 = data1[test_idx_1]
        test2 = data2[test_idx_2]

        train, Q = get_features_Q(train1, train2, version, sigma,
                                  Q_size, Leng, spd, accn, jrk)
        
        train_labels = np.concatenate((train_labels_1, train_labels_2), 0)

        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        train_labels = train_labels[I]
        train_labels = torch.from_numpy(train_labels).long()

        J = np.arange(len(test1)+len(test2))
        np.random.shuffle(J)

        test = get_test_data_Q(test1, test2, Q, version=version, sigma=sigma, 
                               Leng=Leng, spd=spd, accn=accn, jrk=jrk)[J]
        test_labels = np.concatenate((test_labels_1, test_labels_2), 0)[J]
        test_labels = torch.from_numpy(test_labels).long()

        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                Flatten(),
                                nn.Linear(out_channels * (len(train[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )
    
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

        train = torch.from_numpy(train).float()
        test = torch.from_numpy(test).float()

        train = train.view(len(train), 1, len(train[0]))
        
        for k in range(Num_updates):
            x_pred = model(train) # of shape (N, D_out)
            #print(x_pred, train_labels)
            #print(x_pred.shape, train_labels.shape)
            loss = loss_fn(x_pred, train_labels)
            losses[s, k] = loss

            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

            optimizer.zero_grad()

            loss.backward() # Backward pass

            optimizer.step()  # Calling the step function on the Optimizer 

        x_preds = torch.argmax(model(train), axis=1)
        test = test.view(len(test), 1, len(test[0]))        
        y_preds = torch.argmax(model(test), axis=1)
        
        train_errors[s] = sum(abs(train_labels - x_preds)).detach().numpy()/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds)).detach().numpy()/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN": [np.round(train_error_mean, decimals=4), 
                    np.round(test_error_mean, decimals=4),
                    np.round(test_error_std, decimals=4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index',
                                 columns = ['Train Error', 'Test Error', 'Std Error'])
    
    plt.plot(losses[-1].detach().numpy())
    plt.show()
        
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf

"""## $v_Q^+$"""

CNN_classification_Q_physics(cars_time, buses_time, version='unsigned', sigma=1, 
                             Q_size=20, Leng=True, spd=True, accn=True, jrk=True, 
                             lr_decay = 0.9, learning_rate = 1e-3, out_channels = 10, 
                             kernel_size = 5, padding = 1, bias = True, 
                             Num_updates = 1000, D_out = 2, epoch = 50, test_size=0.3)

"""## $v_Q^{\varsigma+}$"""

CNN_classification_Q_physics(cars_time, buses_time, version='signed', sigma=1, 
                             Q_size=20, Leng=True, spd=True, accn=True, jrk=True, 
                             lr_decay = 0.9, learning_rate = 1e-3, out_channels = 10, 
                             kernel_size = 5, padding = 1, bias = True, 
                             Num_updates = 1000, D_out = 2, epoch = 50, test_size=0.3)

"""## Using Perceptron-like algorithm with physical features"""

#
def initialize_Q_(train_1, train_2, Q_size, mu_coeff, out_channels, kernel_size, padding, 
             learning_rate = 1e-3, bias = True, D_out=2, lr_decay = 0.9, Leng=True, 
             spd=True, accn=True, jrk=True, Num_updates = 100):
    
    start_time = time.time()
    train = np.concatenate((train_1, train_2), 0)
    n = len(train)
    train_traj_1 = np.array([train_1[i][:,:2] for i in range(len(train_1))])
    train_traj_2 = np.array([train_2[i][:,:2] for i in range(len(train_2))])
    train_traj = np.array([train[i][:,:2] for i in range(n)])
    train_labels = np.array([1] * len(train_1) + [-1] * len(train_2))
    
    A = train_labels.copy().reshape(-1,1)
    if Leng == True:
        length_ = np.array([length(train_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)
    
    train_labels = torch.from_numpy(train_labels).long()

# Make the classifier
    
    mu = get_mu(train_traj_1, train_traj_2) * mu_coeff
    std = mu/2
    errors = []
    Q = []

    if sum([Leng, spd, accn, jrk]) > 0:
        Train_0 = A[:,1:]
        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                Flatten(),
                                nn.Linear(out_channels * (len(Train_0[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )
        
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

        Train_0 = torch.from_numpy(Train_0).float() # shape = (120, 4)
        Train_0 = Train_0.view(len(Train_0), 1, len(Train_0[0]))
        
        for k in range(Num_updates):
            x_pred = model(Train_0) # of shape (N, D_out)
            #print(x_pred, train_labels)
            #print(x_pred.shape, train_labels.shape)
            loss = loss_fn(x_pred, train_labels)
            

            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

            optimizer.zero_grad()
            loss.backward() # Backward pass
            optimizer.step()  # Calling the step function on the Optimizer 
        print("*")
        #x_preds = torch.argmax(model(train), axis=1)
        index = torch.argmax(torch.max(model(train), axis=1))
        print(index)

        #train_error = sum(abs(train_labels - x_preds)).detach().numpy()/len(train_labels)    
        #errors.append(train_error) 
        
        #if model in ['LSVM', 'GSVM', 'PSVM', 'LR', 'Prn']:
        #    index = np.argmax(np.max(Model.decision_function(Train_0)))
        #else:
        #    probs = Model.predict_proba(Train_0)
        #    index = np.argmax(entropy(probs, axis=1))
    else:
        index = np.random.randint(0, high=n) 
    k = np.random.randint(0, high=len(train_traj[index]))
    q = train_traj[index][k] + np.random.normal(0, std, 2)
    Q.append(q)
    print(Q)

# Iteratively choose landmarks
    for i in range(1, Q_size):
        if sum([Leng, spd, accn, jrk]) > 0:
            Train_0 = A[:,1:]
            Train = np.concatenate((Train_0, np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))), 1)
        else:
            Train = np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))

        Model.fit(Train, train_labels)
        train_pred = Model.predict(Train)
        error = 1 - metrics.accuracy_score(train_labels, train_pred)
        errors.append(error)
        if model in ['LSVM', 'GSVM', 'PSVM', 'LR', 'Prn']:
            index = np.argmax(np.max(Model.decision_function(Train)))
        else:
            probs = Model.predict_proba(Train)
            index = np.argmax(entropy(probs, axis=1))

        k = np.random.randint(0, high=len(train_traj[index]))
        q = train_traj[index][k] + np.random.normal(0, std, 2)
        Q.append(q)
    if sum([Leng, spd, accn, jrk]) > 0:
        Train = np.concatenate((Train_0, np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))), 1)
    else: 
        Train = ExpCurve2Vec(np.array(Q), train_traj, mu)
    Model.fit(Train, train_labels)
    train_pred = Model.predict(Train)

    error = 1 - metrics.accuracy_score(train_labels, train_pred)
    errors.append(error)

    print(colored(f"Total time for mapping row data: {time.time() - start_time}", 'green'))

    return np.array(Q), mu, np.array(errors), error

def train_test(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size = test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size = test_size)

    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
                train_idx_2, test_idx_2, train_label_2, test_label_2

def get_features_(data_time, Leng=True, spd=True, accn=True, jrk=True):
    
    n = len(data_time)
    data_traj = np.array([data_time[i][:,:2] for i in range(len(data_time))])
    data_labels = np.ones(n)
    A = data_labels.copy().reshape(-1,1)

    if Leng == True:
        length_ = np.array([length(data_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(data_time[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(data_time[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(data_time[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A[:, 1:]

def MD_Pys_NeuralNetworkClassificationCNN(data_1_time, data_2_time, 
                                          maj_num, epoch, init_iter, std_coeff, 
                                          out_channels, kernel_size, Q_size, padding, 
                                          learning_rate=1e-3, bias=True, D_out=2, 
                                          lr_decay=0.9, test_size=0.3, Num_updates=100, 
                                          Leng=True, spd=True, accn=True, jrk=True):
        
    start_time = time.time()

    train_errors = np.zeros(epoch) 
    test_errors = np.zeros(epoch)
    
    data_1 = np.array([data_1_time[i][:,:2] for i in range(len(data_1_time))])
    data_2 = np.array([data_2_time[i][:,:2] for i in range(len(data_2_time))])

    n_1 = len(data_1)
    n_2 = len(data_2) 
    
    data_1_phys = get_features_(data_1_time, Leng, spd, accn, jrk)
    data_2_phys = get_features_(data_2_time, Leng, spd, accn, jrk)
    
    for s in range(epoch):
        train_idx_1, test_idx_1, train_label_1, test_label_1, train_idx_2, \
        test_idx_2, train_label_2, test_label_2 = train_test(data_1, data_2, test_size)

        train = np.concatenate((data_1[train_idx_1], data_2[train_idx_2]), 0)
        test = np.concatenate((data_1[test_idx_1], data_2[test_idx_2]), 0)
        train_labels = np.concatenate((train_label_1, train_label_2), axis = 0)
        test_labels = np.concatenate((test_label_1, test_label_2), axis = 0)
        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()
        
        data_phys_train = np.concatenate((data_1_phys[train_idx_1], data_2_phys[train_idx_2]), 0)
        data_phys_test = np.concatenate((data_1_phys[test_idx_1], data_2_phys[test_idx_2]), 0)
        
        x_preds = np.zeros((maj_num, len(train)))
        y_preds = np.zeros((maj_num, len(test)))
        
        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        data_phys_train = data_phys_train[I]
        train_labels = train_labels[I]
        
        J = np.arange(len(test))
        np.random.shuffle(J)
        test = test[J]
        data_phys_test = data_phys_test[J]
        test_labels = test_labels[J]

        for t in range(maj_num):

            Q_list = []
            temp_errors = []
            mu_temp = []

            for j in range(init_iter):
                B = initialize_Q(data_1[train_idx_1], data_2[train_idx_2], 
                                 std_coeff, out_channels, kernel_size, Q_size, 
                                 padding, learning_rate = 1e-3, bias = True, 
                                 D_out=2, lr_decay = 0.9, Num_updates = 100)

                Q_list.append(B[0])
                mu_temp.append(B[2])
                temp_errors.append(B[-1])

            h = np.argmin(temp_errors)
            Q = Q_list[h]
            mu = mu_temp[h]

            train_data = np.concatenate((ExpCurve2Vec(Q, train, mu), data_phys_train), 1)
            train_data = torch.from_numpy(train_data).float()
            train_data = train_data.view(len(train_data), 1, len(train_data[0]))

            test_data = np.concatenate((ExpCurve2Vec(Q, test, mu), data_phys_test), 1)
            test_data = torch.from_numpy(test_data).float()
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))

            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                    nn.ReLU(),
                    Flatten(),
                    nn.Linear(out_channels * (len(train_data[0][0]) - kernel_size + 1 + 2 * padding), 
                              D_out)
                    )
        
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)

                if (k+1) % 50 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
                optimizer.zero_grad()
                loss.backward() 
                optimizer.step() 

            scores = model(train_data)
            
            x_preds[t] = torch.argmax(scores, axis=1).detach().numpy()
            y_preds[t] = torch.argmax(model(test_data), axis=1).detach().numpy()

        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))

        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
                            np.round(train_error_mean, decimals = 4), 
                            np.round(test_error_mean, decimals = 4),
                            np.round(test_error_std, decimals = 4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                                 columns=['Channel 1', 'Learning Rate', 
                                          'Train Error', 'Test Error', 'Std Error'])

    print(colored(f"total time = {time.time() - start_time}", "red"))
    print("mu =", mu)

    return pdf

MD_Pys_NeuralNetworkClassificationCNN(cars_time, buses_time, 
                                      maj_num=1, epoch=50, init_iter=3, 
                                      test_size=0.3, std_coeff=1, out_channels=10, 
                                      kernel_size=5, Q_size=10, padding=1, 
                                      learning_rate = 1e-2, bias = True, D_out=2, 
                                      lr_decay = 0.9, Num_updates = 1000)

"""# Plot test errors"""

def lower_limit_error(x, y):
    if y - x < 0:
        return y
    else:
        return x

lower_limit_error = np.vectorize(lower_limit_error)

classifiers = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

FM = (r'KNN', r'Vote(MD $v_Q^{exp}$)', r'Vote(Rand $v_Q$)', r'Vote($v_Q^{\varsigma}$)', 
      r'Vote(Rand $v_Q^{exp}$)', r'MD $v_Q^{exp}$', r'Rand $v_Q$', 
      r'Rand $v_Q^{\varsigma}$', r'Rand $v_Q^{exp}$', 'Endpoints')

labels = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

dists = [r'$d_F$', r'$d_{dF}$', r'$dH$', r'DTW', r'soft-dtw', r'fastdtw', 
         r'LCSS', r'SSPD', r'EDR', r'ERP', r'LSH', r'$d_Q^{\pi}$']

A_LSVM = [0.1957, 0.2827, 0.4222, 0.2276, 0.2238, 0.2832, 0.4335, 0.2686, 0.4654]
A_GSVM = [0.1724, 0.1800, 0.2173, 0.1859, 0.1762, 0.2000, 0.2389, 0.2259, 0.2119]
A_PSVM = [0.1778, 0.3535, 0.3519, 0.2119, 0.1762, 0.3724, 0.3714, 0.2330, 0.2573]
A_DT = [0.1908, 0.1735, 0.1811, 0.1827, 0.2189, 0.2189, 0.2130, 0.2178, 0.1897]
A_RF = [0.1546, 0.1568, 0.1751, 0.1659, 0.1724, 0.1778, 0.1876, 0.1914, 0.1611]
A_KNN = [0.2292, 0.2497, 0.3395, 0.2265, 0.2530, 0.2643, 0.3443, 0.2476, 0.2481]
A_LR = [0.2427, 0.3795, 0.3795, 0.2773, 0.2573, 0.3784, 0.3805, 0.2881, 0.3784]
A_CNN = [0.2081, 0.1805, 0.2503, 0.2032, 0.2038, 0.1881, 0.1778, 0.2168, 0.3784]

A = np.array([A_LSVM, A_GSVM, A_PSVM, A_DT, A_RF, A_KNN, A_LR, A_CNN])
B = A.T

std_LSVM = [0.0581, 0.0711, 0.0604, 0.0723, 0.0688, 0.0732, 0.0764, 0.0808, 0.0695]
std_GSVM = [0.0628, 0.0596, 0.0747, 0.0594, 0.0562, 0.0510, 0.0678, 0.0787, 0.0475]
std_PSVM = [0.0541, 0.0628, 0.0417, 0.0555, 0.0647, 0.0243, 0.0526, 0.0606, 0.0387]
std_DT = [0.0578, 0.0538, 0.0660, 0.0470, 0.0699, 0.0667, 0.0689, 0.0750, 0.0625]
std_RF = [0.0557, 0.0504, 0.0557, 0.0487, 0.0639, 0.0633, 0.0588, 0.0639, 0.0611]
std_KNN = [0.0649, 0.0586, 0.0924, 0.0690, 0.0644, 0.0568, 0.0837, 0.0697, 0.0556]
std_LR = [0.0427, 0.0076, 0.0053, 0.0474, 0.0555, 0.0000, 0.0091, 0.0613, 0.0000]
std_CNN = [0.0709, 0.0576, 0.0800, 0.0598, 0.0780, 0.0569, 0.0597, 0.0706, 0.0000]

C = np.array([std_LSVM, std_GSVM, std_PSVM, std_DT, std_RF, std_KNN, std_LR, std_CNN]).T

# test errors and stds for KNN
errors = [0.2573, 0.2557, 0.2405, 0.2589, 0.2535, 0.2438, 0.3022, 0.2200, 0.3124, 
          0.3335, 0.2968, 0.2514]

stds = [0.0633, 0.0642, 0.0663, 0.0682, 0.0698, 0.0702, 0.0677, 0.0465, 0.0774,
        0.0555, 0.0931, 0.0636]

width=0.1
index = np.arange(len(stds))
ind = np.arange(len(A)) + len(index) * 0.25

plt.subplots(figsize = (16, 6.5), tight_layout=True)
bars = [0] * 10

lower_lim = lower_limit_error(stds, errors)
bars[0] = plt.bar(index * 0.2, errors, width, yerr=[lower_lim, stds], capsize=2)

for i in range(len(FM)-1):
    lower_limit = lower_limit_error(C[i], B[i])
    bars[i+1] = plt.bar(ind+width*i-0.3, B[i], width, yerr=[lower_limit, C[i]], capsize=2)

plt.title('Car-Bus Data')
plt.xticks(list(np.arange(len(stds)) * 0.2) + list(ind + 0.5 * width), 
           dists + labels, fontsize = 10)

plt.legend(tuple(bars), FM, loc=0)

plt.gca().yaxis.grid(color='gray', linestyle='dotted', linewidth=0.6)
plt.xticks(rotation='vertical')

path = '/content/gdrive/My Drive/plots/plots car-bus/car-bus bar chart all horizontal.csv'

plt.savefig(path, bbox_inches='tight', dpi=200)

plt.show()

"""# Plot of Physical feature mapping test errors"""

A_LSVM = [0.0314, 0.0195, 0.0259, 0.0389]
A_GSVM = [0.0692, 0.0638, 0.0935, 0.0816]
A_PSVM = [0.3341, 0.1141, 0.1114, 0.0649]
A_DT = [0.0784, 0.0730, 0.0670, 0.0584]
A_RF = [0.0795, 0.0530, 0.0897, 0.0568]
A_KNN = [0.0773, 0.0870, 0.0832, 0.0870]
A_LR = [0.0465, 0.0384, 0.0432, 0.0519]
A_CNN = [0.0497, 0.0432, 0.0395, 0.0503]

A = np.array([A_LSVM, A_GSVM, A_PSVM, A_DT, A_RF, A_KNN, A_LR, A_CNN])
B = A.T

std_LSVM = [0.0317, 0.0259, 0.0247, 0.0347]
std_GSVM = [0.0496, 0.0381, 0.0481, 0.0420]
std_PSVM = [0.0737, 0.0740, 0.0852, 0.0320]
std_DT = [0.0440, 0.0437, 0.0372, 0.0372]
std_RF = [0.0440, 0.0393, 0.0447, 0.0330]
std_KNN = [0.0412, 0.0460, 0.0445, 0.0409]
std_LR = [0.0367, 0.0383, 0.0394, 0.0370]
std_CNN = [0.0395, 0.0354, 0.0302, 0.0397]

C = np.array([std_LSVM, std_GSVM, std_PSVM, std_DT, std_RF, std_KNN, std_LR, std_CNN]).T

FM = (r'Rand $v_Q^+$', r'Rand $v_Q^{\varsigma +}$', r'MD^+$', 'Physical Features')
labels = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

ind = np.arange(len(A))
width = 0.2
plt.subplots(figsize = (6, 4), tight_layout=True)
bars = [0] * 4

for i in range(len(FM)):
    lower_limit = lower_limit_error(C[i], B[i])
    bars[i] = plt.bar(ind+width*i, B[i], width, yerr=[lower_limit, C[i]], 
                       ecolor='black', capsize=2)

plt.title('Car-Bus with physical features')
plt.xticks(ind+1.5 * width, labels)
plt.legend(tuple(bars), FM, loc=0)

plt.gca().yaxis.grid(color='gray', linestyle='dotted', linewidth=0.6)

path = '/content/gdrive/My Drive/plots/plots car-bus-physical/Physical car-bus bar chart.csv'

plt.savefig(path, bbox_inches='tight', dpi=200)

plt.show()

