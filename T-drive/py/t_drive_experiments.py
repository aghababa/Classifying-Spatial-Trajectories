# -*- coding: utf-8 -*-
"""T-Drive_Experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v0EQx3A2HhvK4HegN7wl1llIEwMPjME1

# Libraries
"""

import glob
import numpy as np 
import time
import math
import random
from scipy import linalg as LA
import pandas as pd
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.svm import NuSVC
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import Perceptron
import statsmodels.api as sm
from autograd import grad
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
from termcolor import colored
import matplotlib as mpl
import matplotlib.pyplot as plt
from scipy import optimize
from sklearn.svm import LinearSVC
from termcolor import colored
from datetime import datetime
import os
import similaritymeasures
import tslearn
from tslearn.metrics import dtw
from scipy.spatial.distance import directed_hausdorff
from frechetdist import frdist
import trjtrypy as tt
from trjtrypy.distances import d_Q
from trjtrypy.distances import d_Q_pi
import trjtrypy.visualizations as vs
from scipy.spatial import distance
from trjtrypy.featureMappings import curve2vec

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import sampler
import torchvision.transforms as T

"""# Read Data"""

I = glob.glob('T-Drive/**/*.txt', recursive=True)

# runtime about 35s
Start_time = time.time()
J = [0] * len(I)

for i in range(len(I)):
    for j in range(len(I)):
        if int(I[j][28:-4]) == i+1:
            J[i] = I[j]
print('total time =', time.time() - Start_time)

taxi_id = np.array([J[i][28:-4] for i in range(len(J))])
I = np.sort(list(map(int, taxi_id)))
True in I == list(map(int, taxi_id)) # so the taxi ids are nn.arange(len(J))

len(J)

# create 'daysDate' function to convert start and end time to a float number of days

def days_date(time_str):
    date_format = "%Y-%m-%d %H:%M:%S"
    current = datetime.strptime(time_str, date_format)
    date_format = "%Y-%m-%d"
    bench = datetime.strptime('2006-12-30', date_format)
    no_days = current - bench
    delta_time_days = no_days.days + current.hour / 24.0 + current.minute / (24. * 60.) + current.second / (24. * 3600.)
    return delta_time_days

days_date = np.vectorize(days_date)
float1 = np.vectorize(float)

def read_file(file_name):
    data = []
    f = open(file_name, "r")
    for line in f:
        item = line.strip().split(",")
        if len(item) ==4:
            data.append(np.asarray(item[1:]))
    data = np.array(data)
    return data

"""### Remove stationary points"""

def remove_segments(traj): # removes stationary points
    p2 = traj[:,1:][1:]
    p1 = traj[:,1:][:-1]
    L = ((p2-p1)*(p2-p1)).sum(axis =1)
    I = np.where(L>1e-16)[0]
    return traj[I]

"""### In data1 below:
    (1) there is no stationary points
    (2) there is no trajectories with less than 1 or more than 200 waypoints
"""

# runtime about 350s
Start_time = time.time()
T = len(J)
data1 = [] 
taxi_ids = []
for i in range(T):
    a = read_file(J[i])
    if len(a) > 1000:  
        a[:,0] = days_date(a[:,0])
        a = float1(a)
        data1.append(remove_segments(a)) 
        taxi_ids.append(i)

taxi_ids = np.array(taxi_ids)   
data1 = np.array(data1, dtype='object')
I = np.where(np.array([len(data1[i]) for i in range(len(data1))]) > 0)[0]
data1 = data1[I]
taxi_ids = np.array(taxi_ids[I], dtype='object')
print('total time =', time.time() - Start_time)
print("len(data1) =", len(data1))
print("selected taxi_ids: \n", taxi_ids)

def partition(trajectory, threshold=20):
    trajectories = []
    a = 24 * 60 * (trajectory[-1][0] - trajectory[0][0])
    if a <= threshold:
        return np.array(trajectory.reshape(1, len(trajectory), 3))
    else: 
        i = 0
        while a > threshold:
            j = i + 0
            val = 0
            while val < threshold: 
                if i < len(trajectory) - 1:
                    temp = val + 0
                    val += 24 * 60 * (trajectory[:,0][1:][i] - trajectory[:,0][:-1][i])
                    i += 1
                else: 
                    break
            if len(trajectory[j:i-1]) > 0:
                trajectories.append(trajectory[j:i-1])
            a = a - val
        if len(trajectory[i:]) > 0:
            trajectories.append(trajectory[i:])
    trajectories = np.array(trajectories, dtype='object')
    return trajectories

"""### In data3 below 
    1) any taxi has between 100 and 200 trajectories
    2) in each taxi dataset any trajectory has less than 20 minutes time
"""

Start_time = time.time()
data3 = []
idxs = []
for i in range(len(data1)):
    D = []
    A = partition(data1[i], threshold=20)
    for j in range(len(A)):
        if ((len(A[j]) >= 10) and (len(A[j]) <= 200)):
            D.append(A[j])
    if (len(D) >= 100 and len(D) <= 200):
        data3.append(np.array(D, dtype='object'))
        idxs.append(i)

taxi_idxs = taxi_ids[idxs]
data3 = np.array(data3, dtype='object')
taxi_idxs = np.array(taxi_idxs)

print('total time =', time.time() - Start_time)
print("len(data3) =", len(data3))
print("len(taxi_idxs) =", len(taxi_idxs))

A = np.array([len(data3[i]) for i in range(len(data3))])
print(colored(f"min and max length of selected users: {min(A), max(A)}", "green"))

print("number of selected users:", len(data3))
print("idxs: \n", idxs)
print(colored(f"selected taxi ids: \n {taxi_idxs}", "blue"))

data = data3
Min = np.min([np.min([np.min(data[i][j][:,1:], 0) for j in range(len(data[i]))], 0) for i in range(len(data))], 0)
Max = np.max([np.max([np.max(data[i][j][:,1:], 0) for j in range(len(data[i]))], 0) for i in range(len(data))], 0)
Mean = np.mean([np.mean([np.mean(data[i][j][:,1:], 0) for j in range(len(data[i]))], 0) for i in range(len(data))], 0)
Std = np.std([np.std([np.std(data[i][j][:,1:], 0) for j in range(len(data[i]))], 0) for i in range(len(data))], 0)
Min, Mean, Max, Std

pairs = []
for i in range(len(data)-1):
    for j in range(i+1, len(data)):
        pairs.append([i,j])
pairs = np.array(pairs)
print(len(pairs))

#I = np.arange(len(pairs))
#np.random.shuffle(I)
#pairs_100 = pairs[I[:100]]
# we did above and chose the following pairs
pairs_100 = np.array([[ 87, 166], [161, 396], [370, 425], [209, 421], [ 84, 278], 
                      [ 24,  95], [ 72, 249], [358, 419], [ 86, 382], [241, 297],
                      [315, 363], [273, 335], [118, 270], [323, 430], [ 30, 431],
                      [111, 297], [ 17, 103], [191, 210], [179, 240], [ 53, 139],
                      [156, 240], [128, 412], [  8, 166], [ 96,  98], [270, 303],
                      [160, 319], [ 17,  41], [ 46, 381], [165, 216], [  8, 148],
                      [231, 287], [293, 426], [131, 264], [136, 271], [ 69, 137],
                      [120, 139], [111, 261], [ 13, 114], [138, 291], [ 31, 414], 
                      [252, 276], [ 38, 126], [ 31, 243], [382, 417], [240, 262],
                      [ 45,  49], [313, 359], [107, 206], [212, 243], [ 91, 383],
                      [118, 402], [ 31, 355], [ 74, 365], [110, 132], [136, 330],
                      [326, 376], [ 28, 128], [258, 302], [193, 256], [ 48, 151],
                      [255, 309], [340, 391], [ 90, 216], [341, 400], [ 45, 296], 
                      [206, 370], [184, 379], [248, 343], [ 16, 271], [245, 401],
                      [ 25, 130], [ 15, 430], [ 20, 285], [362, 385], [192, 343], 
                      [111, 410], [400, 414], [116, 185], [ 48, 202], [ 85, 234], 
                      [281, 392], [318, 320], [ 85, 417], [390, 427], [357, 428], 
                      [150, 189], [ 40, 343], [173, 249], [ 38, 399], [121, 393], 
                      [152, 316], [  0, 141], [ 24, 397], [255, 424], [ 92, 330], 
                      [121, 245], [ 78, 240], [ 68, 255], [ 21, 288], [278, 399]])

print(pairs_100.tolist())

taxi_ids_selected = list(set(pairs_100.reshape(200,)))
print("Chosen Users: \n", taxi_ids_selected)

print("Chosen Users From Data:")
taxi_idxs[taxi_ids_selected] + 1

data = data[taxi_ids_selected]
data.shape

"""# Classifiers"""

CC = [100, 100, 1000]
number_estimators = 50

clf1 = [make_pipeline(LinearSVC(dual=False, C=CC[0], tol=1e-5, 
                               class_weight ='balanced', max_iter=1000)), 
        "SVM, LinearSVC, C = "+str(CC[0])]
clf2 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[1], kernel='rbf', gamma='auto', max_iter=200000)),
        "Gaussian SVM, C="+str(CC[1])+", gamma=auto"]
clf3 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[2], kernel='poly', degree=3, max_iter=400000)),
        "Poly kernel SVM, C="+str(CC[2])+", deg=auto"]
clf4 = [DecisionTreeClassifier(), "Decision Tree"]
clf5 = [RandomForestClassifier(n_estimators=number_estimators), 
         "RandomForestClassifier, n="+str(number_estimators)]
clf6 = [KNeighborsClassifier(n_neighbors=5), "KNN"]
clf7 = [LogisticRegression(solver='lbfgs'), "Logistic Regression"]

clf = [clf1, clf2, clf3, clf4, clf5, clf6, clf7]

classifs = [item[0] for item in clf]
keys = [item[1] for item in clf]

"""# Chosen 5 pairs of Taxies"""

pairs_final = [[50, 94], [83, 144], [25, 80], [43, 53], [12, 82]]
pairs_final

"""# Experiments on 5 chosen pairs start from here

# Plot 50 trajectory from each user
"""

pairs_from_data = [(3142, 6834), (6168, 9513), (1950, 5896), (2876, 3260), (1350, 5970)]
pairs_final = [[50, 94], [83, 144], [25, 80], [43, 53], [12, 82]]
path = '/Users/hasan/Desktop/Anaconda/Research/Pictures for 2ed paper/'

for k in range(len(pairs_final)):
    m, n = pairs_final[k]
    r, s = pairs_from_data[k]
    for i in range(10,60):
        plt.plot(data[m][i][:,0], data[m][i][:,1], color='blue')
    for i in range(10,60):
        plt.plot(data[n][i][:,0], data[n][i][:,1], color='red')
    plt.title(f'50 trajectories from users {r}, {s} in T-drive data')
    plt.savefig(path+f'Taxi-pairs {pairs_from_data[k]}.png', bbox_inches='tight', 
                dpi=200)
    plt.show()

"""# Get length and width of the box containig pairs"""

pairs_from_data = [(3142, 6834), (6168, 9513), (1950, 5896), (2876, 3260), (1350, 5970)]
pairs_final = [[50, 94], [83, 144], [25, 80], [43, 53], [12, 82]]

for k in range(len(pairs_final)):
    m, n = pairs_final[k]
    r, s = pairs_from_data[k]
    min_x, min_y = np.min((np.min([np.min(data[m][i], axis=0) for i in range(len(data[m]))], axis=0), 
           np.min([np.min(data[n][i], axis=0) for i in range(len(data[n]))], axis=0)), axis=0)

    max_x, max_y = np.max((np.max([np.max(data[m][i], axis=0) for i in range(len(data[m]))], axis=0), 
                   np.max([np.max(data[n][i], axis=0) for i in range(len(data[n]))], axis=0)), axis=0)

    print(f"length and width of pair {r, s}:", np.round(max_x - min_x, decimals=2), 
          np.round(max_y - min_y, decimals=2))
    print("len(data[m]), len(data[n]):", len(data[m]), len(data[n]))
    print()

"""## KNN classification with 10 distances: (done)
    discret_frechet, hausdorff, dtw_tslearn, soft-dtw, fastdtw, 'lcss', sspd, edr, erp, d_Q_pi
"""

import numpy as np
import time
import pandas as pd
import random
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw
import traj_dist.distance as tdist
import pickle
import tslearn
from tslearn.metrics import dtw as dtw_tslearn
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from trjtrypy.distances import d_Q_pi
from sdtw import SoftDTW
from termcolor import colored
from KNN_Class import KNN

metrics = ['discret_frechet', 'hausdorff', dtw_tslearn, SoftDTW, fastdtw, 'lcss', 'sspd',
           'edr', 'erp', 'd_Q_pi']

path = 'Calculated Distance Matrices for KNN-T-drive/'

train_test_mean_median_std_KNN_errors = []
test_KNN_errors = []
std_KNN_errors = []

for pair in pairs_final:
    print('pair =', pair)
    temp = []
    for i in range(len(metrics)):
        KNN_class = KNN(data[pair[0]], data[pair[1]], metric=metrics[i], gamma=1e-14, 
                        eps_edr=0.005, eps_lcss=0.005, Q_size=20, Q=None, p=2, 
                        path=path+str(metrics[i])+'-'+str(pair), 
                        test_size=0.3, n_neighbors=5, num_trials=50, pair=[0,1])

        KNN_class.write_matrix_to_csv()
        A = KNN_class.KNN_average_error()
        temp.append(A[1:])        
        print(i, A[0])
        print("==========================================================================")
    
    train_test_mean_median_std_KNN_errors.append(temp)
    test_KNN_errors.append(np.array(temp)[:,1])
    std_KNN_errors.append(np.array(temp)[:,3])
    print(colored("******************************************************************************", 'red'))
    print(colored("******************************************************************************", 'red'))
    print(colored("******************************************************************************", 'red'))
    
train_test_mean_median_std_KNN_errors = np.array(train_test_mean_median_std_KNN_errors)
test_KNN_errors = np.array(test_KNN_errors)
std_KNN_errors = np.array(std_KNN_errors)

list_test_error = list(np.round(np.mean(test_KNN_errors, 0), decimals=4))
list_std_error = list(np.round(np.mean(std_KNN_errors, 0), decimals=4))
print(colored(f'average test errors of 5 pairs: \n {list_test_error}', 'magenta'))
print(colored(f'average stds of 5 pairs: \n {list_std_error}', 'yellow'))

"""# Choosing landmarks by the mistake deriven algorithm (done)

## Boost(MD $v_Q^{\exp}$) with epoch=50, majority=11 and init_num=3
"""

pairs_final = [[50, 94], [83, 144], [25, 80], [43, 53], [12, 82]]

from Perceptron_Like_Algo_Class import classification

test_errors = []
std_errors = []

m, n = pairs_final[0]
print(f"pair: {m, n}")
test_errors_0 = []
std_errors_0 = []

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, 
                    gamma='auto', classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                    std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    test_errors_0.append(np.array(A[0])[0][2])
    std_errors_0.append(np.array(A[0])[0][3])
    print(colored('=======================================================================', 'yellow'))
test_errors.append(test_errors_0)
std_errors.append(std_errors_0)
print(colored(f'test errors:     {test_errors_0}', 'magenta'))
print(colored(f'std test errors: {std_errors_0}', 'green'))

m, n = pairs_final[1]
print(f"pair: {m, n}")
test_errors_1 = []
std_errors_1 = []

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, 
                    gamma='auto', classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                    std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    test_errors_1.append(np.array(A[0])[0][2])
    std_errors_1.append(np.array(A[0])[0][3])
    print(colored('=======================================================================', 'yellow'))
    time.sleep(60)
test_errors.append(test_errors_1)
std_errors.append(std_errors_1)
print(colored(f'test errors:     {test_errors_1}', 'magenta'))
print(colored(f'std test errors: {std_errors_1}', 'green'))

m, n = pairs_final[2]
print(f"pair: {m, n}")
test_errors_2 = []
std_errors_2 = []

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, 
                    gamma='auto', classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                    std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    test_errors_2.append(np.array(A[0])[0][2])
    std_errors_2.append(np.array(A[0])[0][3])
    print(colored('=======================================================================', 'yellow'))
    time.sleep(60)
test_errors.append(test_errors_2)
std_errors.append(std_errors_2)
print(colored(f'test errors:     {test_errors_2}', 'magenta'))
print(colored(f'std test errors: {std_errors_2}', 'green'))

m, n = pairs_final[3]
print(f"pair: {m, n}")
test_errors_3 = []
std_errors_3 = []

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, 
                    gamma='auto', classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                    std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    test_errors_3.append(np.array(A[0])[0][2])
    std_errors_3.append(np.array(A[0])[0][3])
    print(colored('=======================================================================', 'yellow'))
    time.sleep(60)
test_errors.append(test_errors_3)
std_errors.append(std_errors_3)
print(colored(f'test errors:     {test_errors_3}', 'magenta'))
print(colored(f'std test errors: {std_errors_3}', 'green'))

m, n = pairs_final[4]
print(f"pair: {m, n}")
test_errors_4 = []
std_errors_4 = []

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, 
                    gamma='auto', classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                    std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    test_errors_4.append(np.array(A[0])[0][2])
    std_errors_4.append(np.array(A[0])[0][3])
    print(colored('=======================================================================', 'yellow'))
    time.sleep(60)
test_errors.append(test_errors_4)
std_errors.append(std_errors_4)
print(colored(f'test errors:     {test_errors_4}', 'magenta'))
print(colored(f'std test errors: {std_errors_4}', 'green'))

test_errors.append(test_errors_4)
std_errors.append(std_errors_4)
print(colored(f'test errors:     {test_errors_4}', 'magenta'))
print(colored(f'std test errors: {std_errors_4}', 'green'))

print(test_errors)
print(std_errors)

# average errors of 5 pairs reported in the paper (epoch=50, maj=11, init_iter=3)

test_errors = np.array(test_errors)
std_errors = np.array(std_errors)

print('average test error for 5 pairs:')
print(list(np.round(list(np.mean(test_errors, 0)), decimals=4)))
print()
print('average std error for 5 pairs:')
print(list(np.round(list(np.mean(std_errors, 0)), decimals=4)))

#average test error for 5 pairs:
#[0.2864, 0.2894, 0.2767, 0.276, 0.271, 0.2898, 0.2923]

#average std error for 5 pairs:
#[0.0418, 0.0458, 0.044, 0.0433, 0.043, 0.0426, 0.0373]

"""## MD $v_Q^{\exp}$ with epoch=50, majority=1 and init_num=3"""

test_errors_ave = []
std_errors_ave = []

for pair in pairs_final: 
    m, n = pair
    print(f"pair: {m, n}")
    
    test_errors_temp = []
    std_errors_temp = []

    for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
        classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, 
                        gamma='auto', classifiers=[], epoch=50, maj_num=1, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

        A = classif.classification_Q()
        print(A[0])
        print(colored(f"mu = {A[1]}", 'blue'))
        test_errors_temp.append(np.array(A[0])[0][2])
        std_errors_temp.append(np.array(A[0])[0][3])
        print(colored('=======================================================================', 'yellow'))
        time.sleep(10)
    time.sleep(60)
    print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')
    test_errors_ave.append(test_errors_temp)
    std_errors_ave.append(std_errors_temp)
    print(colored(f'test errors:     {test_errors_temp}', 'magenta'))
    print(colored(f'std test errors: {std_errors_temp}', 'green'))

print(colored('************************************************************************', 'red'))
print(colored('************************************************************************', 'red'))
print(colored('************************************************************************', 'red'))

# average errors of 5 pairs reported in the paper (epoch=50, maj=1, init_iter=3)

test_errors_ave = np.array(test_errors_ave)
std_errors_ave = np.array(std_errors_ave)

print('average test error for 5 pairs:', list(np.round(list(np.mean(test_errors_ave, 0)), decimals=4)))
print()
print('average std error for 5 pairs: ', list(np.round(list(np.mean(std_errors_ave, 0)), decimals=4)))

print('average test error for 5 pairs:', list(np.round(list(np.mean(test_errors_ave, 0)), decimals=4)))
print()
print('average std error for 5 pairs: ', list(np.round(list(np.mean(std_errors_ave, 0)), decimals=4)))

#average test error for 5 pairs: [0.2911, 0.2975, 0.2953, 0.3179, 0.2711, 0.2929, 0.3029]

#average std error for 5 pairs:  [0.0426, 0.0448, 0.0463, 0.0475, 0.0392, 0.0446, 0.0415]

"""# Classification with $v_Q$, $v_Q^{\varsigma}$, $v_Q^{\exp}$ and endpoints (done)

        Rand: on 50 epochs
        Boost: on 50 epochs and 11 majority votes
"""

from v_Q_mu_endpoints_classification import binaryClassificationAverageMajority

"""## Classification with $v_Q$ with random $Q$ in each iteration

### Vote($v_Q$)
"""

v_Q_maj_errors = []
v_Q_maj_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=11, classifiers=clf, 
                                            version='unsigned', test_size=0.3)
    A = classif.classification_v_Q()
    print(A[0])
    v_Q_maj_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_maj_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_maj_errors = \n', v_Q_maj_errors)
print('v_Q_maj_stds = \n', v_Q_maj_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_maj_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds: {list(np.round(np.mean(v_Q_maj_stds, 0), decimals=4))}', 'green'))

"""### Rand $v_Q$"""

v_Q_errors = []
v_Q_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=1, classifiers=clf, 
                                            version='unsigned', test_size=0.3)
    A = classif.classification_v_Q()
    print(A[0])
    v_Q_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_errors = \n', v_Q_errors)
print('v_Q_stds = \n', v_Q_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds: {list(np.round(np.mean(v_Q_stds, 0), decimals=4))}', 'green'))

"""## Classification with $v_Q^{\varsigma}$ with random $Q$ in each iteration

### Vote($v_Q^{\varsigma}$)
"""

v_Q_sigma_maj_errors = []
v_Q_sigma_maj_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=11, classifiers=clf, 
                                            version='signed', sigma=10, test_size=0.3)
    A = classif.classification_v_Q()
    print(A[0])
    v_Q_sigma_maj_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_sigma_maj_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_sigma_maj_errors = \n', v_Q_sigma_maj_errors)
print('v_Q_sigma_maj_stds = \n', v_Q_sigma_maj_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_sigma_maj_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds: {list(np.round(np.mean(v_Q_sigma_maj_stds, 0), decimals=4))}', 'green'))

"""### Rand $v_Q^{\varsigma}$"""

v_Q_sigma_errors = []
v_Q_sigma_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=1, classifiers=clf, 
                                            version='signed', sigma=10, test_size=0.3)
    A = classif.classification_v_Q()
    print(A[0])
    v_Q_sigma_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_sigma_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_sigma_errors = \n', v_Q_sigma_errors)
print('v_Q_sigma_stds = \n', v_Q_sigma_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_sigma_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds: {list(np.round(np.mean(v_Q_sigma_stds, 0), decimals=4))}', 'green'))

"""## Classification with $v_Q^{\exp}$ with random $Q$ in each iteration

### Vote($v_Q^{\exp}$)
"""

v_Q_mu_maj_errors = []
v_Q_mu_maj_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=11, classifiers=clf, 
                                            test_size=0.3)
    A = classif.classification_v_Q_mu()
    print(A[0])
    v_Q_mu_maj_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_mu_maj_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_mu_maj_errors = \n', v_Q_mu_maj_errors)
print('v_Q_mu_maj_stds = \n', v_Q_mu_maj_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_mu_maj_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds: {list(np.round(np.mean(v_Q_mu_maj_stds, 0), decimals=4))}', 'green'))

"""### Rand $v_Q^{\exp}$"""

v_Q_mu_errors = []
v_Q_mu_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=1, classifiers=clf, 
                                            test_size=0.3)
    A = classif.classification_v_Q_mu()
    print(A[0])
    v_Q_mu_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_mu_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_mu_errors = \n', v_Q_mu_errors)
print('v_Q_mu_stds = \n', v_Q_mu_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_mu_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds: {list(np.round(np.mean(v_Q_mu_stds, 0), decimals=4))}', 'green'))

"""# Endpoint classification"""

endpoint_errors = []
endpoint_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=1, classifiers=clf, 
                                            test_size=0.3)
    A = classif.endpoint_classification()
    print(A[0])
    endpoint_errors.append(np.round(A[2], decimals=4).tolist())
    endpoint_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('endpoint_errors = \n', endpoint_errors)
print('endpoint_stds = \n', endpoint_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(endpoint_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds:   {list(np.round(np.mean(endpoint_stds, 0), decimals=4))}', 'green'))

"""# Neural Networks (CNN)

## Helper functions
"""

from collections import Counter

def find_majority(votes):
    vote_count = Counter(votes)
    top = vote_count.most_common(1)
    return top[0][0]

def find_majority_array(A): # column-wise majority
    return list(map(find_majority, A.T))

def ExpCurve2Vec(points, curves, mu):
    D = tt.distsbase.DistsBase()
    a = np.array([np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves])
    return a

def get_mu(data_1, data_2):
    a = np.mean([np.mean(data_1[i], 0) for i in range(len(data_1))], 0)
    b = np.mean([np.mean(data_2[i], 0) for i in range(len(data_2))], 0)
    c = abs(a-b)
    return max(c)

def get_endpoints(data):
    n = len(data)
    data_endpoints = np.zeros((n, 4))
    for i in range(n):
        data_endpoints[i] = np.concatenate((data[i][0], data[i][-1]), 0)
    return data_endpoints

def train_test(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size = test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size = test_size)

    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
                train_idx_2, test_idx_2, train_label_2, test_label_2

def train_test_mu(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size=test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size=test_size)

    train_1 = data_1[train_idx_1]
    train_2 = data_2[train_idx_2]
    test_1 = data_1[test_idx_1]
    test_2 = data_2[test_idx_2]

    arr1 = np.arange(len(train_1)+len(train_2))
    I_1 = np.random.shuffle(arr1)

    arr2 = np.arange(len(test_1)+len(test_2))
    I_2 = np.random.shuffle(arr2)
    
    train = np.concatenate((train_1, train_2), 0)[arr1[I_1]]
    train_labels = np.concatenate((train_label_1, train_label_2), 0)[arr1[I_1]]
    test = np.concatenate((test_1, test_2), 0)[arr2[I_2]]
    test_labels = np.concatenate((test_label_1, test_label_2), 0)[arr2[I_2]]

    a = np.mean([np.mean(train_1[i], 0) for i in range(len(train_1))], 0)
    b = np.mean([np.mean(train_2[i], 0) for i in range(len(train_2))], 0)
    mu = max(abs(a-b))
    
    return mu, train[0], test[0], train_labels[0], test_labels[0]

def flatten(x):
    N = x.shape[0] 
    return x.view(N, -1)

class Flatten(nn.Module):
    def forward(self, x):
        return flatten(x)

"""## CNN for $v_Q$, $v_Q^{\varsigma}$, $v_Q^{exp}$, endpoints"""

def neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 1e-3, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 10, 
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 10, test_size = 0.3):
    
    """ 
    in_channels: the dimension of hidden layer
    D_out: output dimension
    version: 'signed' or 'unsigned' or 'exp' 
    stride: should be fixed to 1
    """

    start_time = time.time()

    train_errors = np.zeros(epoch)
    test_errors = np.zeros(epoch)

    losses = torch.zeros(epoch, num_trials_maj, Num_updates)
    
    for s in range(epoch):

        mu, train, test, train_labels, test_labels = train_test_mu(data_1, data_2, test_size)

        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()
        
        x_preds = torch.zeros(num_trials_maj, len(train))
        y_preds = torch.zeros(num_trials_maj, len(test))

        Min = np.min([np.min(train[i], 0) for i in range(len(train))], 0)
        Max = np.max([np.max(train[i], 0) for i in range(len(train))], 0)
        Mean = np.mean([np.mean(train[i], 0) for i in range(len(train))], 0)
        Std = np.std([np.std(train[i], 0) for i in range(len(train))], 0)
        
        for t in range(num_trials_maj):
            Q = np.ones((Q_size, 2))
            Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], Q_size)
            Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], Q_size)

            if (version == 'unsigned' or version == 'signed'):
                train_data = curve2vec(Q, train, version = version, sigma = sigma)
                test_data = curve2vec(Q, test, version = version, sigma = sigma)
            elif version == 'exp':
                train_data = ExpCurve2Vec(Q, train, mu)
                test_data = ExpCurve2Vec(Q, test, mu)
            elif version == 'endpoints':
                train_data = get_endpoints(train)
                test_data = get_endpoints(test)
            
            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                #nn.LeakyReLU(0.01),
                                #nn.Tanh(),
                                Flatten(),
                                nn.Linear(out_channels * (len(train_data[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )
    
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
            
            train_data = torch.from_numpy(train_data).float()
            test_data = torch.from_numpy(test_data).float()

            train_data = train_data.view(len(train_data), 1, len(train_data[0]))
            
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)
                losses[s, t, k] = loss
                    
                if (k+1) % 1000 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

                optimizer.zero_grad()

                loss.backward() # Backward pass

                optimizer.step()  # Calling the step function on the Optimizer 
                
            x_preds[t] = torch.argmax(model(train_data), axis=1)
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))        
            y_preds[t] = torch.argmax(model(test_data), axis=1)
        
        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))
        
        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    plt.plot((torch.mean(losses, dim=(0,1))).detach().numpy())
    plt.show()

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
               np.round(train_error_mean, decimals=4), 
                np.round(test_error_mean, decimals=4),
                np.round(test_error_std, decimals=4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                columns=['Channel 1', 'Learning Rate', 'Train Error', 
                         'Test Error', 'Std Error'])
    
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf, test_error_mean, test_error_std

"""## num_maj = 1

### Endpoints
"""

Data = data + 0
for i in range(len(data)):
    Data[i] = np.array([data[i][j][:,[1,2]] for j in range(len(data[i]))], dtype='object')

endpoint_errors = []
endpoint_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=1, out_channels=10, kernel_size=2,  
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'endpoints', 
                                       sigma = 10, test_size = 0.3)
    print(A[0])
    endpoint_errors.append(np.round(A[1], decimals=4).tolist())
    endpoint_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('endpoint_errors = \n', endpoint_errors)
print('endpoint_stds = \n', endpoint_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(endpoint_errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(endpoint_stds), decimals=4)}', 'green'))

"""### Rand($v_Q$)"""

errors = []
errors_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=1, out_channels= 10, kernel_size= 5,
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'unsigned', 
                                       sigma = 10, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_errors = \n', errors)
print('v_Q_errors_stds = \n', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))

"""### Rand($v_Q^{\varsigma}$)"""

errors = []
errors_stds = []

s_time = time.time()
for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=1, out_channels= 10, kernel_size= 5,
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'signed', 
                                       sigma = 10, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_varsigma_errors = ', errors)
print('v_Q_varsigma_errors_stds = ', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))

print("Total time: ", time.time() - s_time)

"""### Rand($v_Q^{exp}$)"""

errors = []
errors_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=1, out_channels= 10, kernel_size= 5,
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'exp', 
                                       sigma = 10, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_exp_errors = \n', errors)
print('v_Q_exp_errors_stds = \n', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))

"""## num_maj = 11

### Vote($v_Q$)
"""

errors = []
errors_stds = []
s_time = time.time()

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=11, out_channels=10, kernel_size=5,
                                       padding = 1, bias = True, Num_updates = 500, 
                                       D_out = 2, epoch = 50, version = 'unsigned', 
                                       sigma = 10, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_errors = \n', errors)
print('v_Q_errors_stds = \n', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))
print("Total time: ", time.time() - s_time)

"""### Vote($v_Q^{\varsigma}$)"""

errors = []
errors_stds = []
s_time = time.time()

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=11, out_channels=10, kernel_size=5,
                                       padding = 1, bias = True, Num_updates = 500, 
                                       D_out = 2, epoch = 50, version = 'signed', 
                                       sigma = 10, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_varsigma_errors = \n', errors)
print('v_Q_varsigma_errors_stds = \n', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))
print("Total time: ", time.time() - s_time)

"""### Vote($v_Q^{exp}$)"""

errors = []
errors_stds = []
s_time = time.time()

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=11, out_channels=10, kernel_size=5,
                                       padding = 1, bias = True, Num_updates = 500, 
                                       D_out = 2, epoch = 50, version = 'exp', 
                                       sigma = 10, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_exp_errors = \n', errors)
print('v_Q_exp_errors_stds = \n', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))
print("Total time: ", time.time() - s_time)

"""# Neural Network for Mistake Driven Method of Choosing $Q$

## Initialize $Q$
"""

def initialize_Q(train_1, train_2, std_coeff, out_channels, kernel_size, Q_size, 
                 padding, learning_rate = 1e-3, bias = True, D_out=2, lr_decay = 0.9, 
                 Num_updates = 100): 
        
    Q = []
    errors = []
    losses = np.zeros((Q_size - kernel_size, Num_updates))
    
    mu = get_mu(train_1, train_2)
    std = mu * std_coeff

    trajectory_train_data = np.concatenate((train_1, train_2), axis = 0)
    train_labels = np.concatenate(([1] * len(train_1), [0] * len(train_2)), 0)
    train_labels = torch.from_numpy(train_labels).long()
    index = np.random.randint(0, high=len(trajectory_train_data)) 
    k = np.random.randint(0, high=len(trajectory_train_data[index]))
    for i in range(kernel_size):
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)

    for i in range(Q_size - kernel_size):
        train_data = ExpCurve2Vec(np.array(Q), trajectory_train_data, mu)
        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                            out_channels = out_channels, 
                            kernel_size = kernel_size,
                            stride  = 1,
                            padding = padding,
                            bias = bias),
                nn.ReLU(),
                #nn.LeakyReLU(0.01),
                #nn.Tanh(),
                Flatten(),
                nn.Linear(out_channels * (len(train_data[0]) - kernel_size + 1 + 2 * padding), 
                            D_out)
                )
    
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
        train_data = torch.from_numpy(train_data).float()
        train_data = train_data.view(len(train_data), 1, len(train_data[0]))
        
        for k in range(Num_updates):
            x_pred = model(train_data) # of shape (N, D_out)
            loss = loss_fn(x_pred, train_labels)
            losses[i, k] = loss
            
            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
            optimizer.zero_grad()
            loss.backward() 
            optimizer.step() 
        
        train_pred = torch.argmax(model(train_data), axis=1)
        scores = model(train_data)
        I = np.where((train_labels == train_pred) == False)[0]

        temp_labels = 2 * train_labels.numpy().reshape(len(train_labels.numpy()), 1) - 1
        temp = temp_labels * scores.detach().numpy()
        temp = np.max(temp, axis=1)
        index = I[np.argmax(temp[I])]

        error = sum(train_labels != train_pred)/len(train_labels)
        errors.append(error.item())
        
        k = np.random.randint(0, high=len(trajectory_train_data[index]))
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)

    final_error = errors[-1]

    #plt.plot((np.mean(losses, axis=0)))
    #plt.show()

    return np.array(Q), np.array(errors), mu, final_error

"""## MD_CNN"""

def MD_NeuralNetworkClassificationCNN(data_1, data_2, maj_num, epoch, init_iter, 
                                      test_size, std_coeff, out_channels, kernel_size, 
                                      Q_size, padding, learning_rate = 1e-3, 
                                      bias = True, D_out=2, lr_decay = 0.9, 
                                      Num_updates = 100):
        
    start_time = time.time()

    train_errors = np.zeros(epoch) 
    test_errors = np.zeros(epoch)

    n_1 = len(data_1)
    n_2 = len(data_2) 

    for s in range(epoch):
        train_idx_1, test_idx_1, train_label_1, test_label_1, train_idx_2, \
        test_idx_2, train_label_2, test_label_2 = train_test(data_1, data_2, test_size)

        train = np.concatenate((data_1[train_idx_1], data_2[train_idx_2]), 0)
        test = np.concatenate((data_1[test_idx_1], data_2[test_idx_2]), 0)
        train_labels = np.concatenate((train_label_1, train_label_2), axis = 0)
        test_labels = np.concatenate((test_label_1, test_label_2), axis = 0)
        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()

        x_preds = torch.zeros((maj_num, len(train)))
        y_preds = torch.zeros((maj_num, len(test)))
        
        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        train_labels = train_labels[I]
        
        J = np.arange(len(test))
        np.random.shuffle(J)
        test = test[J]
        test_labels = test_labels[J]

        for t in range(maj_num):

            Q_list = []
            temp_errors = []
            mu_temp = []

            for j in range(init_iter):
                B = initialize_Q(data_1[train_idx_1], data_2[train_idx_2], 
                                 std_coeff, out_channels, kernel_size, Q_size, 
                                 padding, learning_rate = 1e-3, bias = True, 
                                 D_out=2, lr_decay = 0.9, Num_updates = 100)

                Q_list.append(B[0])
                mu_temp.append(B[2])
                temp_errors.append(B[-1])

            h = np.argmin(temp_errors)
            Q = Q_list[h]
            mu = mu_temp[h]

            train_data = torch.from_numpy(ExpCurve2Vec(Q, train, mu)).float()
            train_data = train_data.view(len(train_data), 1, len(train_data[0]))

            test_data = torch.from_numpy(ExpCurve2Vec(Q, test, mu)).float()
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))

            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                    nn.ReLU(),
                    #nn.LeakyReLU(0.01),
                    #nn.Tanh(),
                    Flatten(),
                    nn.Linear(out_channels * (len(train_data[0][0]) - kernel_size + 1 + 2 * padding), 
                              D_out)
                    )
        
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)

                if (k+1) % 10 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
                optimizer.zero_grad()
                loss.backward() 
                optimizer.step() 

            scores = model(train_data)
            
            x_preds[t] = torch.argmax(scores, axis=1)
            y_preds[t] = torch.argmax(model(test_data), axis=1)

        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))

        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
                            np.round(train_error_mean, decimals = 4), 
                            np.round(test_error_mean, decimals = 4),
                            np.round(test_error_std, decimals = 4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                                 columns=['Channel 1', 'Learning Rate', 
                                          'Train Error', 'Test Error', 'Std Error'])

    print(colored(f"total time = {time.time() - start_time}", "red"))
    print("mu =", mu)

    return pdf, test_error_mean, test_error_std

"""### num_maj = 1"""

errors = []
errors_stds = []

s_time = time.time()
for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = MD_NeuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], maj_num=1, 
                                          epoch=50, init_iter=3, 
                                          test_size=0.3, std_coeff=1, out_channels=10, 
                                          kernel_size = 5, Q_size = 20, padding = 1, 
                                          learning_rate = 1e-2, bias = True, D_out=2, 
                                          lr_decay = 0.9, Num_updates = 1000)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('errors = ', errors)
print('errors_stds = ', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))

print("Total time: ", time.time() - s_time)

"""### num_maj = 11"""

MD_errors = []
MD_errors_stds = []

s_time = time.time()
pair = pairs_final[0]
print(colored(f"pair={pair}", 'green'))
A = MD_NeuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], maj_num=11, epoch=50, 
                                      init_iter=3, test_size=0.3, std_coeff=1, 
                                      out_channels=10, kernel_size = 5, Q_size = 20, 
                                      padding = 1, learning_rate = 1e-2, bias = True, 
                                      D_out=2, lr_decay = 0.9, Num_updates = 1000)
print(A[0])
MD_errors.append(np.round(A[1], decimals=4).tolist())
MD_errors_stds.append(np.round(A[2], decimals=4).tolist())
print("time: ", time.time() - s_time)

s_time = time.time()
pair = pairs_final[1]
print(colored(f"pair={pair}", 'green'))
A = MD_NeuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], maj_num=11, epoch=50, 
                                      init_iter=3, test_size=0.3, std_coeff=1, 
                                      out_channels=10, kernel_size = 5, Q_size = 20, 
                                      padding = 1, learning_rate = 1e-2, bias = True, 
                                      D_out=2, lr_decay = 0.9, Num_updates = 1000)
print(A[0])
MD_errors.append(np.round(A[1], decimals=4).tolist())
MD_errors_stds.append(np.round(A[2], decimals=4).tolist())
print("time: ", time.time() - s_time)

s_time = time.time()
pair = pairs_final[2]
print(colored(f"pair={pair}", 'green'))
A = MD_NeuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], maj_num=11, epoch=50, 
                                      init_iter=3, test_size=0.3, std_coeff=1, 
                                      out_channels=10, kernel_size = 5, Q_size = 20, 
                                      padding = 1, learning_rate = 1e-2, bias = True, 
                                      D_out=2, lr_decay = 0.9, Num_updates = 1000)
print(A[0])
MD_errors.append(np.round(A[1], decimals=4).tolist())
MD_errors_stds.append(np.round(A[2], decimals=4).tolist())
print("time: ", time.time() - s_time)

s_time = time.time()
pair = pairs_final[3]
print(colored(f"pair={pair}", 'green'))
A = MD_NeuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], maj_num=11, epoch=50, 
                                      init_iter=3, test_size=0.3, std_coeff=1, 
                                      out_channels=10, kernel_size = 5, Q_size = 20, 
                                      padding = 1, learning_rate = 1e-2, bias = True, 
                                      D_out=2, lr_decay = 0.9, Num_updates = 1000)
print(A[0])
MD_errors.append(np.round(A[1], decimals=4).tolist())
MD_errors_stds.append(np.round(A[2], decimals=4).tolist())
print("time: ", time.time() - s_time)

s_time = time.time()
pair = pairs_final[4]
print(colored(f"pair={pair}", 'green'))
A = MD_NeuralNetworkClassificationCNN(Data[pair[0]], Data[pair[1]], maj_num=11, epoch=50, 
                                      init_iter=3, test_size=0.3, std_coeff=1, 
                                      out_channels=10, kernel_size = 5, Q_size = 20, 
                                      padding = 1, learning_rate = 1e-2, bias = True, 
                                      D_out=2, lr_decay = 0.9, Num_updates = 1000)
print(A[0])
MD_errors.append(np.round(A[1], decimals=4).tolist())
MD_errors_stds.append(np.round(A[2], decimals=4).tolist())
print("time: ", time.time() - s_time)

print('errors = ', MD_errors)
print('errors_stds = ', MD_errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(MD_errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(MD_errors_stds), decimals=4)}', 'green'))

"""# Plot test errors"""

A_LSVM = [0.2864, 0.2920, 0.4900, 0.3303, 0.2911, 0.2948, 0.4944, 0.3273, 0.3701]
A_GSVM = [0.2894, 0.2857, 0.3796, 0.3137, 0.2975, 0.2893, 0.3979, 0.3154, 0.3032]
A_PSVM = [0.2767, 0.2906, 0.4467, 0.3267, 0.2953, 0.3001, 0.4577, 0.3308, 0.3357]
A_DT = [0.2760, 0.2890, 0.3440, 0.3095, 0.3179, 0.3200, 0.3857, 0.3343, 0.3316]
A_RF = [0.2710, 0.2764, 0.3266, 0.3036, 0.2711, 0.2885, 0.3417, 0.3072, 0.2943]
A_KNN = [0.2898, 0.2868, 0.3942, 0.2923, 0.2929, 0.2943, 0.4028, 0.3082, 0.2944]
A_LR = [0.2923, 0.3361, 0.4609, 0.3308, 0.3029, 0.3381, 0.4615, 0.3341, 0.3959]
A_CNN = [0.3020, 0.2851, 0.4085, 0.3156, 0.3098, 0.2850, 0.3958, 0.3262, 0.4626]

A = np.array([A_LSVM, A_GSVM, A_PSVM, A_DT, A_RF, A_KNN, A_LR, A_CNN])
B = A.T

std_LSVM = [0.0418, 0.0416, 0.0489, 0.0460, 0.0426, 0.0414, 0.0518, 0.0510, 0.0461]
std_GSVM = [0.0458, 0.0400, 0.0486, 0.0406, 0.0448, 0.0393, 0.0519, 0.0478, 0.0420]
std_PSVM = [0.0440, 0.0405, 0.0456, 0.0411, 0.0463, 0.0404, 0.0493, 0.0457, 0.0404]
std_DT = [0.0433, 0.0470, 0.0474, 0.0423, 0.0475, 0.0465, 0.0533, 0.0477, 0.0480]
std_RF = [0.0430, 0.0434, 0.0485, 0.0410, 0.0392, 0.0410, 0.0485, 0.0394, 0.0450]
std_KNN = [0.0426, 0.0435, 0.0490, 0.0430, 0.0446, 0.0446, 0.0485, 0.0471, 0.0421]
std_LR = [0.0373, 0.0418, 0.0075, 0.0353, 0.0415, 0.0467, 0.0110, 0.0424, 0.0363]
std_CNN = [0.0492, 0.0436, 0.0651, 0.0454, 0.0492, 0.0450, 0.0566, 0.0387, 0.0400]

C = np.array([std_LSVM, std_GSVM, std_PSVM, std_DT, std_RF, std_KNN, std_LR, std_CNN]).T

errors = [0.2967, 0.3038, 0.2912, 0.2947, 0.2860, 0.2830, 0.2847, 0.2726, 0.5140, 
          0.3550, 0.3607, 0.2864]

stds = [0.0383, 0.0424, 0.0453, 0.0430, 0.0397, 0.0404, 0.0397, 0.0415, 0.0415, 
        0.0498, 0.0564, 0.0433]

classifiers = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

FM = (r'KNN', r'Vote(MD $v_Q^{exp}$)', r'Vote(Rand $v_Q$)', r'Vote($v_Q^{\varsigma}$)', 
      r'Vote(Rand $v_Q^{exp}$)', r'MD $v_Q^{exp}$', r'Rand $v_Q$', 
      r'Rand $v_Q^{\varsigma}$', r'Rand $v_Q^{exp}$', 'Endpoints')

labels = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

dists = [r'$d_F$', r'$d_{dF}$', r'$dH$', r'DTW', r'soft-dtw', r'fastdtw', 
         r'LCSS', r'SSPD', r'EDR', r'ERP', r'LSH', r'$d_Q^{\pi}$']

def lower_limit_error(x, y):
    if y - x < 0:
        return y
    else:
        return x

lower_limit_error = np.vectorize(lower_limit_error)

width=0.1
index = np.arange(len(stds))
ind = np.arange(len(A)) + len(index) * 0.25

plt.subplots(figsize = (22, 8), tight_layout=True)
bars = [0] * 10

lower_lim = lower_limit_error(stds, errors)
bars[0] = plt.bar(index * 0.2, errors, width, yerr=[lower_lim, stds], capsize=2)

for i in range(len(FM)-1):
    lower_limit = lower_limit_error(C[i], B[i])
    bars[i+1] = plt.bar(ind+width*i-0.3, B[i], width, yerr=[lower_limit, C[i]], capsize=2)

plt.title('T-drive Data', fontsize = 20)
plt.xticks(list(np.arange(len(stds)) * 0.2) + list(ind + 0.5 * width), 
           dists + labels, fontsize = 14)

plt.legend(tuple(bars), FM, loc=2, fontsize = 12)

plt.gca().yaxis.grid(color='gray', linestyle='dotted', linewidth=0.6)
plt.xticks(rotation='vertical', fontsize = 14)
plt.yticks(fontsize = 14)

plt.savefig('/Users/hasan/Desktop/plots/plots T-drive/T-drive bar chart all horizontal.png', 
            bbox_inches='tight', dpi=200)

plt.show()

