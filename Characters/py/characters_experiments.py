# -*- coding: utf-8 -*-
"""Characters_Experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a3Yl-cp8t2uyoAsjWkSkyjtdXYB_tg13
"""

#pip install similaritymeasures

#pip install tslearn

#pip install frechetdist

pip install matplotlib==3.1.3

pip install trjtrypy

import glob
import numpy as np 
import time
import math
import random
from scipy import linalg as LA
import pandas as pd
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.svm import NuSVC
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import Perceptron
import statsmodels.api as sm
from autograd import grad
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
from termcolor import colored
import matplotlib as mpl
import matplotlib.pyplot as plt
from scipy import optimize
from sklearn.svm import LinearSVC
from termcolor import colored
from scipy.stats import entropy
import os
import ast
import csv
import json 
import scipy.io
#import similaritymeasures
#import tslearn
#from tslearn.metrics import dtw
#from scipy.spatial.distance import directed_hausdorff
#from frechetdist import frdist

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import sampler
import torchvision.transforms as T

from google.colab import drive
drive.mount("/content/gdrive")

import trjtrypy as tt
from trjtrypy.featureMappings import curve2vec

def ExpCurve2Vec(points,curves,mu):
    D=tt.distsbase.DistsBase()
    return [np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves]

"""# Reading data"""

mat = scipy.io.loadmat('/content/gdrive/My Drive/Research/Characters/mixoutALL_shifted.mat')

s = set()
for i in range(len(mat['mixout'][0])):
    s.add(mat['mixout'][0][i].shape[1])
print("length of data points belongs to: \n", s)

labels = mat['consts'][0][0][4][0]
print(labels)

Data = [0] * len(mat['mixout'][0])
for j in range(len(Data)):
    Data[j] = [0] * len(mat['mixout'][0][j][0])
    Data[j][0] = np.zeros(2)
    for i in range(1, len(mat['mixout'][0][j][0])):
        Data[j][i] = Data[j][i-1] + mat['mixout'][0][j][:2, i]
    Data[j] = np.array(Data[j])
        
Data = np.array(Data, dtype='object')
print(Data.shape)


data = [0] * 20
for i in range(1, 21):
    I = np.where(labels == i)[0]
    data[i-1] = Data[I]
data = np.array(data, dtype='object')

mat = scipy.io.loadmat('/content/gdrive/My Drive/Research/Characters/mixoutALL_shifted.mat')

s = set()
for i in range(len(mat['mixout'][0])):
    s.add(mat['mixout'][0][i].shape[1])
print("length of data points belongs to: \n", s)

labels = mat['consts'][0][0][4][0]
labels

Data = [0] * len(mat['mixout'][0])
for j in range(len(Data)):
    Data[j] = [0] * len(mat['mixout'][0][j][0])
    Data[j][0] = np.zeros(2)
    for i in range(1, len(mat['mixout'][0][j][0])):
        Data[j][i] = Data[j][i-1] + mat['mixout'][0][j][:2, i]
    Data[j] = np.array(Data[j])
        
Data = np.array(Data)
Data.shape

data = [0] * 20
for i in range(1, 21):
    I = np.where(labels == i)[0]
    data[i-1] = Data[I]
data = np.array(data, dtype='object')

"""## removes stationary points"""

def remove_segments(traj): 
    p2 = traj[:,:2][1:]
    p1 = traj[:,:2][:-1]
    L = ((p2-p1)*(p2-p1)).sum(axis =1)
    I = np.where(L>1e-16)[0]
    return traj[I]

for i in range(len(data)):
    data[i] = np.array(list(map(remove_segments, data[i])), dtype='object')

"""# Classifiers, get $\mu$, pairs"""

CC = [100, 100, 10]
number_estimators = [50, 50]


clf0 = [make_pipeline(LinearSVC(dual=False, C=CC[0], tol=1e-5, 
                               class_weight ='balanced', max_iter=1000)), 
        "SVM, LinearSVC, C = "+str(CC[0])]
clf1 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[1], kernel='rbf', gamma='auto', max_iter=200000)),
        "Gaussian SVM, C="+str(CC[1])+", gamma=auto"]
clf2 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[2], kernel='poly', degree=3, max_iter=400000)),
        "Poly kernel SVM, C="+str(CC[2])+", deg=auto"]
clf3 = [DecisionTreeClassifier(), "Decision Tree"]
clf4 = [RandomForestClassifier(n_estimators=number_estimators[0]), 
         "RandomForestClassifier, n="+str(number_estimators[0])]
clf5 = [KNeighborsClassifier(n_neighbors=5), "KNN"]
clf6 = [LogisticRegression(solver='newton-cg'), "Logistic Regression"]

clff = [clf0, clf1, clf2, clf3, clf4, clf5, clf6]
classifs = [item[0] for item in clff]
keys = [item[1] for item in clff]

def get_mu(data_1, data_2):
    a = np.mean([np.mean(data_1[i], 0) for i in range(len(data_1))], 0)
    b = np.mean([np.mean(data_2[i], 0) for i in range(len(data_2))], 0)
    c = abs(a-b)
    return max(c)

pairs = [[16,18], [10,18], [10,16], [2,8], [3,11]]
pairs = np.array(pairs) - 1
# (u,w), (n,w), (n,u), (b,c), (c,o)

"""# Choosing pairs (Don't run again)"""

def classify(n, m, k = 20, t = 10, std_x = 20, std_y = 20, clf = clf):
    
    I = np.where(labels == n)[0] 
    J = np.where(labels == m)[0]
    K = np.concatenate((I, J), 0)
    
    Min = np.min([np.min(Data[K][i], 0) for i in range(len(Data[K]))], 0)
    Max = np.max([np.max(Data[K][i], 0) for i in range(len(Data[K]))], 0)
    Mean = np.mean([np.mean(Data[K][i], 0) for i in range(len(Data[K]))], 0)
    Std = np.std([np.std(Data[K][i], 0) for i in range(len(Data[K]))], 0)
    
    data_1 = Data[I]
    data_2 = Data[J]
    
    Q = np.ones((k,2))
    Q[:,0] = np.random.normal(Mean[0], Std[0] + std_x, k)
    Q[:,1] = np.random.normal(Mean[1], Std[1] + std_y, k)
    
    n_1 = len(data_1)
    n_2 = len(data_2)
    
    # unsigned feature mapping
    proj_1 = [0] * n_1 
    proj_2 = [0] * n_2 
    for i in range(n_1):
        proj_1[i] = np.concatenate((Curve2Vec(Q, [data_1[i].tolist()])[0], [1]), axis = 0)
    for i in range(n_2):
        proj_2[i] = np.concatenate((Curve2Vec(Q, [data_2[i].tolist()])[0],[-1]), axis = 0)
    proj_1 = np.array(proj_1)
    proj_2 = np.array(proj_2)
    
    error_tr = np.zeros((len(clf), t))
    error_ts = np.zeros((len(clf), t))
    
    for i in range(t): 
        R1 = random.sample(range(n_1), n_1//3)
        R = np.sort(R1)
        R_c = np.sort(list(set(range(n_1)) - set(R)))
        S1 = random.sample(range(n_2), n_2//3)
        S = np.sort(S1)
        S_c = np.sort(list(set(range(n_2)) - set(S)))

        data_tr = np.insert(proj_1[R_c], len(R_c), proj_2[S_c], axis = 0)
        data_ts = np.insert(proj_1[R], len(R), proj_2[S], axis = 0)

        random.shuffle(data_tr)
        random.shuffle(data_ts)

        for s in range(len(clf)):
            model = clf[s][0]
            model.fit(data_tr[:,:-1], data_tr[:,-1])
            y_pre = model.predict(data_ts[:,:-1])
            error_ts[s][i] = 1 - metrics.accuracy_score(data_ts[:,-1], y_pre)
            x_pre = model.predict(data_tr[:,:-1])
            error_tr[s][i] = 1 - metrics.accuracy_score(data_tr[:,-1], x_pre)

    Dic = {}
    models = ["L SVM, C=1000", "G SVM, C=1", "G SVM, C=2", "G SVM, C=0.5", 
              "P SVM, C=1000", "DTree", "RForest, n=100"]
    for l in range(len(models)): 
        Dic[l+1] = [models[l], np.round(np.mean(error_tr[l]), decimals = 4), 
                    np.round(np.mean(error_ts[l]), decimals = 4),
                    np.round(np.std(error_ts[l]), decimals = 4)]
    
    print("n, m =", (n, m))
    
    print(pd.DataFrame.from_dict(Dic, orient='index', 
                                 columns=['Classifier', 'Train', 'Test', 'Std']))
    
    return n, m, t

for n in range(1, 20):
    for m in range(n+1, 21):
        classify(n, m, k=10, t=3, clf=clf)

pairs = [[16,18], [10,18], [10,16], [2,8], [3,11]]

for n, m in pairs:
    classify(n, m, k = 10, t = 20, clf = clf)

"""# Pairs and plotting (Extra)"""

pairs = [[16,18], [10,18], [10,16], [2,8], [3,11]]
pairs = np.array(pairs) - 1
# (u,w), (n,w), (n,u), (b,c), (c,o)

for i in range(len(pairs)):
    print("pairs =", pairs[i])
    print("mu =", get_mu(data[pairs[i][0]], data[pairs[i][1]]))

len(data[2]), len(data[10])

i = 0 
m, n = pairs[i]
mu = get_mu(data[m], data[n])
print("pairs =", f"{m},", n)
print("mu =", mu)
plt.plot(data[n][0][:,0], data[n][0][:,1])
plt.plot(data[m][0][:,0], data[m][0][:,1])
plt.show()

m= 9
n = 17
for i in range(10):
    #plt.plot(data[n][i][:,0], data[n][i][:,1])
    plt.plot(data[m][i][:,0], data[m][i][:,1])
    plt.show()

for n, m in pairs:
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

characters = [['u, w'], ['n, w'], ['n, u'], ['b, c'], ['c, o']]
for j in range(len(pairs)):
    for i in range(20):
        plt.plot(data[pairs[j][0]][i][:,0], data[pairs[j][0]][i][:,1], color='blue')
        plt.plot(data[pairs[j][1]][i][:,0], data[pairs[j][1]][i][:,1], color='red')
    plt.title(f'Characters {characters[j][0]}')
    print(j)
    plt.show()

"""# KNN

## KNN with $d_Q^{\pi}$ distance with matrix storing method

### Calculate distance matrix
"""

def calculate_dists_d_Q_pi(data1, data2, p, path): 
    start_time = time.time() 
    data = np.concatenate((data1, data2), 0)
    n = len(data)
    A = []
    for i in range(n-1):
        for j in range(i+1, n):
            A.append(d_pi_QQ(Q, data[i], data[j], p=p))
    A = np.array(A)
    tri = np.zeros((n, n))
    tri[np.triu_indices(n, 1)] = A
    for i in range(1, n):
        for j in range(i):
            tri[i][j] = tri[j][i]
    np.savetxt(path, tri, delimiter=',')

    total_time = time.time() - start_time
    return total_time

paths_d_Q_pi = []
for i in range(len(pairs)):
    paths_d_Q_pi.append('/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (Characters Pairs['+str(i)+'])/d_Q_pi.csv')

for i in range(len(pairs)):
    calculate_dists_d_Q_pi(data[pairs[i][0]], data[pairs[i][1]], p=1, path=paths_d_Q_pi[i])

def KNN_with_dists(n_1, n_2, path_to_dists):
    '''path example: '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (Characters Pairs['+str(0)+'])/d_Q_pi.csv'
       path_to_dists: the path to the corresponding distance matrix
       n_1: len(data_1)
       n_2: len(data_2)'''

    I_1, J_1, y_train_1, y_test_1 = train_test_split(np.arange(n_1), 
                                                np.ones(n_1), test_size=0.3)
    I_2, J_2, y_train_2, y_test_2 = train_test_split(np.arange(n_1, n_1+n_2), 
                                                np.ones(n_2), test_size=0.3)
    labels = np.array([1] * n_1 + [0] * n_2)
    I = np.concatenate((I_1, I_2), 0)
    np.random.shuffle(I)
    J = np.concatenate((J_1, J_2), 0)
    np.random.shuffle(J)

    dist_matrix = np.array(pd.read_csv(path_to_dists, header=None))

    D_train = dist_matrix[I][:, I]
    D_test = dist_matrix[J][:,I]
    train_labels = labels[I]
    test_labels = labels[J]

    clf = KNeighborsClassifier(n_neighbors=5, metric='precomputed')
    
    #Train the model using the training sets
    clf.fit(D_train, list(train_labels))

    #Predict labels for train dataset
    train_pred = clf.predict(D_train)
    train_error = sum(train_labels != train_pred)/len(I)
    
    #Predict labels for test dataset
    test_pred = clf.predict(D_test)
    test_error = sum((test_labels != test_pred))/len(J)
        
    return train_error, test_error

def KNN_average_error(data1, data2, num_trials, path_to_dists, pair):

    '''path_to_dists: the path to the corresponding distance matrix'''

    Start_time = time.time()

    train_errors = np.zeros(num_trials)
    test_errors = np.zeros(num_trials)

    for i in range(num_trials):
        train_errors[i], test_errors[i] = KNN_with_dists(len(data1), len(data2), path_to_dists)

    Dict = {}
    Dict[1] = [f"KNN with d_Q_pi for pairs {pair}", 
                    np.round(np.mean(train_errors), decimals = 4), 
                    np.round(np.mean(test_errors), decimals = 4), 
                    np.round(np.std(test_errors), decimals = 4)]

    df = pd.DataFrame.from_dict(Dict, orient='index', columns=['Classifier',
                                'Train Error', 'Test Error', 'std'])
    print(colored(f"num_trials = {num_trials}", "blue"))
    print(colored(f'total time = {time.time() - Start_time}', 'green'))

    return (df, np.mean(train_errors), np.mean(test_errors), np.std(test_errors))

paths_d_Q_pi = []
for i in range(len(pairs)):
    paths_d_Q_pi.append('/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (Characters Pairs['+str(i)+'])/d_Q_pi.csv')

for i in range(len(pairs)):
    print(KNN_average_error(data[pairs[i][0]], data[pairs[i][1]], num_trials=50, 
                            path_to_dists=paths_d_Q_pi[i], pair=pairs[i])[0])
    print("=====================================================================")

(0.1656+0.3577+0.078+0.2365+0.0551)/5, (0.0235+  0.0313+  0.0245+  0.0481+  0.0389)/5

"""## KNN with DTW from tslearn by saving matrx method

### Calculate distance matrix
"""

def calculate_dists_dtw_tslearn(data1, data2, path): 
    start_time = time.time() 
    data = np.concatenate((data1, data2), 0)
    n = len(data)
    A = []
    for i in range(n-1):
        for j in range(i+1, n):
            A.append(tslearn.metrics.dtw(data[i], data[j]))
    A = np.array(A)
    tri = np.zeros((n, n))
    tri[np.triu_indices(n, 1)] = A
    for i in range(1, n):
        for j in range(i):
            tri[i][j] = tri[j][i]
    np.savetxt(path, tri, delimiter=',')

    total_time = time.time() - start_time
    return total_time

paths = []
for i in range(len(pairs)):
    paths.append('/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (Characters Pairs['+str(i)+'])/dtw-tslearn.csv')

for i in range(len(pairs)):
    calculate_dists_dtw_tslearn(data[pairs[i][0]], data[pairs[i][1]], path=paths[i])

def KNN_with_dists_dtw_tslearn(n_1, n_2, path_to_dists):
    '''path example: '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/sspd.csv'
       path_to_dists: the path to the corresponding distance matrix
       n_1: len(data_1)
       n_2: len(data_2)'''

    I_1, J_1, y_train_1, y_test_1 = train_test_split(np.arange(n_1), 
                                                np.ones(n_1), test_size=0.3)
    I_2, J_2, y_train_2, y_test_2 = train_test_split(np.arange(n_1, n_1+n_2), 
                                                np.ones(n_2), test_size=0.3)
    labels = np.array([1] * n_1 + [0] * n_2)
    I = np.concatenate((I_1, I_2), 0)
    np.random.shuffle(I)
    J = np.concatenate((J_1, J_2), 0)
    np.random.shuffle(J)
    dist_matrix = np.array(pd.read_csv(path_to_dists, header=None))

    D_train = dist_matrix[I][:, I]
    D_test = dist_matrix[J][:,I]
    train_labels = labels[I]
    test_labels = labels[J]

    clf = KNeighborsClassifier(n_neighbors=5, metric='precomputed')
    
    #Train the model using the training sets
    clf.fit(D_train, list(train_labels))

    #Predict labels for train dataset
    train_pred = clf.predict(D_train)
    train_error = sum(train_labels != train_pred)/len(I)
    
    #Predict labels for test dataset
    test_pred = clf.predict(D_test)
    test_error = sum((test_labels != test_pred))/len(J)
        
    return train_error, test_error

def KNN_average_error_dtw_tslearn(data1, data2, num_trials, path_to_dists, pair):

    '''path_to_dists: the path to the corresponding distance matrix'''

    Start_time = time.time()

    train_errors = np.zeros(num_trials)
    test_errors = np.zeros(num_trials)

    for i in range(num_trials):
        train_errors[i], test_errors[i] = KNN_with_dists_dtw_tslearn(len(data1), len(data2), path_to_dists)

    Dict = {}
    Dict[1] = [f"KNN with dtw from tslearn for pairs {pair}", 
                    np.round(np.mean(train_errors), decimals = 4), 
                    np.round(np.mean(test_errors), decimals = 4), 
                    np.round(np.std(test_errors), decimals = 4)]

    df = pd.DataFrame.from_dict(Dict, orient='index', columns=['Classifier',
                                'Train Error', 'Test Error', 'std'])
    print(colored(f"num_trials = {num_trials}", "blue"))
    print(colored(f'total time = {time.time() - Start_time}', 'green'))

    return (df, np.mean(train_errors), np.mean(test_errors), np.std(test_errors))

paths = []
for i in range(len(pairs)):
    paths.append('/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (Characters Pairs['+str(i)+'])/dtw-tslearn.csv')

for i in range(len(pairs)):
    print(KNN_average_error_dtw_tslearn(data[pairs[i][0]], data[pairs[i][1]], 
                                        num_trials=50, path_to_dists=paths[i], 
                                        pair=pairs[i])[0])
    print("===============================================================================")

(0.1015+0.2034+0.0651+0.0012+0.0012)/5, (0.0042+  0.004+  0.0261+  0.0453+  0.0344)/5

"""# Classification with $v_Q, v_Q^{\varsigma}, v_Q^{\exp}$ with random landmarks"""

from google.colab import files
files.upload()

import v_Q_mu_endpoints_classification
from v_Q_mu_endpoints_classification import binaryClassificationAverageMajority

"""## Classification with the feature function  $v_q(\gamma)$ and 20 random Landmarks

### Boost($v_Q$) with epoch=50 and num_trials_maj=11
"""

errors_v_Q = []
stds_v_Q = []

for i in range(len(pairs)):
    n, m = pairs[i]
    print("pairs =", f"{m},", n)

    classifs = binaryClassificationAverageMajority(data[m], data[n], Q_size=20, epoch=50, 
                                        num_trials_maj=11, classifiers=clff, 
                                        version='unsigned', test_size=0.3)
    A = classifs.classification_v_Q()

    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    errors_v_Q.append(A[2])
    stds_v_Q.append(A[3])
    print(A[0])
    print("=========================================================================")

print(colored(f"mean errors: {list(np.round(np.mean(errors_v_Q,0), decimals=4))}", 'yellow'))
print(colored(f"std errors: {list(np.round(np.mean(stds_v_Q,0), decimals=4))}", 'magenta'))

"""### Rand $v_Q$ with epoch=50 and num_trials_maj=1"""

errors_v_Q_ave = []
stds_v_Q_ave = []

for i in range(len(pairs)):
    n, m = pairs[i]
    print("pairs =", f"{m},", n)

    classifs = binaryClassificationAverageMajority(data[m], data[n], Q_size=20, epoch=50, 
                                        num_trials_maj=1, classifiers=clff, 
                                        version='unsigned', sigma=1, test_size=0.3)
    A = classifs.classification_v_Q()

    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    errors_v_Q_ave.append(A[2])
    stds_v_Q_ave.append(A[3])
    print(A[0])
    print("=========================================================================")

print(colored(f"mean errors: {list(np.round(np.mean(errors_v_Q_ave,0), decimals=4))}", 'yellow'))
print(colored(f"std errors: {list(np.round(np.mean(stds_v_Q_ave,0), decimals=4))}", 'magenta'))

"""## Classification with the feature function  $v_q^{\varsigma}(\gamma)$ and 20 random landmarks and $\varsigma=100$

### Boost($v_Q^{\varsigma}$) with epoch=50 and num_trials_maj=11
"""

errors_v_Q_sigma_ave_maj = []
stds_v_Q_sigma_ave_maj = []

for i in range(len(pairs)):
    n, m = pairs[i]
    print("pairs =", f"{m},", n)

    classifs = binaryClassificationAverageMajority(data[m], data[n], Q_size=20, epoch=50, 
                                        num_trials_maj=11, classifiers=clff, 
                                        version='signed', sigma=100, test_size=0.3)
    A = classifs.classification_v_Q()

    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    errors_v_Q_sigma_ave_maj.append(A[2])
    stds_v_Q_sigma_ave_maj.append(A[3])
    print(A[0])
    print("=========================================================================")

print(colored(f"mean errors: {list(np.round(np.mean(errors_v_Q_sigma_ave_maj,0), decimals=4))}", 'yellow'))
print(colored(f"std errors: {list(np.round(np.mean(stds_v_Q_sigma_ave_maj,0), decimals=4))}", 'magenta'))

"""### Rand $v_Q^{\varsigma}$ with epoch=50 and num_trials_maj=1"""

errors_v_Q_sigma_ave = []
stds_v_Q_sigma_ave = []

for i in range(len(pairs)):
    n, m = pairs[i]
    print("pairs =", f"{m},", n)

    classifs = binaryClassificationAverageMajority(data[m], data[n], Q_size=20, epoch=50, 
                                        num_trials_maj=1, classifiers=clff, 
                                        version='signed', sigma=100, test_size=0.3)
    A = classifs.classification_v_Q()

    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    errors_v_Q_sigma_ave.append(A[2])
    stds_v_Q_sigma_ave.append(A[3])
    print(A[0])
    print("=========================================================================")

print(colored(f"mean errors: {list(np.round(np.mean(errors_v_Q_sigma_ave,0), decimals=4))}", 'yellow'))
print(colored(f"std errors: {list(np.round(np.mean(stds_v_Q_sigma_ave,0), decimals=4))}", 'magenta'))

"""## Classification with the feature function  $v_Q^{\exp}$ and 20 random landmarks

### Boost($v_Q^{\exp})$ with epoch=50 and num_maj=11
"""

errors_v_Q_mu = []
stds_v_Q_mu = []

for i in range(len(pairs)):
    n, m = pairs[i]
    print("pairs =", f"{m},", n)

    classifs = binaryClassificationAverageMajority(data[m], data[n], Q_size=20, epoch=50, 
                                        num_trials_maj=11, classifiers=clff, test_size=0.3)
    A = classifs.classification_v_Q_mu()

    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    errors_v_Q_mu.append(A[2])
    stds_v_Q_mu.append(A[3])
    print(A[0])
    print("=========================================================================")

print(colored(f"mean errors: {list(np.round(np.mean(errors_v_Q_mu,0), decimals=4))}", 'yellow'))
print(colored(f"std errors: {list(np.round(np.mean(stds_v_Q_mu,0), decimals=4))}", 'magenta'))

"""### Rand $v_Q^{\exp}$ with epoch=50 and num_maj=1"""

errors_v_Q_mu_ave = []
stds_v_Q_mu_ave = []

for i in range(len(pairs)):
    n, m = pairs[i]
    print("pairs =", f"{m},", n)

    classifs = binaryClassificationAverageMajority(data[m], data[n], Q_size=20, epoch=50, 
                                        num_trials_maj=1, classifiers=clff, test_size=0.3)
    A = classifs.classification_v_Q_mu()

    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    errors_v_Q_mu_ave.append(A[2])
    stds_v_Q_mu_ave.append(A[3])
    print(A[0])
    print("=========================================================================")

print(colored(f"mean errors: {list(np.round(np.mean(errors_v_Q_mu_ave,0), decimals=4))}", 'yellow'))
print(colored(f"std errors: {list(np.round(np.mean(stds_v_Q_mu_ave,0), decimals=4))}", 'magenta'))

"""# Classification based on endpoints with epoch=50"""

errors_endpoint = []
stds_endpoint = []

for i in range(len(pairs)):
    n, m = pairs[i]
    print("pairs =", f"{m},", n)

    classifs = binaryClassificationAverageMajority(data[m], data[n], 
                                            Q_size=1, epoch=50, num_trials_maj=1, 
                                            classifiers=clff, test_size=0.3)
    A = classifs.endpoint_classification()

    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    errors_endpoint.append(A[2])
    stds_endpoint.append(A[3])
    print(A[0])
    print("=========================================================================")

print(colored(f"mean errors: {list(np.round(np.mean(errors_endpoint,0), decimals=4))}", 'yellow'))
print(colored(f"std errors: {list(np.round(np.mean(stds_endpoint,0), decimals=4))}", 'magenta'))

"""# Classification with perceptron-like algorithm with 20 landmarks"""

# to import Perceptron_Like_Algo_Class from my computer
from google.colab import files
files.upload()

import Perceptron_Like_Algo_Class
from Perceptron_Like_Algo_Class import classification

"""## Boost(MD $v_Q^{\exp}$) with epoch=50, majority=11 and init_num=3"""

perc_LSVM_errors = []
perc_LSVM_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='LSVM', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=11, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_LSVM_errors.append(np.round(A[3], decimals=4)) 
    perc_LSVM_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("=============================================================")
    
print(colored(f'Mistake deriven LSVM test errors for 5 pairs: \n {np.array(perc_LSVM_errors)}', 'green'))
print(colored(f'Mistake deriven LSVM stds of test error for 5 pairs: \n {np.array(perc_LSVM_stds)}', 'magenta'))

print(colored(f'Mistake deriven LSVM average test error on 5 pairs: \n {list(np.round(np.mean(perc_LSVM_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven LSVM average stds of test error on 5 pairs: \n {list(np.round(np.mean(perc_LSVM_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/LSVM-error.csv', 
            perc_LSVM_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/LSVM-std.csv', 
            perc_LSVM_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/LSVM-error-ave.csv', 
            np.round(np.mean(perc_LSVM_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/LSVM-std-ave.csv', 
            np.round(np.mean(perc_LSVM_stds, 0), decimals=4), delimiter=',')

#pd.read_csv('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/LSVM-error-ave.csv', header=None)

perc_GSVM_errors = []
perc_GSVM_stds = []

for i in range(len(pairs)): 
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='GSVM', 
                                C=100, gamma='auto', classifiers=[], epoch=50, maj_num=11, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_GSVM_errors.append(np.round(A[3], decimals=4)) 
    perc_GSVM_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven GSVM test errors for 5 pairs: \n {np.array(perc_GSVM_errors)}', 'green'))
print(colored(f'Mistake deriven GSVM stds of test error for 5 pairs: \n {np.array(perc_GSVM_stds)}', 'magenta'))

print(colored(f'Mistake deriven GSVM average test error on 5 pairs: \n {list(np.round(np.mean(perc_GSVM_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven GSVM average stds of test error on 5 pairs: \n {list(np.round(np.mean(perc_GSVM_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/GSVM-error.csv', 
            perc_GSVM_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/GSVM-std.csv', 
            perc_GSVM_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/GSVM-error-ave.csv', 
            np.round(np.mean(perc_GSVM_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/GSVM-std-ave.csv', 
            np.round(np.mean(perc_GSVM_stds, 0), decimals=4), delimiter=',')

perc_PSVM_errors = []
perc_PSVM_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='PSVM', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=11, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_PSVM_errors.append(np.round(A[3], decimals=4)) 
    perc_PSVM_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven PSVM test errors for 5 pairs: \n {np.array(perc_PSVM_errors)}', 'green'))
print(colored(f'Mistake deriven PSVM stds of test error for 5 pairs: \n {np.array(perc_PSVM_stds)}', 'magenta'))

print(colored(f'Mistake deriven PSVM average test error on 5 pairs: \n       {list(np.round(np.mean(perc_PSVM_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven PSVM average stds of test error on 5 pairs: \n       {list(np.round(np.mean(perc_PSVM_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/PSVM-error.csv', 
            perc_PSVM_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/PSVM-std.csv', 
            perc_PSVM_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/PSVM-error-ave.csv', 
            np.round(np.mean(perc_PSVM_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/PSVM-std-ave.csv', 
            np.round(np.mean(perc_PSVM_stds, 0), decimals=4), delimiter=',')

perc_DT_errors = []
perc_DT_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='DT', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=11, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_DT_errors.append(np.round(A[3], decimals=4)) 
    perc_DT_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven DT test errors for 5 pairs: \n {np.array(perc_DT_errors)}', 'green'))
print(colored(f'Mistake deriven DT stds of test error for 5 pairs: \n {np.array(perc_DT_stds)}', 'magenta'))

print(colored(f'Mistake deriven DT average test error on 5 pairs: \n       {list(np.round(np.mean(perc_DT_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven DT average stds of test error on 5 pairs: \n       {list(np.round(np.mean(perc_DT_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/DT-error.csv', 
            perc_DT_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/DT-std.csv', 
            perc_DT_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/DT-error-ave.csv', 
            np.round(np.mean(perc_DT_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/DT-std-ave.csv', 
            np.round(np.mean(perc_DT_stds, 0), decimals=4), delimiter=',')

perc_RF_errors = []
perc_RF_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='RF', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=11, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_RF_errors.append(np.round(A[3], decimals=4)) 
    perc_RF_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven RF test errors for 5 pairs: \n {np.array(perc_RF_errors)}', 'green'))
print(colored(f'Mistake deriven RF stds of test error for 5 pairs: \n {np.array(perc_RF_stds)}', 'magenta'))

print(colored(f'Mistake deriven RF average test error on 5 pairs: \n {list(np.round(np.mean(perc_RF_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven RF average stds of test error on 5 pairs: \n {list(np.round(np.mean(perc_RF_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/RF-error.csv', 
            perc_RF_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/RF-std.csv', 
            perc_RF_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/RF-error-ave.csv', 
            np.round(np.mean(perc_RF_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/RF-std-ave.csv', 
            np.round(np.mean(perc_RF_stds, 0), decimals=4), delimiter=',')

perc_KNN_errors = []
perc_KNN_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='KNN', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=11, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_KNN_errors.append(np.round(A[3], decimals=4)) 
    perc_KNN_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven KNN test errors for 5 pairs: \n {np.array(perc_KNN_errors)}', 'green'))
print(colored(f'Mistake deriven KNN stds of test error for 5 pairs: \n {np.array(perc_KNN_stds)}', 'magenta'))

print(colored(f'Mistake deriven KNN average test error on 5 pairs: \n       {list(np.round(np.mean(perc_KNN_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven KNN average stds of test error on 5 pairs: \n       {list(np.round(np.mean(perc_KNN_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/KNN-error.csv', 
            perc_KNN_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/KNN-std.csv', 
            perc_KNN_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/KNN-error-ave.csv', 
            np.round(np.mean(perc_KNN_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/KNN-std-ave.csv', 
            np.round(np.mean(perc_KNN_stds, 0), decimals=4), delimiter=',')

perc_LR_errors = []
perc_LR_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='LR', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=11, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_LR_errors.append(np.round(A[3], decimals=4)) 
    perc_LR_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven LR test errors for 5 pairs: \n {np.array(perc_LR_errors)}', 'green'))
print(colored(f'Mistake deriven LR stds of test error for 5 pairs: \n {np.array(perc_LR_stds)}', 'magenta'))

print(colored(f'Mistake deriven LR average test error on 5 pairs: \n       {list(np.round(np.mean(perc_LR_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven LR average stds of test error on 5 pairs: \n       {list(np.round(np.mean(perc_LR_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/LR-error.csv', 
            perc_LR_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/LR-std.csv', 
            perc_LR_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/LR-error-ave.csv', 
            np.round(np.mean(perc_LR_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/test errors and stds (Perceptron-like)/LR-std-ave.csv', 
            np.round(np.mean(perc_LR_stds, 0), decimals=4), delimiter=',')

"""## MD $v_Q^{\exp}$ with epoch=50, majority=1 and init_num=3"""

perc_LSVM_errors = []
perc_LSVM_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='LSVM', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=1, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_LSVM_errors.append(np.round(A[3], decimals=4)) 
    perc_LSVM_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven LSVM test errors for 5 pairs: \n {np.array(perc_LSVM_errors)}', 'green'))
print(colored(f'Mistake deriven LSVM stds of test error for 5 pairs: \n {np.array(perc_LSVM_stds)}', 'magenta'))

print(colored(f'Mistake deriven LSVM average test error on 5 pairs: \n       {list(np.round(np.mean(perc_LSVM_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven LSVM average stds of test error on 5 pairs: \n       {list(np.round(np.mean(perc_LSVM_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/LSVM-error.csv', 
            perc_LSVM_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/LSVM-std.csv', 
            perc_LSVM_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/LSVM-error-ave.csv', 
            np.round(np.mean(perc_LSVM_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/LSVM-std-ave.csv', 
            np.round(np.mean(perc_LSVM_stds, 0), decimals=4), delimiter=',')

perc_GSVM_errors = []
perc_GSVM_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='GSVM', 
                                C=100, gamma='auto', classifiers=[], epoch=50, maj_num=1, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_GSVM_errors.append(np.round(A[3], decimals=4)) 
    perc_GSVM_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven GSVM test errors for 5 pairs: \n {np.array(perc_GSVM_errors)}', 'green'))
print(colored(f'Mistake deriven GSVM stds of test error for 5 pairs: \n {np.array(perc_GSVM_stds)}', 'magenta'))

print(colored(f'Mistake deriven GSVM average test error on 5 pairs: \n {list(np.round(np.mean(perc_GSVM_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven GSVM average stds of test error on 5 pairs: \n {list(np.round(np.mean(perc_GSVM_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/GSVM-error.csv', 
            perc_GSVM_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/GSVM-std.csv', 
            perc_GSVM_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/GSVM-error-ave.csv', 
            np.round(np.mean(perc_GSVM_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/GSVM-std-ave.csv', 
            np.round(np.mean(perc_GSVM_stds, 0), decimals=4), delimiter=',')

perc_PSVM_errors = []
perc_PSVM_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='PSVM', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=1, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_PSVM_errors.append(np.round(A[3], decimals=4)) 
    perc_PSVM_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven PSVM test errors for 5 pairs: \n {np.array(perc_PSVM_errors)}', 'green'))
print(colored(f'Mistake deriven PSVM stds of test error for 5 pairs: \n {np.array(perc_PSVM_stds)}', 'magenta'))

print(colored(f'Mistake deriven PSVM average test error on 5 pairs: \n {list(np.round(np.mean(perc_PSVM_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven PSVM average stds of test error on 5 pairs: \n {list(np.round(np.mean(perc_PSVM_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/PSVM-error.csv', 
            perc_PSVM_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/PSVM-std.csv', 
            perc_PSVM_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/PSVM-error-ave.csv', 
            np.round(np.mean(perc_PSVM_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/PSVM-std-ave.csv', 
            np.round(np.mean(perc_PSVM_stds, 0), decimals=4), delimiter=',')

perc_DT_errors = []
perc_DT_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='DT', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=1, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_DT_errors.append(np.round(A[3], decimals=4)) 
    perc_DT_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven DT test errors for 5 pairs: \n {np.array(perc_DT_errors)}', 'green'))
print(colored(f'Mistake deriven DT stds of test error for 5 pairs: \n {np.array(perc_DT_stds)}', 'magenta'))

print(colored(f'Mistake deriven DT average test error on 5 pairs: \n {list(np.round(np.mean(perc_DT_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven DT average stds of test error on 5 pairs: \n {list(np.round(np.mean(perc_DT_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/DT-error.csv', 
            perc_DT_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/DT-std.csv', 
            perc_DT_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/DT-error-ave.csv', 
            np.round(np.mean(perc_DT_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/DT-std-ave.csv', 
            np.round(np.mean(perc_DT_stds, 0), decimals=4), delimiter=',')

perc_RF_errors = []
perc_RF_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='RF', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=1, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_RF_errors.append(np.round(A[3], decimals=4)) 
    perc_RF_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven RF test errors for 5 pairs: \n {np.array(perc_RF_errors)}', 'green'))
print(colored(f'Mistake deriven RF stds of test error for 5 pairs: \n {np.array(perc_RF_stds)}', 'magenta'))

print(colored(f'Mistake deriven RF average test error on 5 pairs: \n {list(np.round(np.mean(perc_RF_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven RF average stds of test error on 5 pairs: \n {list(np.round(np.mean(perc_RF_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/RF-error.csv', 
            perc_RF_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/RF-std.csv', 
            perc_RF_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/RF-error-ave.csv', 
            np.round(np.mean(perc_RF_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/RF-std-ave.csv', 
            np.round(np.mean(perc_RF_stds, 0), decimals=4), delimiter=',')

perc_KNN_errors = []
perc_KNN_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='KNN', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=1, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_KNN_errors.append(np.round(A[3], decimals=4)) 
    perc_KNN_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven KNN test errors for 5 pairs: \n {np.array(perc_KNN_errors)}', 'green'))
print(colored(f'Mistake deriven KNN stds of test error for 5 pairs: \n {np.array(perc_KNN_stds)}', 'magenta'))

print(colored(f'Mistake deriven KNN average test error on 5 pairs: \n {list(np.round(np.mean(perc_KNN_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven KNN average stds of test error on 5 pairs: \n {list(np.round(np.mean(perc_KNN_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/KNN-error.csv', 
            perc_KNN_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/KNN-std.csv', 
            perc_KNN_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/KNN-error-ave.csv', 
            np.round(np.mean(perc_KNN_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/KNN-std-ave.csv', 
            np.round(np.mean(perc_KNN_stds, 0), decimals=4), delimiter=',')

perc_LR_errors = []
perc_LR_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)

    classif = classification(data[n], data[m], Q_size=20, model='LR', 
                                C=100, gamma=10, classifiers=[], epoch=50, maj_num=1, 
                                init_iter=3, std_coeff=1, test_size=0.3, n_neighbors=5, 
                                n_estimators=50)

    A = classif.classification_Q()
    plt.plot(data[n][0][:,0], data[n][0][:,1])
    plt.plot(data[m][0][:,0], data[m][0][:,1])
    plt.show()

    perc_LR_errors.append(np.round(A[3], decimals=4)) 
    perc_LR_stds.append(np.round(A[4], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'Mistake deriven LR test errors for 5 pairs: \n {np.array(perc_LR_errors)}', 'green'))
print(colored(f'Mistake deriven LR stds of test error for 5 pairs: \n {np.array(perc_LR_stds)}', 'magenta'))

print(colored(f'Mistake deriven LR average test error on 5 pairs: \n {list(np.round(np.mean(perc_LR_errors, 0), decimals=4))}', 'yellow'))
print(colored(f'Mistake deriven LR average stds of test error on 5 pairs: \n {list(np.round(np.mean(perc_LR_stds, 0), decimals=4))}', 'blue'))

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/LR-error.csv', 
            perc_LR_errors, delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/LR-std.csv', 
            perc_LR_stds, delimiter=',')

np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/LR-error-ave.csv', 
            np.round(np.mean(perc_LR_errors, 0), decimals=4), delimiter=',')
np.savetxt('/content/gdrive/My Drive/Research/Characters/average test errors and stds (Perceptron-like)/LR-std-ave.csv', 
            np.round(np.mean(perc_LR_stds, 0), decimals=4), delimiter=',')

"""# Neural Network Classification

## Helper functions
"""

from collections import Counter

def find_majority(votes):
    vote_count = Counter(votes)
    top = vote_count.most_common(1)
    return top[0][0]

def find_majority_array(A): # column-wise majority
    return list(map(find_majority, A.T))

def ExpCurve2Vec(points, curves, mu):
    D = tt.distsbase.DistsBase()
    a = np.array([np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves])
    return a

def get_endpoints(data):
    n = len(data)
    data_endpoints = np.zeros((n, 4))
    for i in range(n):
        data_endpoints[i] = np.concatenate((data[i][0], data[i][-1]), 0)
    return data_endpoints

def train_test(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size = test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size = test_size)

    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
                train_idx_2, test_idx_2, train_label_2, test_label_2

def train_test_mu(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size=test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size=test_size)

    train_1 = data_1[train_idx_1]
    train_2 = data_2[train_idx_2]
    test_1 = data_1[test_idx_1]
    test_2 = data_2[test_idx_2]

    arr1 = np.arange(len(train_1)+len(train_2))
    I_1 = np.random.shuffle(arr1)

    arr2 = np.arange(len(test_1)+len(test_2))
    I_2 = np.random.shuffle(arr2)
    
    train = np.concatenate((train_1, train_2), 0)[arr1[I_1]]
    train_labels = np.concatenate((train_label_1, train_label_2), 0)[arr1[I_1]]
    test = np.concatenate((test_1, test_2), 0)[arr2[I_2]]
    test_labels = np.concatenate((test_label_1, test_label_2), 0)[arr2[I_2]]

    a = np.mean([np.mean(train_1[i], 0) for i in range(len(train_1))], 0)
    b = np.mean([np.mean(train_2[i], 0) for i in range(len(train_2))], 0)
    mu = max(abs(a-b))
    
    return mu, train[0], test[0], train_labels[0], test_labels[0]

def flatten(x):
    N = x.shape[0] 
    return x.view(N, -1)

class Flatten(nn.Module):
    def forward(self, x):
        return flatten(x)

"""## CNN"""

def neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 1e-3, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 10, 
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3):
    
    """ 
    in_channels: the dimension of hidden layer
    D_out: output dimension
    version: 'signed' or 'unsigned' or 'exp' 
    stride: should be fixed to 1
    """

    start_time = time.time()

    train_errors = np.zeros(epoch)
    test_errors = np.zeros(epoch)

    losses = torch.zeros(epoch, num_trials_maj, Num_updates)
    
    for s in range(epoch):

        mu, train, test, train_labels, test_labels = train_test_mu(data_1, data_2, test_size)

        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()
        
        x_preds = torch.zeros(num_trials_maj, len(train))
        y_preds = torch.zeros(num_trials_maj, len(test))

        Min = np.min([np.min(train[i], 0) for i in range(len(train))], 0)
        Max = np.max([np.max(train[i], 0) for i in range(len(train))], 0)
        Mean = np.mean([np.mean(train[i], 0) for i in range(len(train))], 0)
        Std = np.std([np.std(train[i], 0) for i in range(len(train))], 0)
        
        for t in range(num_trials_maj):
            Q = np.ones((Q_size, 2))
            Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], Q_size)
            Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], Q_size)

            if (version == 'unsigned' or version == 'signed'):
                train_data = curve2vec(Q, train, version = version, sigma = sigma)
                test_data = curve2vec(Q, test, version = version, sigma = sigma)
            elif version == 'exp':
                train_data = ExpCurve2Vec(Q, train, mu)
                test_data = ExpCurve2Vec(Q, test, mu)
            elif version == 'endpoints':
                train_data = get_endpoints(train)
                test_data = get_endpoints(test)
            
            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                #nn.LeakyReLU(0.01),
                                #nn.Tanh(),
                                Flatten(),
                                nn.Linear(out_channels * (len(train_data[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )
    
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
            
            train_data = torch.from_numpy(train_data).float()
            test_data = torch.from_numpy(test_data).float()

            train_data = train_data.view(len(train_data), 1, len(train_data[0]))
            
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)
                losses[s, t, k] = loss
                    
                if (k+1) % 1000 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

                optimizer.zero_grad()

                loss.backward() # Backward pass

                optimizer.step()  # Calling the step function on the Optimizer 
                
            x_preds[t] = torch.argmax(model(train_data), axis=1)
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))        
            y_preds[t] = torch.argmax(model(test_data), axis=1)
        
        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))
        
        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    plt.plot((torch.mean(losses, dim=(0,1))).detach().numpy())
    plt.show()

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
               np.round(train_error_mean, decimals=4), 
                np.round(test_error_mean, decimals=4),
                np.round(test_error_std, decimals=4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                columns=['Channel 1', 'Learning Rate', 'Train Error', 
                         'Test Error', 'Std Error'])
    
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf, test_error_mean, test_error_std

"""### Endpoints"""

endpoints_errors = []
endpoints_stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)
    
    A = neuralNetworkClassificationCNN(data[n], data[m], Q_size = 20, lr_decay = 0.9, 
                                       learning_rate = 1e-3, num_trials_maj = 1,
                                       out_channels = 10, kernel_size = 2,  
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'endpoints', 
                                       sigma = 100, test_size = 0.3)

    endpoints_errors.append(np.round(A[1], decimals=4)) 
    endpoints_stds.append(np.round(A[2], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'test errors for 5 pairs: \n {np.array(endpoints_errors)}', 'green'))
print(colored(f'stds of test error for 5 pairs: \n {np.array(endpoints_stds)}', 'magenta'))

print(colored(f'average test error on 5 pairs: \n {np.round(np.mean(endpoints_errors, 0), decimals=4)}', 'yellow'))
print(colored(f'average stds of test error on 5 pairs: \n {np.round(np.mean(endpoints_stds, 0), decimals=4)}', 'blue'))

"""### |Q|=20

### Majority = 1

#### Rand($v_Q$)
"""

errors = []
stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)
    
    A = neuralNetworkClassificationCNN(data[n], data[m], Q_size = 20, lr_decay = 0.9, 
                                       learning_rate = 2e-3, num_trials_maj = 1,
                                       out_channels = 10, kernel_size = 5,  
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'unsigned', 
                                       sigma = 100, test_size = 0.3)

    errors.append(np.round(A[1], decimals=4)) 
    stds.append(np.round(A[2], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'test errors for 5 pairs: {np.array(errors)}', 'green'))
print(colored(f'stds of test error for 5 pairs: {np.array(stds)}', 'magenta'))

print(colored(f'average test error on 5 pairs: {np.round(np.mean(errors, 0), decimals=4)}', 'yellow'))
print(colored(f'average stds of test error on 5 pairs: {np.round(np.mean(stds, 0), decimals=4)}', 'blue'))

"""#### Rand($v_Q^{\varsigma}$)"""

errors = []
stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)
    
    A = neuralNetworkClassificationCNN(data[n], data[m], Q_size = 20, lr_decay = 0.9, 
                                       learning_rate = 2e-3, num_trials_maj = 1,
                                       out_channels = 10, kernel_size = 5,  
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'signed', 
                                       sigma = 100, test_size = 0.3)

    errors.append(np.round(A[1], decimals=4)) 
    stds.append(np.round(A[2], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'test errors for 5 pairs: {np.array(errors)}', 'green'))
print(colored(f'stds of test error for 5 pairs: {np.array(stds)}', 'magenta'))

print(colored(f'average test error on 5 pairs: {np.round(np.mean(errors, 0), decimals=4)}', 'yellow'))
print(colored(f'average stds of test error on 5 pairs: {np.round(np.mean(stds, 0), decimals=4)}', 'blue'))

"""#### Rand($v_Q^{exp}$)"""

errors = []
stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)
    
    A = neuralNetworkClassificationCNN(data[n], data[m], Q_size = 20, lr_decay = 0.9, 
                                       learning_rate = 1e-2, num_trials_maj = 1,
                                       out_channels = 10, kernel_size = 5,  
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'exp', 
                                       sigma = 100, test_size = 0.3)

    errors.append(np.round(A[1], decimals=4)) 
    stds.append(np.round(A[2], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'test errors for 5 pairs: {np.array(errors)}', 'green'))
print(colored(f'stds of test error for 5 pairs: {np.array(stds)}', 'magenta'))

print(colored(f'average test error on 5 pairs: {np.round(np.mean(errors, 0), decimals=4)}', 'yellow'))
print(colored(f'average stds of test error on 5 pairs: {np.round(np.mean(stds, 0), decimals=4)}', 'blue'))

"""### Majority = 11

#### Vote(Rand $v_Q$)
"""

errors = []
stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)
    
    A = neuralNetworkClassificationCNN(data[n], data[m], Q_size = 20, lr_decay = 0.9, 
                                       learning_rate = 2e-3, num_trials_maj = 11,
                                       out_channels = 10, kernel_size = 5,  
                                       padding = 1, bias = True, Num_updates = 500, 
                                       D_out = 2, epoch = 50, version = 'unsigned', 
                                       sigma = 100, test_size = 0.3)

    errors.append(np.round(A[1], decimals=4)) 
    stds.append(np.round(A[2], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'test errors for 5 pairs: {np.array(errors)}', 'green'))
print(colored(f'stds of test error for 5 pairs: {np.array(stds)}', 'magenta'))

print(colored(f'average test error on 5 pairs: {np.round(np.mean(errors, 0), decimals=4)}', 'yellow'))
print(colored(f'average stds of test error on 5 pairs: {np.round(np.mean(stds, 0), decimals=4)}', 'blue'))

"""#### Vote(Rand $v_Q^{\varsigma}$)"""

errors = []
stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)
    
    A = neuralNetworkClassificationCNN(data[n], data[m], Q_size = 20, lr_decay = 0.9, 
                                       learning_rate = 1e-2, num_trials_maj = 11,
                                       out_channels = 10, kernel_size = 5,  
                                       padding = 1, bias = True, Num_updates = 500, 
                                       D_out = 2, epoch = 50, version = 'signed', 
                                       sigma = 100, test_size = 0.3)

    errors.append(np.round(A[1], decimals=4)) 
    stds.append(np.round(A[2], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'test errors for 5 pairs: {np.array(errors)}', 'green'))
print(colored(f'stds of test error for 5 pairs: {np.array(stds)}', 'magenta'))

print(colored(f'average test error on 5 pairs: {np.round(np.mean(errors, 0), decimals=4)}', 'yellow'))
print(colored(f'average stds of test error on 5 pairs: {np.round(np.mean(stds, 0), decimals=4)}', 'blue'))

"""#### Vote(Rand $v_Q^{exp}$)"""

errors = []
stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)
    
    A = neuralNetworkClassificationCNN(data[n], data[m], Q_size = 20, lr_decay = 0.9, 
                                       learning_rate = 1e-2, num_trials_maj = 11,
                                       out_channels = 10, kernel_size = 5,  
                                       padding = 1, bias = True, Num_updates = 500, 
                                       D_out = 2, epoch = 50, version = 'exp', 
                                       sigma = 100, test_size = 0.3)

    errors.append(np.round(A[1], decimals=4)) 
    stds.append(np.round(A[2], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'test errors for 5 pairs: {np.array(errors)}', 'green'))
print(colored(f'stds of test error for 5 pairs: {np.array(stds)}', 'magenta'))

print(colored(f'average test error on 5 pairs: {np.round(np.mean(errors, 0), decimals=4)}', 'yellow'))
print(colored(f'average stds of test error on 5 pairs: {np.round(np.mean(stds, 0), decimals=4)}', 'blue'))

"""# Neural Network for Mistake Driven Method of Choosing Q

## Initialize Q
"""

def initialize_Q(train_1, train_2, std_coeff, out_channels, kernel_size, Q_size, 
                 padding, learning_rate = 1e-3, bias = True, D_out=2, lr_decay = 0.9, 
                 Num_updates = 100): 
        
    Q = []
    errors = []
    losses = np.zeros((Q_size - kernel_size, Num_updates))
    
    mu = get_mu(train_1, train_2)
    std = mu * std_coeff

    trajectory_train_data = np.concatenate((train_1, train_2), axis = 0)
    train_labels = np.concatenate(([1] * len(train_1), [0] * len(train_2)), 0)
    train_labels = torch.from_numpy(train_labels).long()
    index = np.random.randint(0, high=len(trajectory_train_data)) 
    k = np.random.randint(0, high=len(trajectory_train_data[index]))
    for i in range(kernel_size):
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)

    for i in range(Q_size - kernel_size):
        train_data = ExpCurve2Vec(np.array(Q), trajectory_train_data, mu)
        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                            out_channels = out_channels, 
                            kernel_size = kernel_size,
                            stride  = 1,
                            padding = padding,
                            bias = bias),
                nn.ReLU(),
                #nn.LeakyReLU(0.01),
                #nn.Tanh(),
                Flatten(),
                nn.Linear(out_channels * (len(train_data[0]) - kernel_size + 1 + 2 * padding), 
                            D_out)
                )
    
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
        train_data = torch.from_numpy(train_data).float()
        train_data = train_data.view(len(train_data), 1, len(train_data[0]))
        
        for k in range(Num_updates):
            x_pred = model(train_data) # of shape (N, D_out)
            loss = loss_fn(x_pred, train_labels)
            losses[i, k] = loss
            
            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
            optimizer.zero_grad()
            loss.backward() 
            optimizer.step() 
        
        train_pred = torch.argmax(model(train_data), axis=1)
        scores = model(train_data)
        I = np.where((train_labels == train_pred) == False)[0]

        temp_labels = 2 * train_labels.numpy().reshape(len(train_labels.numpy()), 1) - 1
        temp = temp_labels * scores.detach().numpy()
        temp = np.max(temp, axis=1)
        index = I[np.argmax(temp[I])]

        error = sum(train_labels != train_pred)/len(train_labels)
        errors.append(error.item())
        
        k = np.random.randint(0, high=len(trajectory_train_data[index]))
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)

    final_error = errors[-1]

    #plt.plot((np.mean(losses, axis=0)))
    #plt.show()

    return np.array(Q), np.array(errors), mu, final_error

"""## Mistake Driven with CNN"""

def MD_NeuralNetworkClassificationCNN(data_1, data_2, maj_num, epoch, init_iter, 
                                      test_size, std_coeff, out_channels, kernel_size, 
                                      Q_size, padding, learning_rate = 1e-3, 
                                      bias = True, D_out=2, lr_decay = 0.9, 
                                      Num_updates = 100):
        
    start_time = time.time()

    train_errors = np.zeros(epoch) 
    test_errors = np.zeros(epoch)

    n_1 = len(data_1)
    n_2 = len(data_2) 

    for s in range(epoch):
        train_idx_1, test_idx_1, train_label_1, test_label_1, train_idx_2, \
        test_idx_2, train_label_2, test_label_2 = train_test(data_1, data_2, test_size)

        train = np.concatenate((data_1[train_idx_1], data_2[train_idx_2]), 0)
        test = np.concatenate((data_1[test_idx_1], data_2[test_idx_2]), 0)
        train_labels = np.concatenate((train_label_1, train_label_2), axis = 0)
        test_labels = np.concatenate((test_label_1, test_label_2), axis = 0)
        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()

        x_preds = torch.zeros((maj_num, len(train)))
        y_preds = torch.zeros((maj_num, len(test)))
        
        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        train_labels = train_labels[I]
        
        J = np.arange(len(test))
        np.random.shuffle(J)
        test = test[J]
        test_labels = test_labels[J]

        for t in range(maj_num):

            Q_list = []
            temp_errors = []
            mu_temp = []

            for j in range(init_iter):
                B = initialize_Q(data_1[train_idx_1], data_2[train_idx_2], 
                                 std_coeff, out_channels, kernel_size, Q_size, 
                                 padding, learning_rate = 1e-3, bias = True, 
                                 D_out=2, lr_decay = 0.9, Num_updates = 100)

                Q_list.append(B[0])
                mu_temp.append(B[2])
                temp_errors.append(B[-1])

            h = np.argmin(temp_errors)
            Q = Q_list[h]
            mu = mu_temp[h]

            train_data = torch.from_numpy(ExpCurve2Vec(Q, train, mu)).float()
            train_data = train_data.view(len(train_data), 1, len(train_data[0]))

            test_data = torch.from_numpy(ExpCurve2Vec(Q, test, mu)).float()
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))

            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                    nn.ReLU(),
                    #nn.LeakyReLU(0.01),
                    #nn.Tanh(),
                    Flatten(),
                    nn.Linear(out_channels * (len(train_data[0][0]) - kernel_size + 1 + 2 * padding), 
                              D_out)
                    )
        
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)

                if (k+1) % 10 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
                optimizer.zero_grad()
                loss.backward() 
                optimizer.step() 

            scores = model(train_data)
            
            x_preds[t] = torch.argmax(scores, axis=1)
            y_preds[t] = torch.argmax(model(test_data), axis=1)

        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))

        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
                            np.round(train_error_mean, decimals = 4), 
                            np.round(test_error_mean, decimals = 4),
                            np.round(test_error_std, decimals = 4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                                 columns=['Channel 1', 'Learning Rate', 
                                          'Train Error', 'Test Error', 'Std Error'])

    print(colored(f"total time = {time.time() - start_time}", "red"))
    print("mu =", mu)

    return pdf, test_error_mean, test_error_std

"""## With num_maj = 1"""

errors = []
stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)
    
    A = MD_NeuralNetworkClassificationCNN(data[m], data[n], maj_num=1, epoch=50, init_iter=3, 
                                          test_size=0.3, std_coeff=1, out_channels=10, 
                                          kernel_size = 5, Q_size = 10, padding = 1, 
                                          learning_rate = 1e-2, bias = True, D_out=2, 
                                          lr_decay = 0.9, Num_updates = 1000)

    errors.append(np.round(A[1], decimals=4)) 
    stds.append(np.round(A[2], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'test errors for 5 pairs: {np.array(errors)}', 'green'))
print(colored(f'stds of test error for 5 pairs: {np.array(stds)}', 'magenta'))

print(colored(f'average test error on 5 pairs: {np.round(np.mean(errors, 0), decimals=4)}', 'yellow'))
print(colored(f'average stds of test error on 5 pairs: {np.round(np.mean(stds, 0), decimals=4)}', 'blue'))

"""## With num_maj = 11"""

errors = []
stds = []

for i in range(len(pairs)):
    m, n = pairs[i]
    print("pairs =", f"{m},", n)
    
    A = MD_NeuralNetworkClassificationCNN(data[m], data[n], maj_num=11, epoch=50, init_iter=3, 
                                          test_size=0.3, std_coeff=1, out_channels=10, 
                                          kernel_size = 5, Q_size = 10, padding = 1, 
                                          learning_rate = 1e-2, bias = True, D_out=2, 
                                          lr_decay = 0.9, Num_updates = 1000)

    errors.append(np.round(A[1], decimals=4)) 
    stds.append(np.round(A[2], decimals=4))
    
    print(A[0])
    print("===========================================================================")
    
print(colored(f'test errors for 5 pairs: {np.array(errors)}', 'green'))
print(colored(f'stds of test error for 5 pairs: {np.array(stds)}', 'magenta'))

print(colored(f'average test error on 5 pairs: {np.round(np.mean(errors, 0), decimals=4)}', 'yellow'))
print(colored(f'average stds of test error on 5 pairs: {np.round(np.mean(stds, 0), decimals=4)}', 'blue'))

"""# Plot test errors"""

A_LSVM = [0.0556, 0.0644, 0.0579, 0.0933, 0.1138, 0.1060, 0.0941, 0.1440, 0.2291]
A_GSVM = [0.0512, 0.0599, 0.0470, 0.0775, 0.1143, 0.0843, 0.0860, 0.1429, 0.1814]
A_PSVM = [0.0662, 0.0717, 0.0674, 0.0914, 0.1434, 0.1026, 0.1078, 0.1533, 0.2745]
A_DT = [0.1013, 0.0904, 0.0703, 0.0900, 0.1885, 0.1610, 0.1370, 0.1733, 0.2137]
A_RF = [0.0709, 0.0926, 0.0705, 0.0884, 0.1070, 0.1105, 0.0934, 0.1237, 0.1972]
A_KNN = [0.0931, 0.1050, 0.0860, 0.1025, 0.1447, 0.1213, 0.1158, 0.1562, 0.2061]
A_LR = [0.0820, 0.0640, 0.1501, 0.1199, 0.1220, 0.0903, 0.1752, 0.1615, 0.2300]
A_CNN = [0.1901, 0.0929, 0.0728, 0.1183, 0.1913, 0.1000, 0.0844, 0.1197, 0.1816]

A = np.array([A_LSVM, A_GSVM, A_PSVM, A_DT, A_RF, A_KNN, A_LR, A_CNN])
B = A.T

std_LSVM = [0.0231, 0.0247, 0.0213, 0.0281, 0.0369, 0.0401, 0.0371, 0.0456, 0.0334]
std_GSVM = [0.0229, 0.0240, 0.0183, 0.0240, 0.0490, 0.0332, 0.0333, 0.0438, 0.0297]
std_PSVM = [0.0268, 0.0268, 0.0235, 0.0290, 0.0459, 0.0372, 0.0366, 0.0473, 0.0379]
std_DT = [0.0325, 0.0302, 0.0254, 0.0273, 0.0510, 0.0488, 0.0398, 0.0536, 0.0350]
std_RF = [0.0258, 0.0295, 0.0257, 0.0269, 0.0407, 0.0389, 0.0310, 0.0435, 0.0288]
std_KNN = [0.0315, 0.0317, 0.0271, 0.0289, 0.0438, 0.0408, 0.0351, 0.0489, 0.0312]
std_LR = [0.0278, 0.0246, 0.0371, 0.0334, 0.0421, 0.0357, 0.0455, 0.0487, 0.0337]
std_CNN = [0.0673, 0.0346, 0.0302, 0.0397, 0.0692, 0.0359, 0.0375, 0.0428, 0.0302]

C = np.array([std_LSVM, std_GSVM, std_PSVM, std_DT, std_RF, std_KNN, std_LR, std_CNN]).T

errors = [0.0734, 0.0606, 0.0797, 0.0745, 0.0591, 0.0473, 0.1532, 0.1209, 0.1496,
          0.0167, 0.1947, 0.1786]

stds = [0.0284, 0.0199, 0.0273, 0.0228, 0.0212, 0.0154, 0.0352, 0.0322, 0.0330, 
        0.0117, 0.0526, 0.0333]

classifiers = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

FM = (r'KNN', r'Vote(MD $v_Q^{exp}$)', r'Vote(Rand $v_Q$)', r'Vote($v_Q^{\varsigma}$)', 
      r'Vote(Rand $v_Q^{exp}$)', r'MD $v_Q^{exp}$', r'Rand $v_Q$', 
      r'Rand $v_Q^{\varsigma}$', r'Rand $v_Q^{exp}$', 'Endpoints')

labels = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

dists = [r'$d_F$', r'$d_{dF}$', r'$dH$', r'DTW', r'soft-dtw', r'fastdtw', 
         r'LCSS', r'SSPD', r'EDR', r'ERP', r'LSH', r'$d_Q^{\pi}$']

def lower_limit_error(x, y):
    if y - x < 0:
        return y
    else:
        return x

lower_limit_error = np.vectorize(lower_limit_error)

width=0.1
index = np.arange(len(stds))
ind = np.arange(len(A)) + len(index) * 0.25

plt.subplots(figsize = (18, 6), tight_layout=True)
bars = [0] * 10

lower_lim = lower_limit_error(stds, errors)
bars[0] = plt.bar(index * 0.2, errors, width, yerr=[lower_lim, stds], capsize=2)

for i in range(len(FM)-1):
    lower_limit = lower_limit_error(C[i], B[i])
    bars[i+1] = plt.bar(ind+width*i-0.3, B[i], width, yerr=[lower_limit, C[i]], capsize=2)

plt.title('Characters Data', fontsize = 20)
plt.xticks(list(np.arange(len(stds)) * 0.2) + list(ind + 0.5 * width), 
           dists + labels, fontsize = 14)

plt.legend(tuple(bars), FM, loc=0, fontsize = 12)

plt.gca().yaxis.grid(color='gray', linestyle='dotted', linewidth=0.6)
plt.xticks(rotation='vertical', fontsize = 14)
plt.yticks(fontsize = 14)

path = '/content/gdrive/My Drive/Research/plots/plots characters/characters bar chart all horizontal.png'
plt.savefig(path, bbox_inches='tight', dpi=200)

plt.show()

