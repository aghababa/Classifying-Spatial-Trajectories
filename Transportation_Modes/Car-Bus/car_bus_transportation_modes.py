# -*- coding: utf-8 -*-
"""Car_Bus_Transportation_Modes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hnrABbawZ7C2C-d1kaPgCrh5JMCETscJ
"""

pip install trjtrypy

"""# Libraries"""

import glob
import numpy as np 
import time
import math
import random
from scipy import linalg as LA
import pandas as pd
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.svm import NuSVC
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import Perceptron
import statsmodels.api as sm
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
from termcolor import colored
import matplotlib as mpl
from scipy import optimize
from sklearn.svm import LinearSVC
from termcolor import colored
from scipy.stats import entropy
import os
import ast
import csv
import json 
import scipy.io
import trjtrypy as tt
from trjtrypy.featureMappings import curve2vec
from trjtrypy.distances import d_Q_pi


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import sampler
import torchvision.transforms as T

from google.colab import drive
drive.mount("/content/gdrive")

"""# Read data"""

def read_file(file_name):
    data = []
    with open(file_name, "r") as f:
        for line in f:
            item = line.strip().split(",")
            data.append(np.array(item))
    return np.array(data)

data1 = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_tracks.csv')[1:,:-1]
data2 = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_trackspoints.csv')[1:,1:4]

float1 = np.vectorize(float)
int1 = np.vectorize(int)
track_id = int1(data1[:,0])
labels = np.where(int1(data1[:,-1]) < 1.5, int1(data1[:,-1]), -1) 
traj = float1(data2)

sum(labels==1), sum(labels==-1)

trajec = [0] * 163

for i in range(163):
    trajec[i] = []
    I = np.where(traj[:,2] == track_id[i])
    trajec[i] = np.array([labels[i], traj[I]], dtype = 'object')

trajec = np.array(trajec)

trajectory = [0] * 163
trajectory_label_id = [0] * 163

for i in range(163):
    trajectory[i] = trajec[i][1][:,:2]
    trajectory_label_id[i] = np.array([trajec[i][1][:,:2], trajec[i][0], trajec[i][1][:,2][0]], dtype = 'object')
    
trajectory_label_id = np.array(trajectory_label_id, dtype = 'object')
trajectory = np.array(trajectory, dtype = 'object')

min_length = 10
max_length = 1000 #160 for balance data
l = 0
index = [] 
for i in range(163):
    if len(trajectory[i]) < min_length or len(trajectory[i]) > max_length:
        l = l + 1
    else:
        index.append(i)
        
l, 163-l

trajectories = [0] * (163-l)
trajectories_label_id = [0] * (163-l)

j = 0
for i in range(163):
    if len(trajectory[i]) >= min_length and len(trajectory[i]) <= max_length:
        trajectories[j] = np.array(trajectory[i])
        trajectories_label_id[j] = trajectory_label_id[i]
        j = j + 1

trajectories_label_id = np.array(trajectories_label_id, dtype = 'object')
trajectories = np.array(trajectories, dtype = 'object')

cars = trajectories_label_id[np.where(trajectories_label_id[:,1] == 1)][:,:2][:,0]
buses = trajectories_label_id[np.where(trajectories_label_id[:,1] == -1)][:,:2][:,0]

cars_copy = cars.copy()
buses_copy = buses.copy()
len(cars), len(buses)

def remove_segments(traj): # removes stationary points
    p2 = traj[1:]
    p1 = traj[:-1]
    L = ((p2-p1)*(p2-p1)).sum(axis =1)
    I = np.where(L>1e-16)[0]
    return traj[I]

cars = np.array(list(map(remove_segments, cars)), dtype='object')
buses = np.array(list(map(remove_segments, buses)), dtype='object')

I = np.where(np.array([len(cars[i]) for i in range(len(cars))]) > 1)
J = np.where(np.array([len(buses[i]) for i in range(len(buses))]) > 1)

cars = cars[I]
buses = buses[J]

cars_copy = cars.copy()
buses_copy = buses.copy()
print("len(cars), len(buses)=", len(cars), len(buses))

"""# Removing 2 outliers"""

a = np.arange(len(cars))
I = np.where((a != 28) & (a != 29))
cars = cars[I]
buses = buses[:-1]
len(cars), len(buses)

a, c = np.min((np.min([np.min(cars[i], axis=0) for i in range(len(cars))], axis=0), 
       np.min([np.min(buses[i], axis=0) for i in range(len(buses))], axis=0)), axis=0)
  
b, d = np.max((np.max([np.max(cars[i], axis=0) for i in range(len(cars))], axis=0), 
               np.max([np.max(buses[i], axis=0) for i in range(len(buses))], axis=0)), axis=0)

m = 20
Q = np.ones((m,2))

Q[:,0] = 0.2 * (b - a ) * np.random.random_sample(m) + a - 0.01
Q[:,1] = 0.2 *(d - c ) * np.random.random_sample(m) + c - 0.02

for i in range(len(cars)):
    plt.plot(cars[i][:,0], cars[i][:,1], color = "steelblue");
for i in range(len(buses)):
    plt.plot(buses[i][:,0], buses[i][:,1], color = "r");
plt.scatter(Q[:,0], Q[:,1], color = "black")
print(colored(f'Original car-bus', 'yellow'))
plt.show()

"""# Classifiers and get_mu function"""

CC = [100, 100, 10]
number_estimators = [50, 50]


clf0 = [make_pipeline(LinearSVC(dual=False, C=CC[0], tol=1e-5, 
                               class_weight ='balanced', max_iter=1000)), 
        "SVM, LinearSVC, C = "+str(CC[0])]
clf1 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[1], kernel='rbf', gamma='auto', max_iter=200000)),
        "Gaussian SVM, C="+str(CC[1])+", gamma=auto"]
clf2 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[2], kernel='poly', degree=3, max_iter=400000)),
        "Poly kernel SVM, C="+str(CC[2])+", deg=auto"]
clf3 = [DecisionTreeClassifier(), "Decision Tree"]
clf4 = [RandomForestClassifier(n_estimators=number_estimators[0]), 
         "RandomForestClassifier, n="+str(number_estimators[0])]
clf5 = [KNeighborsClassifier(n_neighbors=5), "KNN"]
clf6 = [LogisticRegression(solver='newton-cg'), "Logistic Regression"]

clf = [clf0, clf1, clf2, clf3, clf4, clf5, clf6]
classifs = [item[0] for item in clf]
keys = [item[1] for item in clf]

def get_mu(data_1, data_2):
    a = np.mean([np.mean(data_1[i], 0) for i in range(len(data_1))], 0)
    b = np.mean([np.mean(data_2[i], 0) for i in range(len(data_2))], 0)
    c = abs(a-b)
    return max(c)

"""# Using physical and geometric features together

## Length, speed, acceleration and jerk functions
"""

# average speed from data
A = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_tracks.csv')[1:,:]

I = np.where(np.array(list(map(int, A[1:,-2]))) == 1)[0]
print(np.mean(list(map(float, A[I][:,2]))))

J = np.where(np.array(list(map(int, A[1:,-2]))) == 2)[0]
np.mean(list(map(float, A[J][:,2])))

# length calculator 
def length(x):
    p1 = x[:,:2][:-1]
    p2 = x[:,:2][1:]
    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))
    Length = [sum(L)/len(L)]
    return Length

# speed calculator 
def speed(x):
    t = x[:,2][1:] - x[:,2][:-1] + 1e-10
    p1 = x[:,:2][:-1]
    p2 = x[:,:2][1:]
    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))
    speeds = L/t
    s = [sum(speeds)/len(speeds)]
    return s

# acceleration calculator
def acceleration(x):
    t = x[:,2][1:] - x[:,2][:-1] + 1e-10
    p1 = x[:,:2][:-1]
    p2 = x[:,:2][1:]
    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))
    accelerations = L/t**2
    a = [1e-5 * sum(accelerations)/len(accelerations)]
    return a

# jerk calculator 
def jerk(x):
    t = x[:,2][1:] - x[:,2][:-1] + 1e-10
    p1 = x[:,:2][:-1]
    p2 = x[:,:2][1:]
    L = np.sqrt(((p2-p1)*(p2-p1)).sum(axis =1))
    jerks = L/t**3
    a = [1e-5 * sum(jerks)/len(jerks)]
    return a

def train_test(data1, data2, test_size):
    
    n_1 = len(data1)
    n_2 = len(data2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 = \
            train_test_split(np.arange(n_1), np.ones(n_1), test_size=test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 = \
    train_test_split(np.arange(n_2), -np.ones(n_2), test_size=test_size)
    
    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
            train_idx_2, test_idx_2, train_label_2, test_label_2

"""## Prepairing data to get physical features """

# create 'daysDate' function to convert start and end time to a float number of days
from datetime import datetime
def days_date(time_str):
    date_format = "%Y-%m-%d %H:%M:%S"
    current = datetime.strptime(time_str, date_format)
    date_format = "%Y-%m-%d"
    bench = datetime.strptime('2006-12-30', date_format)
    no_days = current - bench
    delta_time_days = no_days.days + current.hour / 24.0 + current.minute / (24. * 60.) + current.second / (24. * 3600.)
    return delta_time_days

days_date = np.vectorize(days_date)
float1 = np.vectorize(float)

A = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_tracks.csv')#[1:,:]
print(A[:2])
traj1 = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_trackspoints.csv')[1:,1:5]
traj1[:, -1] = [traj1[:, -1][i][1:-1] for i in range(len(traj1))]
traj1[:, -1] = days_date(traj1[:, -1])
traj1 = float1(traj1)
traj1[:2]
# B is the data with time in last dimension

trajec1 = [0] * 163

for i in range(163):
    trajec1[i] = []
    I = np.where(traj[:,2] == track_id[i])
    trajec1[i] = np.array([labels[i], traj1[:,[0,1,3]][I]], dtype='object')

trajec1 = np.array(trajec1)

trajectory1 = [0] * 163
trajectory_label_id_1 = [0] * 163

for i in range(163):
    trajectory1[i] = trajec1[i][1]
    trajectory_label_id_1[i] = np.array([trajec1[i][1], trajec1[i][0], 
                                         trajec1[i][1][0]], dtype='object')

trajectory_label_id_1 = np.array(trajectory_label_id_1, dtype='object')
trajectory1 = np.array(trajectory1, dtype='object')

min_length = 10
max_length = 1000 #160 for balance data
l = 0
index = [] 
for i in range(163):
    if len(trajectory1[i]) < min_length or len(trajectory1[i]) > max_length:
        l = l + 1
    else:
        index.append(i)
        
l, 163-l

trajectories1 = [0] * (163-l)
trajectories_label_id_1 = [0] * (163-l)

j = 0
for i in range(163):
    if len(trajectory1[i]) >= min_length and len(trajectory1[i]) <= max_length:
        trajectories1[j] = np.array(trajectory1[i])
        trajectories_label_id_1[j] = trajectory_label_id_1[i]
        j = j + 1

trajectories_label_id_1 = np.array(trajectories_label_id_1, dtype='object')
trajectories1 = np.array(trajectories1, dtype='object')

cars_time = trajectories_label_id_1[np.where(trajectories_label_id_1[:,1] == 1)][:,:2][:,0]
buses_time = trajectories_label_id_1[np.where(trajectories_label_id_1[:,1] == -1)][:,:2][:,0]
cars_time_copy = cars_time.copy()
buses_time_copy = buses_time.copy()
len(cars_time), len(buses_time)

a = np.arange(len(cars_time))
I = np.where((a != 28) & (a != 29))
cars_time = cars_time[I]
buses_time = buses_time[:-1]
len(cars_time), len(buses_time)

def remove_segments_time(traj): # removes stationary points
    p2 = traj[:,:2][1:]
    p1 = traj[:,:2][:-1]
    L = ((p2-p1)*(p2-p1)).sum(axis =1)
    I = np.where(L>1e-16)[0]
    return traj[I]

cars_time = np.array(list(map(remove_segments, cars_time)), dtype='object')
buses_time = np.array(list(map(remove_segments, buses_time)), dtype='object')

cars_time_copy = cars_time.copy()
buses_time_copy = buses_time.copy()
print("len(cars), len(buses)=", len(cars_time), len(buses_time))

"""## Classification with only physical features"""

def get_features(data_1, data_2, Leng=True, spd=True, accn=True, jrk=True):

    data = np.concatenate((data_1, data_2), 0)
    n = len(data)
    data_traj_1 = np.array([data_1[i][:,:2] for i in range(len(data_1))])
    data_traj_2 = np.array([data_2[i][:,:2] for i in range(len(data_2))])
    data_traj = np.array([data[i][:,:2] for i in range(n)])
    data_labels = np.array([1] * len(data_1) + [-1] * len(data_2))
    A = data_labels.copy().reshape(-1,1)

    if Leng == True:
        length_ = np.array([length(data_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(data[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(data[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(data[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A[:, 1:]

def classification_physical_features(data_1, data_2, epoch, classifiers, 
                                       Leng=True, spd=True, accn=True, jrk=True, 
                                       test_size=0.3):
    start_time = time.time()
    models = [item[0] for item in classifiers]
    keys = [item[1] for item in classifiers]
    r = len(classifiers)

    train_error_mean = np.ones(r)
    test_error_mean = np.ones(r)
    test_error_std = np.ones(r)
    
    train_errors = np.ones((r, epoch)) 
    test_errors = np.ones((r, epoch))

    data = get_features(data_1, data_2, Leng=Leng, spd=spd, accn=accn, jrk=jrk)
    data1 = data[:len(data_1)]
    data2 = data[len(data_1):]

    for s in range(epoch):
        train_idx_1, test_idx_1, train_labels_1, test_labels_1, \
            train_idx_2, test_idx_2, train_labels_2, test_labels_2 \
                = train_test(data1, data2, test_size=test_size)

        train1 = data1[train_idx_1]
        train2 = data2[train_idx_2]
        test1 = data1[test_idx_1]
        test2 = data2[test_idx_2]

        I = np.arange(len(train1)+len(train2))
        np.random.shuffle(I)
        J = np.arange(len(test1)+len(test2))
        np.random.shuffle(J)

        train = np.concatenate((train1, train2), 0)[I]
        test = np.concatenate((test1, test2), 0)[J]
        train_labels = np.concatenate((train_labels_1, train_labels_2), 0)[I]
        test_labels = np.concatenate((test_labels_1, test_labels_2), 0)[J]

        x_preds = np.zeros((r, len(train)))
        y_preds = np.zeros((r, len(test)))

        for k in range(r): 
            Model = models[k]
            Model.fit(train, train_labels)
            x_preds[k] = Model.predict(train)                
            y_preds[k] = Model.predict(test)

        for k in range(r):
            train_errors[k][s] = sum(train_labels != x_preds[k])/len(train_labels)
            test_errors[k][s] = sum(test_labels != y_preds[k])/len(test_labels)

    for k in range(r):
        train_error_mean[k] = np.mean(train_errors[k])
        test_error_mean[k] = np.mean(test_errors[k])
        test_error_std[k] = np.std(test_errors[k])

    Dict = {}

    for k in range(len(keys)): 
        Dict[k+1] = [keys[k], np.round(train_error_mean[k], decimals=4), 
                     np.round(test_error_mean[k], decimals=4),
                     np.round(test_error_std[k], decimals=4)]

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                columns=['Classifier','Train Error', 'Test Error', 'Std Error'])
    
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf, train_error_mean, test_error_mean

classification_physical_features(cars_time, buses_time, epoch=50, classifiers=clf, 
                                 Leng=True, spd=True, accn=True, jrk=True, 
                                 test_size=0.3)[0]

"""## Classification with $v_Q^+$ which is random Q and physical features"""

def get_features_Q(train_1, train_2, version='unsigned', sigma=1, Q_size=20, 
                   Leng=True, spd=True, accn=True, jrk=True):

    train = np.concatenate((train_1, train_2), 0)
    n = len(train)
    train_traj_1 = np.array([train_1[i][:,:2] for i in range(len(train_1))])
    train_traj_2 = np.array([train_2[i][:,:2] for i in range(len(train_2))])
    train_traj = np.array([train[i][:,:2] for i in range(n)])

    #Min = np.min([np.min(train_traj[i], 0) for i in range(n)], 0)
    #Max = np.max([np.max(train_traj[i], 0) for i in range(n)], 0)
    Mean = np.mean([np.mean(train_traj[i], 0) for i in range(n)], 0)
    Std = np.std([np.std(train_traj[i], 0) for i in range(n)], 0)
    Q = np.ones((Q_size,2))
    Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], Q_size)
    Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], Q_size)

    #Q[:,0] = (Max[0] - Min[0] + 0.002) * np.random.random_sample(Q_size) + Min[0] - 0.02
    #Q[:,1] = (Max[1] - Min[1] + 0.002) * np.random.random_sample(Q_size) + Min[1] - 0.01

    A = np.array(curve2vec(Q, train_traj, version=version, sigma=sigma))
    
    if Leng == True:
        length_ = np.array([length(train_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A, Q

def get_test_data_Q(test_1, test_2, Q, version='unsigned', sigma=1, Leng=True, 
                    spd=True, accn=True, jrk=True):

    test = np.concatenate((test_1, test_2), 0)
    n = len(test)
    test_traj_1 = np.array([test_1[i][:,:2] for i in range(len(test_1))])
    test_traj_2 = np.array([test_2[i][:,:2] for i in range(len(test_2))])
    test_traj = np.array([test[i][:,:2] for i in range(n)])

    A = np.array(curve2vec(Q, test_traj, version=version, sigma=sigma))
    
    if Leng == True:
        length_ = np.array([length(test_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A

def classification_Q_physics(data1, data2, version='unsigned', sigma=1, Q_size=20, 
                     Leng=True, spd=True, accn=True, jrk=True, num_trials=10, 
                     classifiers=clf):

    Start_time = time.time()
    models = [item[0] for item in classifiers]
    keys = [item[1] for item in classifiers]
    r = len(classifiers)
    train_error_mean = np.zeros(r)
    test_error_mean = np.zeros(r)
    test_error_std = np.zeros(r)
    train_error_list = np.zeros((r, num_trials)) 
    test_error_list = np.zeros((r, num_trials))

    for s in range(num_trials):

        train_idx_1, test_idx_1, train_labels_1, test_labels_1, \
            train_idx_2, test_idx_2, train_labels_2, test_labels_2 \
            = train_test(data1, data2, test_size=0.3)

        train1 = data1[train_idx_1]
        train2 = data2[train_idx_2]
        test1 = data1[test_idx_1]
        test2 = data2[test_idx_2]

        train, Q = get_features_Q(train1, train2, version, sigma,
                                  Q_size, Leng, spd, accn, jrk)
        #train = (train-np.mean(train,0))/np.std(train,0)
        train_labels = np.concatenate((train_labels_1, train_labels_2), 0)

        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        train_labels = train_labels[I]

        J = np.arange(len(test1)+len(test2))
        np.random.shuffle(J)

        test = get_test_data_Q(test1, test2, Q, version=version, sigma=sigma, 
                               Leng=Leng, spd=spd, accn=accn, jrk=jrk)[J]
        test_labels = np.concatenate((test_labels_1, test_labels_2), 0)[J]
        #test = (test-np.mean(test,0))/np.std(test,0)

        for k in range(r):            
            model = models[k]

            #Train the model using the training sets
            model.fit(train, train_labels)

            #Predict train labels
            train_pred = model.predict(train)
            err = sum(train_labels != train_pred)/len(train_labels)
            train_error_list[k][s] = err
            
            #Predict test labels
            test_pred = model.predict(test)
            er = sum(test_labels != test_pred)/len(test_labels)
            test_error_list[k][s] = er
            
    for k in range(r):
        train_error_mean[k] = np.mean(train_error_list[k])
        test_error_mean[k] = np.mean(test_error_list[k])
        test_error_std[k] = np.std(test_error_list[k])
    
    Dic = {}

    for k in range(len(keys)): 
        Dic[k] = [keys[k], np.round(train_error_mean[k], decimals = 4), 
                    np.round(test_error_mean[k], decimals = 4),
                    np.round(test_error_std[k], decimals = 4)]

    pdf = pd.DataFrame.from_dict(Dic, orient='index', columns=['Classifier','Train Error', 
                                                               'Test Error', 'Std Error'])
    print(colored(f"Total time: {time.time() - Start_time}", 'red'))
    return pdf

"""### With adding all physical pheatures

#### $v_Q^+$
"""

classification_Q_physics(cars_time, buses_time, version='unsigned',  
                         Q_size=20, Leng=True, spd=True, accn=True, jrk=True, 
                         num_trials=50, classifiers=clf)

"""#### $v_Q^{\varsigma +}$"""

classification_Q_physics(cars_time, buses_time, version='signed', sigma=1, 
                        Q_size=20, Leng=True, spd=True, accn=True, jrk=True, 
                        num_trials=50, classifiers=clf)

"""## Using Perceptron-like algorithm with physical features"""

import trjtrypy as tt
from trjtrypy.featureMappings import curve2vec

def ExpCurve2Vec(points,curves,mu):
    D=tt.distsbase.DistsBase()
    return [np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves]

def initialize_Q(train_1, train_2, C, gamma, Q_size, mu_coeff, model, Leng=True,
                 spd=True, accn=True, jrk=True, n_estimators=50):
    #Start_time = time.time()
    train = np.concatenate((train_1, train_2), 0)
    n = len(train)
    train_traj_1 = np.array([train_1[i][:,:2] for i in range(len(train_1))])
    train_traj_2 = np.array([train_2[i][:,:2] for i in range(len(train_2))])
    train_traj = np.array([train[i][:,:2] for i in range(n)])
    train_labels = np.array([1] * len(train_1) + [-1] * len(train_2))
    
    A = train_labels.copy().reshape(-1,1)
    if Leng == True:
        length_ = np.array([length(train_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    #print((A-np.mean(A,0))/np.std(A,0))

# Make the classifier
    if model == 'LSVM':
        Model = make_pipeline(LinearSVC(dual=False, C=C, tol=1e-5, 
                                        class_weight='balanced', max_iter=1000))
    elif model == 'GSVM':
        Model = make_pipeline(StandardScaler(), svm.SVC(C=C, kernel='rbf', 
                                                gamma=gamma, max_iter=200000))
    elif model == 'PSVM':
        Model = make_pipeline(StandardScaler(), svm.SVC(C=C, kernel='poly', 
                                                        degree=3, max_iter=400000))
    elif model == 'DT':
        Model = DecisionTreeClassifier()
    elif model == 'RF':
        Model = RandomForestClassifier(n_estimators=n_estimators)
    elif model == 'KNN':
        Model = KNeighborsClassifier(n_neighbors=5)
    elif model == 'LR':
        Model = LogisticRegression(solver='newton-cg')
    elif model == 'Prn':
        Model = Perceptron(tol=1e-5, validation_fraction=0.01, class_weight="balanced")
    else:
        print("model is not supported")
        
    mu = get_mu(train_traj_1, train_traj_2) * mu_coeff
    std = mu/2
    errors = []
    Q = []

    if sum([Leng, spd, accn, jrk]) > 0:
        Train_0 = A[:,1:]
        #Train_0 = (A[:,1:] - np.mean(A[:,1:] , 0))/np.std(A[:,1:],0)
        Model.fit(Train_0, train_labels)
        train_pred = Model.predict(Train_0) 
        error = 1 - metrics.accuracy_score(train_labels, train_pred)
        errors.append(error) 
        if model in ['LSVM', 'GSVM', 'PSVM', 'LR', 'Prn']:
            index = np.argmax(np.max(Model.decision_function(Train_0)))
        else:
            probs = Model.predict_proba(Train_0)
            index = np.argmax(entropy(probs, axis=1))
    else:
        index = np.random.randint(0, high=n) 
    k = np.random.randint(0, high=len(train_traj[index]))
    q = train_traj[index][k] + np.random.normal(0, std, 2)
    Q.append(q)

# Iteratively choose landmarks
    for i in range(1, Q_size):
        if sum([Leng, spd, accn, jrk]) > 0:
            Train_0 = A[:,1:]
            Train = np.concatenate((Train_0, np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))), 1)
        else:
            Train = np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))

        Model.fit(Train, train_labels)
        train_pred = Model.predict(Train)
        error = 1 - metrics.accuracy_score(train_labels, train_pred)
        errors.append(error)
        if model in ['LSVM', 'GSVM', 'PSVM', 'LR', 'Prn']:
            index = np.argmax(np.max(Model.decision_function(Train)))
        else:
            probs = Model.predict_proba(Train)
            index = np.argmax(entropy(probs, axis=1))

        k = np.random.randint(0, high=len(train_traj[index]))
        q = train_traj[index][k] + np.random.normal(0, std, 2)
        Q.append(q)
    if sum([Leng, spd, accn, jrk]) > 0:
        Train = np.concatenate((Train_0, np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))), 1)
    else: 
        Train = ExpCurve2Vec(np.array(Q), train_traj, mu)
    Model.fit(Train, train_labels)
    train_pred = Model.predict(Train)

    error = 1 - metrics.accuracy_score(train_labels, train_pred)
    errors.append(error)

    #print(colored(f"Total time for mapping row data: {time.time() - Start_time}", 'green'))

    return np.array(Q), mu, np.array(errors), error

def train_test(data1, data2, test_size):
    
    n_1 = len(data1)
    n_2 = len(data2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 = \
            train_test_split(np.arange(n_1), np.ones(n_1), test_size=test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 = \
    train_test_split(np.arange(n_2), -np.ones(n_2), test_size=test_size)
    
    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
            train_idx_2, test_idx_2, train_label_2, test_label_2

def classification_init_Q(data1, data2, C, gamma, Q_size, mu_coeff, model, epoch, 
                          init_iter, classifiers, Leng=True, spd=True, accn=True, 
                          jrk=True, test_size=0.3, normal=False, n_estimators=50):

    start_time = time.time()
    models = [item[0] for item in classifiers]
    keys = [item[1] for item in classifiers]

    r = len(classifiers)

    train_error_mean = np.zeros(r)
    test_error_mean = np.zeros(r)
    test_error_std = np.zeros(r)
    
    train_errors = np.zeros((r, epoch)) 
    test_errors = np.zeros((r, epoch))

    data = np.concatenate((data1, data2), 0)
    n1 = len(data1) 
    n2 = len(data2) 
    n = len(data) 
    data_traj1 = np.array([data1[i][:,:2] for i in range(len(data1))])
    data_traj2 = np.array([data2[i][:,:2] for i in range(len(data2))])
    data_traj = np.array([data[i][:,:2] for i in range(n)])
    labels = np.array([1] * len(data1) + [-1] * len(data2))
    #length1 = np.array([length(data_traj1[i]) for i in range(n1)])
    #length2 = np.array([length(data_traj2[i]) for i in range(n2)])

    for s in range(epoch):
        train_idx_1, test_idx_1, train_label_1, test_label_1, \
            train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test(data1, data2, test_size)
        
        train1 = data1[train_idx_1]
        train2 = data2[train_idx_2]
        train_traj1 = data_traj1[train_idx_1]
        train_traj2 = data_traj2[train_idx_2]
        train_labels1 = np.ones(len(train1))
        train_labels2 = np.zeros(len(train2))

        test1 = data1[test_idx_1]
        test2 = data2[test_idx_2]
        test_traj1 = data_traj1[test_idx_1]
        test_traj2 = data_traj2[test_idx_2]
        test_labels1 = np.ones(len(test1))
        test_labels2 = np.zeros(len(test2))

        train = np.concatenate((train1, train2), 0)
        test = np.concatenate((test1, test2), 0)
        train_traj = np.concatenate((train_traj1, train_traj2), 0)
        test_traj = np.concatenate((test_traj1, test_traj2), 0)
        train_labels = np.concatenate((train_labels1, train_labels2), 0)
        test_labels = np.concatenate((test_labels1, test_labels2), 0)

        x_preds = np.zeros((r, len(train)))
        y_preds = np.zeros((r, len(test)))

        Q_list = []
        temp_errors = []
        mu_list = []

        for j in range(init_iter):
            B = initialize_Q(train1, train2, C=C, gamma=gamma, Q_size=Q_size, 
                             mu_coeff=mu_coeff, model=model, Leng=Leng, spd=spd, 
                             accn=accn, jrk=jrk, n_estimators=n_estimators)
            Q_list.append(B[0])
            temp_errors.append(B[-1])
            mu_list.append(B[1])

        h = np.argmin(temp_errors)
        Q = Q_list[h]
        mu = mu_list[h]
        print("mu =", mu)
        if sum([Leng, spd, accn, jrk]) > 0:
            A = train_labels.copy().reshape(-1,1)
            B = test_labels.copy().reshape(-1,1)
            if Leng == True:
                length_train = np.array([length(train_traj[i]) for i in range(len(train))]).reshape(-1,1)
                A = np.concatenate((A, length_train),1)
                length_test = np.array([length(test_traj[i]) for i in range(len(test))]).reshape(-1,1)
                B = np.concatenate((B, length_test),1)
            if spd == True:
                speed_train = np.array([speed(train[i]) for i in range(len(train))]).reshape(-1,1)
                A = np.concatenate((A, speed_train),1)
                speed_test = np.array([speed(test[i]) for i in range(len(test))]).reshape(-1,1)
                B = np.concatenate((B, speed_test),1)
            if accn == True:
                acceleration_train = np.array([acceleration(train[i]) for i in range(len(train))]).reshape(-1,1)
                A = np.concatenate((A, acceleration_train),1)
                acceleration_test = np.array([acceleration(test[i]) for i in range(len(test))]).reshape(-1,1)
                B = np.concatenate((B, acceleration_test),1)
            if jrk == True:
                jerk_train = np.array([jerk(train[i]) for i in range(len(train))]).reshape(-1,1)
                A = np.concatenate((A, jerk_train),1)
                jerk_test = np.array([jerk(test[i]) for i in range(len(test))]).reshape(-1,1)
                B = np.concatenate((B, jerk_test),1)
            if normal == True:
                A[:,1:] = (A[:,1:] - np.mean(A[:,1:] , 0))/(np.std(A[:,1:],0) + 1e-10)
                B[:,1:] = (B[:,1:] - np.mean(B[:,1:] , 0))/(np.std(B[:,1:],0) + 1e-10)
            train_data = np.concatenate((A[:,1:], np.array(ExpCurve2Vec(Q, train_traj, mu))), 1)
            test_data = np.concatenate((B[:,1:], np.array(ExpCurve2Vec(Q, test_traj, mu))), 1)
            #train_data = (train_data - np.mean(train_data))/(np.std(train_data) + 1e-10)
            #test_data = (test_data - np.mean(test_data))/(np.std(test_data) + 1e-10)
        else:
            train_data = np.array(ExpCurve2Vec(Q, train_traj, mu))
            test_data = test_length, np.array(ExpCurve2Vec(Q, test_traj, mu))

        for k in range(r): 
            Model = models[k]
            Model.fit(train_data, train_labels)
            x_preds[k] = Model.predict(train_data)                
            y_preds[k] = Model.predict(test_data)

        for k in range(r):
            train_errors[k][s] = 1 - metrics.accuracy_score(train_labels, x_preds[k])
            test_errors[k][s] = 1 - metrics.accuracy_score(test_labels, y_preds[k])

    for k in range(r):
        train_error_mean[k] = np.mean(train_errors[k])
        test_error_mean[k] = np.mean(test_errors[k])
        test_error_std[k] = np.std(test_errors[k])

    Dict = {}

    for k in range(len(keys)): 
        Dict[k+1] = [keys[k], np.round(train_error_mean[k], decimals = 4), 
                     np.round(test_error_mean[k], decimals = 4),
                     np.round(test_error_std[k], decimals = 4)]

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                columns=['Classifier','Train Error', 'Test Error', 'Std Error'])
    
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf, train_error_mean, test_error_mean

C = 100

clf_L = [make_pipeline(LinearSVC(dual=False, C=C, tol=1e-5, 
                               class_weight ='balanced', max_iter=1000)), 
        "SVM, LinearSVC, C="+str(C)]

L = classification_init_Q(cars_time, buses_time, C=C, gamma=1, Q_size=20, 
                          mu_coeff=1, model='LSVM', epoch=50, init_iter=3, 
                          classifiers=[clf_L], Leng=True, spd=True, accn=True, 
                          jrk=True, test_size=0.3, normal=False)
L[0]

C = 100
g = 'auto'
clf_GSVM = [make_pipeline(StandardScaler(), svm.SVC(C=C, kernel='rbf', gamma=g, 
                                                    max_iter=200000)),
                                    "GSVM, C="+str(C)+", gamma="+str(g)]

G = classification_init_Q(cars_time, buses_time, C=C, gamma=g, Q_size=20, 
                          mu_coeff=1, model='GSVM', epoch=50, init_iter=3, 
                          classifiers=[clf_GSVM], Leng=True, spd=True, 
                          accn=True, jrk=True, test_size=0.3, normal=False)
G[0]

C = 1000
clf_PSVM = [make_pipeline(StandardScaler(), svm.SVC(C=C, kernel='poly', degree=5, 
                                                    max_iter=400000)),
            "Poly kernel SVM, C="+str(C)+", deg=auto"]

P = classification_init_Q(cars_time, buses_time, C=C, gamma=g, Q_size=20, 
                          mu_coeff=1, model='PSVM', epoch=50, init_iter=3, 
                          classifiers=[clf_PSVM], Leng=True, spd=True, 
                          accn=True, jrk=True, test_size=0.3, normal=False)
P[0]

clf_DT = [DecisionTreeClassifier(), "Decision Tree"]

DT = classification_init_Q(cars_time, buses_time, C=1, gamma=1, Q_size=20, 
                          mu_coeff=1, model='DT', epoch=50, init_iter=3, 
                          classifiers=[clf_DT], Leng=True, spd=True, accn=True, 
                          jrk=True, test_size=0.3, normal=False)
DT[0]

n_estimators = 50

clf_RF = [RandomForestClassifier(n_estimators=n_estimators), 
         "RandomForestClassifier, n="+str(n_estimators)]

RF = classification_init_Q(cars_time, buses_time, C=1, gamma=1, Q_size=20, 
                          mu_coeff=1, model='RF', epoch=50, init_iter=3, 
                          classifiers=[clf_RF], Leng=True, spd=True, accn=True, 
                          jrk=True, test_size=0.3, normal=False, 
                           n_estimators=n_estimators)
RF[0]

n_neighbors = 5

clf_KNN = [KNeighborsClassifier(n_neighbors=n_neighbors), "KNN"]

K = classification_init_Q(cars_time, buses_time, C=1, gamma=1, Q_size=20, 
                          mu_coeff=1, model='KNN', epoch=50, init_iter=3, 
                          classifiers=[clf_KNN], Leng=True, spd=True, accn=True, 
                          jrk=True, test_size=0.3, normal=False)
K[0]

clf_LR = [LogisticRegression(solver='newton-cg'), "Logistic Regression"]

LR = classification_init_Q(cars_time, buses_time, C=100, gamma=1, Q_size=20, 
                          mu_coeff=1, model='LR', epoch=50, init_iter=3, 
                          classifiers=[clf_LR], Leng=True, spd=True, 
                          accn=True, jrk=True, test_size=0.3, normal=False)
LR[0]

"""# Convolutional Neural Network For Random Choice of Q

## Helper functions
"""

from collections import Counter

def find_majority(votes):
    vote_count = Counter(votes)
    top = vote_count.most_common(1)
    return top[0][0]

def find_majority_array(A): # column-wise majority
    return list(map(find_majority, A.T))

def ExpCurve2Vec(points, curves, mu):
    D = tt.distsbase.DistsBase()
    a = np.array([np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves])
    return a

def get_endpoints(data):
    n = len(data)
    data_endpoints = np.zeros((n, 4))
    for i in range(n):
        data_endpoints[i] = np.concatenate((data[i][0], data[i][-1]), 0)
    return data_endpoints

def train_test(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size = test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size = test_size)

    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
                train_idx_2, test_idx_2, train_label_2, test_label_2

def train_test_mu(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size=test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size=test_size)

    train_1 = data_1[train_idx_1]
    train_2 = data_2[train_idx_2]
    test_1 = data_1[test_idx_1]
    test_2 = data_2[test_idx_2]

    arr1 = np.arange(len(train_1)+len(train_2))
    I_1 = np.random.shuffle(arr1)

    arr2 = np.arange(len(test_1)+len(test_2))
    I_2 = np.random.shuffle(arr2)
    
    train = np.concatenate((train_1, train_2), 0)[arr1[I_1]]
    train_labels = np.concatenate((train_label_1, train_label_2), 0)[arr1[I_1]]
    test = np.concatenate((test_1, test_2), 0)[arr2[I_2]]
    test_labels = np.concatenate((test_label_1, test_label_2), 0)[arr2[I_2]]

    a = np.mean([np.mean(train_1[i], 0) for i in range(len(train_1))], 0)
    b = np.mean([np.mean(train_2[i], 0) for i in range(len(train_2))], 0)
    mu = max(abs(a-b))
    
    return mu, train, test, train_labels, test_labels

def flatten(x):
    N = x.shape[0] 
    return x.view(N, -1)

class Flatten(nn.Module):
    def forward(self, x):
        return flatten(x)

"""## CNN with Only Physical features"""

def CNN_classification_physical_features(data_1, data_2, lr_decay = 0.9, 
                                learning_rate = 1e-3, out_channels = 10, kernel_size = 10,
                                padding = 1, bias = True, Num_updates = 1000, D_out = 2, 
                                epoch = 50, sigma = 1, Leng=True, spd=True, accn=True, 
                                jrk=True, test_size=0.3):
    """ 
    in_channels: the dimension of hidden layer
    D_out: output dimension
    version: 'signed' or 'unsigned' or 'exp' 
    stride: should be fixed to 1
    """
        
    start_time = time.time()
    
    train_errors = np.ones(epoch) 
    test_errors = np.ones(epoch)
    losses = torch.zeros(epoch, Num_updates)

    data = get_features(data_1, data_2, Leng=Leng, spd=spd, accn=accn, jrk=jrk)
    data1 = data[:len(data_1)]
    data2 = data[len(data_1):]

    for s in range(epoch):
        train_idx_1, test_idx_1, train_labels_1, test_labels_1, \
            train_idx_2, test_idx_2, train_labels_2, test_labels_2 \
                = train_test(data1, data2, test_size=test_size)

        train1 = data1[train_idx_1]
        train2 = data2[train_idx_2]
        test1 = data1[test_idx_1]
        test2 = data2[test_idx_2]

        I = np.arange(len(train1) + len(train2))
        np.random.shuffle(I)
        J = np.arange(len(test1) + len(test2))
        np.random.shuffle(J)

        train = np.concatenate((train1, train2), 0)[I]
        test = np.concatenate((test1, test2), 0)[J]
        train_labels = np.concatenate((train_labels_1, train_labels_2), 0)[I]
        test_labels = np.concatenate((test_labels_1, test_labels_2), 0)[J]
        
        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()

        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                Flatten(),
                                nn.Linear(out_channels * (len(train[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )
    
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

        train = torch.from_numpy(train).float()
        test = torch.from_numpy(test).float()

        train = train.view(len(train), 1, len(train[0]))
        
        for k in range(Num_updates):
            x_pred = model(train) # of shape (N, D_out)
            loss = loss_fn(x_pred, train_labels)
            losses[s, k] = loss

            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

            optimizer.zero_grad()

            loss.backward() # Backward pass

            optimizer.step()  # Calling the step function on the Optimizer 
        
        x_preds = torch.argmax(model(train), axis=1)
        test = test.view(len(test), 1, len(test[0]))        
        y_preds = torch.argmax(model(test), axis=1)
        
        train_errors[s] = sum(abs(train_labels - x_preds)).detach().numpy()/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds)).detach().numpy()/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN": [np.round(train_error_mean, decimals=4), 
                    np.round(test_error_mean, decimals=4),
                    np.round(test_error_std, decimals=4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index',
                                 columns = ['Train Error', 'Test Error', 'Std Error'])
    
    plt.plot(losses[-1].detach().numpy())
    plt.show()
        
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf

CNN_classification_physical_features(cars_time, buses_time, lr_decay = 0.9, 
                                learning_rate = 1e-3, out_channels = 10, kernel_size = 2,
                                padding = 1, bias = True, Num_updates = 1000, D_out = 2, 
                                epoch = 50, sigma = 1, Leng=True, spd=True, accn=True, 
                                jrk=True, test_size=0.3)

"""## CNN with $v_Q^+$, $v_Q^{\varsigma+}$, $v_Q^{exp+}$ which are random Q and physical features"""

def get_features_Q(train_1, train_2, version='unsigned', sigma=1, Q_size=20, 
                   Leng=True, spd=True, accn=True, jrk=True):

    train = np.concatenate((train_1, train_2), 0)
    n = len(train)
    train_traj_1 = np.array([train_1[i][:,:2] for i in range(len(train_1))])
    train_traj_2 = np.array([train_2[i][:,:2] for i in range(len(train_2))])
    train_traj = np.array([train[i][:,:2] for i in range(n)])

    Mean = np.mean([np.mean(train_traj[i], 0) for i in range(n)], 0)
    Std = np.std([np.std(train_traj[i], 0) for i in range(n)], 0)
    Q = np.ones((Q_size,2))
    Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], Q_size)
    Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], Q_size)

    if version == 'exp':
        mu = get_mu(train_traj_1, train_traj_2)
        A = ExpCurve2Vec(Q, train_traj, mu)
    else:
        A = np.array(curve2vec(Q, train_traj, version=version, sigma=sigma))
    
    if Leng == True:
        length_ = np.array([length(train_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A, Q

def get_test_data_Q(test_1, test_2, Q, version='unsigned', sigma=1, Leng=True, 
                    spd=True, accn=True, jrk=True):

    test = np.concatenate((test_1, test_2), 0)
    n = len(test)
    test_traj_1 = np.array([test_1[i][:,:2] for i in range(len(test_1))])
    test_traj_2 = np.array([test_2[i][:,:2] for i in range(len(test_2))])
    test_traj = np.array([test[i][:,:2] for i in range(n)])
    
    if version == 'exp':
        mu = get_mu(test_traj_1, test_traj_2)
        A = ExpCurve2Vec(Q, test_traj, mu)
    else:
        A = np.array(curve2vec(Q, test_traj, version=version, sigma=sigma))
    
    if Leng == True:
        length_ = np.array([length(test_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(test[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A

def CNN_classification_Q_physics(data1, data2, version='unsigned', sigma=1, Q_size=20, 
                             Leng=True, spd=True, accn=True, jrk=True, 
                             lr_decay = 0.9, learning_rate = 1e-3, out_channels = 10, 
                             kernel_size = 5, padding = 1, bias = True, 
                             Num_updates = 1000, D_out = 2, epoch = 50, test_size=0.3):

    start_time = time.time()
    train_errors = np.zeros(epoch)
    test_errors = np.zeros(epoch)
    losses = torch.zeros(epoch, Num_updates)

    for s in range(epoch):

        train_idx_1, test_idx_1, train_labels_1, test_labels_1, \
            train_idx_2, test_idx_2, train_labels_2, test_labels_2 \
            = train_test(data1, data2, test_size)

        train1 = data1[train_idx_1]
        train2 = data2[train_idx_2]
        test1 = data1[test_idx_1]
        test2 = data2[test_idx_2]

        train, Q = get_features_Q(train1, train2, version, sigma,
                                  Q_size, Leng, spd, accn, jrk)
        
        train_labels = np.concatenate((train_labels_1, train_labels_2), 0)

        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        train_labels = train_labels[I]
        train_labels = torch.from_numpy(train_labels).long()

        J = np.arange(len(test1)+len(test2))
        np.random.shuffle(J)

        test = get_test_data_Q(test1, test2, Q, version=version, sigma=sigma, 
                               Leng=Leng, spd=spd, accn=accn, jrk=jrk)[J]
        test_labels = np.concatenate((test_labels_1, test_labels_2), 0)[J]
        test_labels = torch.from_numpy(test_labels).long()

        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                Flatten(),
                                nn.Linear(out_channels * (len(train[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )
    
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

        train = torch.from_numpy(train).float()
        test = torch.from_numpy(test).float()

        train = train.view(len(train), 1, len(train[0]))
        
        for k in range(Num_updates):
            x_pred = model(train) # of shape (N, D_out)
            #print(x_pred, train_labels)
            #print(x_pred.shape, train_labels.shape)
            loss = loss_fn(x_pred, train_labels)
            losses[s, k] = loss

            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

            optimizer.zero_grad()

            loss.backward() # Backward pass

            optimizer.step()  # Calling the step function on the Optimizer 

        x_preds = torch.argmax(model(train), axis=1)
        test = test.view(len(test), 1, len(test[0]))        
        y_preds = torch.argmax(model(test), axis=1)
        
        train_errors[s] = sum(abs(train_labels - x_preds)).detach().numpy()/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds)).detach().numpy()/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN": [np.round(train_error_mean, decimals=4), 
                    np.round(test_error_mean, decimals=4),
                    np.round(test_error_std, decimals=4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index',
                                 columns = ['Train Error', 'Test Error', 'Std Error'])
    
    plt.plot(losses[-1].detach().numpy())
    plt.show()
        
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf

"""### $v_Q^+$"""

CNN_classification_Q_physics(cars_time, buses_time, version='unsigned', sigma=1, 
                             Q_size=20, Leng=True, spd=True, accn=True, jrk=True, 
                             lr_decay = 0.9, learning_rate = 1e-3, out_channels = 10, 
                             kernel_size = 5, padding = 1, bias = True, 
                             Num_updates = 1000, D_out = 2, epoch = 50, test_size=0.3)

"""### $v_Q^{\varsigma+}$"""

CNN_classification_Q_physics(cars_time, buses_time, version='signed', sigma=1, 
                             Q_size=20, Leng=True, spd=True, accn=True, jrk=True, 
                             lr_decay = 0.9, learning_rate = 1e-3, out_channels = 10, 
                             kernel_size = 5, padding = 1, bias = True, 
                             Num_updates = 1000, D_out = 2, epoch = 50, test_size=0.3)

"""## Using Perceptron-like algorithm with physical features"""

def initialize_Q_(train_1, train_2, Q_size, mu_coeff, out_channels, kernel_size, padding, 
             learning_rate = 1e-3, bias = True, D_out=2, lr_decay = 0.9, Leng=True, 
             spd=True, accn=True, jrk=True, Num_updates = 100):
    
    start_time = time.time()
    train = np.concatenate((train_1, train_2), 0)
    n = len(train)
    train_traj_1 = np.array([train_1[i][:,:2] for i in range(len(train_1))])
    train_traj_2 = np.array([train_2[i][:,:2] for i in range(len(train_2))])
    train_traj = np.array([train[i][:,:2] for i in range(n)])
    train_labels = np.array([1] * len(train_1) + [-1] * len(train_2))
    
    A = train_labels.copy().reshape(-1,1)
    if Leng == True:
        length_ = np.array([length(train_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(train[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)
    
    train_labels = torch.from_numpy(train_labels).long()

# Make the classifier
    
    mu = get_mu(train_traj_1, train_traj_2) * mu_coeff
    std = mu/2
    errors = []
    Q = []

    if sum([Leng, spd, accn, jrk]) > 0:
        Train_0 = A[:,1:]
        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                Flatten(),
                                nn.Linear(out_channels * (len(Train_0[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )
        
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

        Train_0 = torch.from_numpy(Train_0).float() # shape = (120, 4)
        Train_0 = Train_0.view(len(Train_0), 1, len(Train_0[0]))
        
        for k in range(Num_updates):
            x_pred = model(Train_0) # of shape (N, D_out)
            loss = loss_fn(x_pred, train_labels)
            

            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

            optimizer.zero_grad()
            loss.backward() # Backward pass
            optimizer.step()  # Calling the step function on the Optimizer 
        index = torch.argmax(torch.max(model(train), axis=1))

    else:
        index = np.random.randint(0, high=n) 
    k = np.random.randint(0, high=len(train_traj[index]))
    q = train_traj[index][k] + np.random.normal(0, std, 2)
    Q.append(q)

# Iteratively choose landmarks
    for i in range(1, Q_size):
        if sum([Leng, spd, accn, jrk]) > 0:
            Train_0 = A[:,1:]
            Train = np.concatenate((Train_0, np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))), 1)
        else:
            Train = np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))

        Model.fit(Train, train_labels)
        train_pred = Model.predict(Train)
        error = 1 - metrics.accuracy_score(train_labels, train_pred)
        errors.append(error)
        if model in ['LSVM', 'GSVM', 'PSVM', 'LR', 'Prn']:
            index = np.argmax(np.max(Model.decision_function(Train)))
        else:
            probs = Model.predict_proba(Train)
            index = np.argmax(entropy(probs, axis=1))

        k = np.random.randint(0, high=len(train_traj[index]))
        q = train_traj[index][k] + np.random.normal(0, std, 2)
        Q.append(q)
    if sum([Leng, spd, accn, jrk]) > 0:
        Train = np.concatenate((Train_0, np.array(ExpCurve2Vec(np.array(Q), train_traj, mu))), 1)
    else: 
        Train = ExpCurve2Vec(np.array(Q), train_traj, mu)
    Model.fit(Train, train_labels)
    train_pred = Model.predict(Train)

    error = 1 - metrics.accuracy_score(train_labels, train_pred)
    errors.append(error)

    print(colored(f"Total time for mapping row data: {time.time() - start_time}", 'green'))

    return np.array(Q), mu, np.array(errors), error

def train_test(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size = test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size = test_size)

    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
                train_idx_2, test_idx_2, train_label_2, test_label_2

def get_features_(data_time, Leng=True, spd=True, accn=True, jrk=True):
    
    n = len(data_time)
    data_traj = np.array([data_time[i][:,:2] for i in range(len(data_time))])
    data_labels = np.ones(n)
    A = data_labels.copy().reshape(-1,1)

    if Leng == True:
        length_ = np.array([length(data_traj[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, length_),1)
    if spd == True:
        speed_ = np.array([speed(data_time[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, speed_),1)
    if accn == True:
        acceleration_ = np.array([acceleration(data_time[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, acceleration_),1)
    if jrk == True:
        jerk_ = np.array([jerk(data_time[i]) for i in range(n)]).reshape(-1,1)
        A = np.concatenate((A, jerk_),1)

    return A[:, 1:]

def MD_Pys_NeuralNetworkClassificationCNN(data_1_time, data_2_time, 
                                          maj_num, epoch, init_iter, std_coeff, 
                                          out_channels, kernel_size, Q_size, padding, 
                                          learning_rate=1e-3, bias=True, D_out=2, 
                                          lr_decay=0.9, test_size=0.3, Num_updates=100, 
                                          Leng=True, spd=True, accn=True, jrk=True):
        
    start_time = time.time()

    train_errors = np.zeros(epoch) 
    test_errors = np.zeros(epoch)
    
    data_1 = np.array([data_1_time[i][:,:2] for i in range(len(data_1_time))])
    data_2 = np.array([data_2_time[i][:,:2] for i in range(len(data_2_time))])

    n_1 = len(data_1)
    n_2 = len(data_2) 
    
    data_1_phys = get_features_(data_1_time, Leng, spd, accn, jrk)
    data_2_phys = get_features_(data_2_time, Leng, spd, accn, jrk)
    
    for s in range(epoch):
        train_idx_1, test_idx_1, train_label_1, test_label_1, train_idx_2, \
        test_idx_2, train_label_2, test_label_2 = train_test(data_1, data_2, test_size)

        train = np.concatenate((data_1[train_idx_1], data_2[train_idx_2]), 0)
        test = np.concatenate((data_1[test_idx_1], data_2[test_idx_2]), 0)
        train_labels = np.concatenate((train_label_1, train_label_2), axis = 0)
        test_labels = np.concatenate((test_label_1, test_label_2), axis = 0)
        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()
        
        data_phys_train = np.concatenate((data_1_phys[train_idx_1], data_2_phys[train_idx_2]), 0)
        data_phys_test = np.concatenate((data_1_phys[test_idx_1], data_2_phys[test_idx_2]), 0)
        
        x_preds = np.zeros((maj_num, len(train)))
        y_preds = np.zeros((maj_num, len(test)))
        
        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        data_phys_train = data_phys_train[I]
        train_labels = train_labels[I]
        
        J = np.arange(len(test))
        np.random.shuffle(J)
        test = test[J]
        data_phys_test = data_phys_test[J]
        test_labels = test_labels[J]

        for t in range(maj_num):

            Q_list = []
            temp_errors = []
            mu_temp = []

            for j in range(init_iter):
                B = initialize_Q(data_1[train_idx_1], data_2[train_idx_2], 
                                 std_coeff, out_channels, kernel_size, Q_size, 
                                 padding, learning_rate = 1e-3, bias = True, 
                                 D_out=2, lr_decay = 0.9, Num_updates = 100)

                Q_list.append(B[0])
                mu_temp.append(B[2])
                temp_errors.append(B[-1])

            h = np.argmin(temp_errors)
            Q = Q_list[h]
            mu = mu_temp[h]

            train_data = np.concatenate((ExpCurve2Vec(Q, train, mu), data_phys_train), 1)
            train_data = torch.from_numpy(train_data).float()
            train_data = train_data.view(len(train_data), 1, len(train_data[0]))

            test_data = np.concatenate((ExpCurve2Vec(Q, test, mu), data_phys_test), 1)
            test_data = torch.from_numpy(test_data).float()
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))

            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                    nn.ReLU(),
                    Flatten(),
                    nn.Linear(out_channels * (len(train_data[0][0]) - kernel_size + 1 + 2 * padding), 
                              D_out)
                    )
        
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)

                if (k+1) % 50 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
                optimizer.zero_grad()
                loss.backward() 
                optimizer.step() 

            scores = model(train_data)
            
            x_preds[t] = torch.argmax(scores, axis=1).detach().numpy()
            y_preds[t] = torch.argmax(model(test_data), axis=1).detach().numpy()

        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))

        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
                            np.round(train_error_mean, decimals = 4), 
                            np.round(test_error_mean, decimals = 4),
                            np.round(test_error_std, decimals = 4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                                 columns=['Channel 1', 'Learning Rate', 
                                          'Train Error', 'Test Error', 'Std Error'])

    print(colored(f"total time = {time.time() - start_time}", "red"))
    print("mu =", mu)

    return pdf

MD_Pys_NeuralNetworkClassificationCNN(cars_time, buses_time, 
                                      maj_num=1, epoch=50, init_iter=3, 
                                      test_size=0.3, std_coeff=1, out_channels=10, 
                                      kernel_size=5, Q_size=10, padding=1, 
                                      learning_rate = 1e-2, bias = True, D_out=2, 
                                      lr_decay = 0.9, Num_updates = 1000)

"""# Plot of Physical feature mapping test errors"""

A_LSVM = [0.0314, 0.0195, 0.0259, 0.0389]
A_GSVM = [0.0692, 0.0638, 0.0935, 0.0816]
A_PSVM = [0.3341, 0.1141, 0.1114, 0.0649]
A_DT = [0.0784, 0.0730, 0.0670, 0.0584]
A_RF = [0.0795, 0.0530, 0.0897, 0.0568]
A_KNN = [0.0773, 0.0870, 0.0832, 0.0870]
A_LR = [0.0465, 0.0384, 0.0432, 0.0519]
A_CNN = [0.0497, 0.0432, 0.0395, 0.0503]

A = np.array([A_LSVM, A_GSVM, A_PSVM, A_DT, A_RF, A_KNN, A_LR, A_CNN])
B = A.T

std_LSVM = [0.0317, 0.0259, 0.0247, 0.0347]
std_GSVM = [0.0496, 0.0381, 0.0481, 0.0420]
std_PSVM = [0.0737, 0.0740, 0.0852, 0.0320]
std_DT = [0.0440, 0.0437, 0.0372, 0.0372]
std_RF = [0.0440, 0.0393, 0.0447, 0.0330]
std_KNN = [0.0412, 0.0460, 0.0445, 0.0409]
std_LR = [0.0367, 0.0383, 0.0394, 0.0370]
std_CNN = [0.0395, 0.0354, 0.0302, 0.0397]

C = np.array([std_LSVM, std_GSVM, std_PSVM, std_DT, std_RF, std_KNN, std_LR, std_CNN]).T

FM = (r'Rand $v_Q^+$', r'Rand $v_Q^{\varsigma +}$', r'MD^+$', 'Physical Features')
labels = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

ind = np.arange(len(A))
width = 0.2
plt.subplots(figsize = (6, 4), tight_layout=True)
bars = [0] * 4

for i in range(len(FM)):
    lower_limit = lower_limit_error(C[i], B[i])
    bars[i] = plt.bar(ind+width*i, B[i], width, yerr=[lower_limit, C[i]], 
                       ecolor='black', capsize=2)

plt.title('Car-Bus with physical features')
plt.xticks(ind+1.5 * width, labels)
plt.legend(tuple(bars), FM, loc=0)

plt.gca().yaxis.grid(color='gray', linestyle='dotted', linewidth=0.6)

path = '/content/gdrive/My Drive/plots/plots car-bus-physical/Physical car-bus bar chart.csv'

plt.savefig(path, bbox_inches='tight', dpi=200)

plt.show()

