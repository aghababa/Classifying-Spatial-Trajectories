# -*- coding: utf-8 -*-
"""Geolife_Experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ufgd7JoYtejmSgtYjqL0-UJd8hBicUMH

# Libraries
"""

import glob
import numpy as np 
import time
import math
import random
from scipy import linalg as LA
import pandas as pd
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.svm import NuSVC
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import Perceptron
import statsmodels.api as sm
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
from termcolor import colored
import matplotlib as mpl
import matplotlib.pyplot as plt
from scipy import optimize
from sklearn.svm import LinearSVC
from datetime import datetime
from scipy.stats import entropy
import os
import pickle
import trjtrypy as tt
from trjtrypy.distances import d_Q
from trjtrypy.distances import d_Q_pi
import trjtrypy.visualizations as vs
from scipy.spatial import distance
from trjtrypy.featureMappings import curve2vec

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import sampler
import torchvision.transforms as T

"""# Read Data"""

I = glob.glob('Geolife Trajectories 1.3/**/*.plt', recursive=True)
S = list(set([I[i][:45] for i in range(len(I))]))
user_idx = np.array([S[i][30:33] for i in range(len(S))])
print(user_idx[:10])
len(S), S[0]

def read_file(path):
    J = glob.glob(path, recursive=True)
    data = [0] * len(J)
    c = 0
    for j in range(len(J)):
        data[j] = []
        with open(J[j], "r") as f:
            for line in f:
                if c > 6:
                    item = line.strip().split(",")
                    if len(item) == 7:
                        data[j].append(np.array([float(item[0]), float(item[1]), 
                                                 float(item[4])]))
                c += 1
        data[j] = np.array(data[j])
    return np.array(data)

"""### data1[i] in the following is user_i from Beijing dataset 

trajectories have time dimension in data1
"""

start_time = time.time()
user_idx_int = np.array(list(map(int, user_idx)))
data1 = [0] * len(S)
for i in range(len(S)):
    idx = np.where(user_idx_int == i)[0][0]
    path = S[idx]+'*.plt'
    J = glob.glob(path, recursive=True)
    data1[i] = read_file(path)
data1 = np.array(data1)
print(time.time() - start_time)

data_1 = data1 + 0
len(data_1)

def remove_segments(traj): # removes stationary points
    p2 = traj[:,:2][1:]
    p1 = traj[:,:2][:-1]
    L = ((p2-p1)*(p2-p1)).sum(axis =1)
    I = np.where(L>1e-16)[0]
    return traj[I]

"""### In data_1 below:
    (1) there is no stationary points
    (2) there is no trajectories with more than 200 waypoints
"""

for i in range(len(data_1)):
    data_1[i] = np.array(list(map(remove_segments, data_1[i])), dtype='object')
    L = np.array([len(data_1[i][j]) for j in range(len(data_1[i]))])
    I = np.where((L > 1000))[0]
    data_1[i] = data_1[i][I]

I = np.where(np.array([len(data_1[i]) for i in range(len(data_1))]) > 0)[0]
data_1 = data_1[I]
print("len(data_1) =", len(data_1))
print("selected users: \n", I)

"""# Partitioning trajectories to less than 20 minutes long"""

def partition(trajectory, threshold=20):
    '''threshold is in minutes'''
    trajectories = []
    a = 24 * 60 * sum(trajectory[:,2][1:] - trajectory[:,2][:-1])
    if a <= threshold:
        return np.array(trajectory.reshape(1, len(trajectory), 3))
    else: 
        i = 0
        while a > threshold:
            j = i + 0
            val = 0
            while val < threshold: 
                if i < len(trajectory) - 1:
                    temp = val + 0
                    val += 24 * 60 * (trajectory[:,2][1:][i] - trajectory[:,2][:-1][i])
                    i += 1
                else: 
                    break
            if len(trajectory[j:i-1]) > 0:
                trajectories.append(trajectory[j:i-1])
            a = a - val
        if len(trajectory[i:]) > 0:
            trajectories.append(trajectory[i:])
    return np.array(trajectories)

# 24 * 60 * (days_date('1899/12/30 2:50:06') - days_date('1899/12/30 2:20:06')) == 20 min
Time = [0] * len(data_1)
for i in range(len(data_1)):
    Time[i] = []
    for j in range(len(data_1[i])):
        Time[i].append(24 * 60 * sum(data_1[i][j][:,2][1:] - data_1[i][j][:,2][:-1])) # = 20 minutes 
    Time[i] = np.array(Time[i], dtype='object')

Time = np.array(Time, dtype='object')
Time.shape

J = [np.where(Time[i] > 20)[0] for i in range(len(Time))]
print(len(J))

"""### data3 below is the array of trajectories having less than 20 minutes long"""

data3 = [0] * len(data_1)

for i in range(len(data_1)):
    data3[i] = []
    for j in range(len(data_1[i])):
        A = partition(data_1[i][j], threshold=20)
        for k in range(len(A)):
            data3[i].append(A[k])
    data3[i] = np.array(data3[i], dtype='object')
    
data3 = np.array(data3, dtype='object')

data3.shape, data3[0].shape, data3[0][0].shape

A = [len(data_1[0][i]) for i in range(len(data_1[0]))]
print(A)

data4 = data3 + 0

"""data4 is the users having between 100 and 200 trajectories and each has length between 10 and 200 trajectory"""

for i in range(len(data4)):
    A = np.array([len(data4[i][j]) for j in range(len(data4[i]))])
    I = np.where((A > 10) & (A < 200))[0]
    data4[i] = data4[i][I]
    
print(len(data4))
A = np.array([len(data4[i]) for i in range(len(data4))])
chosen_users = np.where((A > 100) & (A < 200))[0]
data4 = data4[chosen_users]

print("chosen users:", chosen_users)
print("len(data4) =", len(data4))
A = [len(data4[i]) for i in range(len(data4))]
print("length of preprocessed users in data4: \n", np.sort(A))

"""### data2 is the same as data4 but without time dimension"""

data2 = data4 + 0
for i in range(len(data2)):
    data2[i] = np.array([data4[i][j][:,:2] for j in range(len(data4[i]))], dtype='object')
len(data2)

np.sort(list(map(len, data2)))

"""# Classifiers"""

CC = [100, 100, 10]
number_estimators = [50, 50]


clf0 = [make_pipeline(LinearSVC(dual=False, C=CC[0], tol=1e-5, 
                               class_weight ='balanced', max_iter=1000)), 
        "SVM, LinearSVC, C = "+str(CC[0])]
clf1 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[1], kernel='rbf', gamma='auto', max_iter=200000)),
        "Gaussian SVM, C="+str(CC[1])+", gamma=auto"]
clf2 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[2], kernel='poly', degree=3, max_iter=400000)),
        "Poly kernel SVM, C="+str(CC[2])+", deg=auto"]
clf3 = [DecisionTreeClassifier(), "Decision Tree"]
clf4 = [RandomForestClassifier(n_estimators=number_estimators[0]), 
         "RandomForestClassifier, n="+str(number_estimators[0])]
clf5 = [KNeighborsClassifier(n_neighbors=5), "KNN"]
clf6 = [LogisticRegression(solver='newton-cg'), "Logistic Regression"]

clf = [clf0, clf1, clf2, clf3, clf4, clf5, clf6]
classifs = [item[0] for item in clf]
keys = [item[1] for item in clf]

"""# Chosen pairs of users"""

pairs_final = [[4, 12], [4, 16], [5, 12], [8, 10]]
pairs_final

"""# Plot data"""

data = data2.copy()
pairs_from_data = [(15, 44), (15, 125), (16, 44), (33, 40)]
path = '/Users/hasan/Desktop/Anaconda/Research/Pictures for 2ed paper/'
for k in range(len(pairs_final)):
    m, n = pairs_final[k]
    r, s = pairs_from_data[k]
    for i in range(len(data[m])):
        plt.plot(data[m][i][:,0], data[m][i][:,1], color='blue')
    for i in range(len(data[n])):
        plt.plot(data[n][i][:,0], data[n][i][:,1], color='red')
    plt.title(f'Users {r}, {s} from Geolife dataset')
    plt.savefig(path+f'Beijing-pairs {pairs_from_data[k]}.png', bbox_inches='tight', 
                dpi=200)
    plt.show()

"""# Get length and width of the box containig pairs"""

for k in range(len(pairs_final)):
    m, n = pairs_final[k]
    r, s = pairs_from_data[k]
    min_x, min_y = np.min((np.min([np.min(data[m][i], axis=0) for i in range(len(data[m]))], axis=0), 
           np.min([np.min(data[n][i], axis=0) for i in range(len(data[n]))], axis=0)), axis=0)

    max_x, max_y = np.max((np.max([np.max(data[m][i], axis=0) for i in range(len(data[m]))], axis=0), 
                   np.max([np.max(data[n][i], axis=0) for i in range(len(data[n]))], axis=0)), axis=0)

    print(f"length and width of pair {r, s}:", np.round(max_x - min_x, decimals=2), 
          np.round(max_y - min_y, decimals=2))
    print("len(data[m]), len(data[n]):", len(data[m]), len(data[n]))
    print()

print('average test error for 4 pairs:', list(np.round(list(np.mean(test_errors_ave, 0)), decimals=4)))
print()
print('average std error for 4 pairs: ', list(np.round(list(np.mean(std_errors_ave, 0)), decimals=4)))
print()

#average test error for 4 pairs: [0.2888, 0.2447, 0.2567, 0.2132, 0.1731, 0.2067, 0.3022]

#average std error for 4 pairs:  [0.0448, 0.0364, 0.0397, 0.039, 0.0338, 0.0395, 0.0359]

"""# KNN classification with 10 distances:
    discret_frechet, hausdorff, dtw, sspd, erp, edr, lcss, fastdtw, dtw_tslearn, d_Q_pi
"""

from KNN_Class import KNN

import numpy as np
import time
import pandas as pd
import random
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw
import traj_dist.distance as tdist
import pickle
import tslearn
from tslearn.metrics import dtw as dtw_tslearn
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from trjtrypy.distances import d_Q_pi
from termcolor import colored
from sdtw import SoftDTW

metrics = ['discret_frechet', 'hausdorff', dtw_tslearn, SoftDTW, fastdtw, 'lcss', 'sspd',
           'edr', 'erp', 'd_Q_pi']

path = 'Calculated Distance Matrices for KNN-T-drive/'

train_test_mean_median_std_KNN_errors = []
test_KNN_errors = []
std_KNN_errors = []

for pair in pairs_final:
    print('pair =', pair)
    temp = []
    for i in range(len(metrics)):
        KNN_class = KNN(data[pair[0]], data[pair[1]], metric=metrics[i], gamma=1e-15, 
                        eps_edr=0.001, eps_lcss=0.001, Q_size=20, Q=None, p=2, 
                        path=path+str(metrics[i])+'-'+str(pair), 
                        test_size=0.3, n_neighbors=5, num_trials=50, pair=[0,1])

        KNN_class.write_matrix_to_csv()
        A = KNN_class.KNN_average_error()
        temp.append(A[1:])        
        print(i, A[0])
        print("==========================================================================")
    
    train_test_mean_median_std_KNN_errors.append(temp)
    test_KNN_errors.append(np.array(temp)[:,1])
    std_KNN_errors.append(np.array(temp)[:,3])
    print(colored("******************************************************************************", 'red'))
    print(colored("******************************************************************************", 'red'))
    print(colored("******************************************************************************", 'red'))
    
train_test_mean_median_std_KNN_errors = np.array(train_test_mean_median_std_KNN_errors)
test_KNN_errors = np.array(test_KNN_errors)
std_KNN_errors = np.array(std_KNN_errors)

list_test_error = list(np.round(np.mean(test_KNN_errors, 0), decimals=4))
list_std_error = list(np.round(np.mean(std_KNN_errors, 0), decimals=4))
print(colored(f'average test errors of 4 pairs: \n {list_test_error}', 'magenta'))
print(colored(f'average stds of 4 pairs: \n {list_std_error}', 'yellow'))

"""## KNN with LSH"""

from KNN_with_LSH_class import KNN_with_LSH

train_test_mean_median_std_KNN_LSH_errors = []
test_KNN_LSH_errors = []
std_KNN_LSH_errors = []
i = 0
for pair in pairs_final:
    st = time.time()
    print('pair =', pair)
    KNN_with_LSH_class = KNN_with_LSH(data[pair[0]], data[pair[1]], number_circles=20, 
                                      num_trials=50)
    A = KNN_with_LSH_class.KNN_LSH_average_error()
    train_test_mean_median_std_KNN_LSH_errors.append(A[1:])        
    print(i, A[0])
    i += 1
    print(colored(f"total time for pair {i}: {time.time()-st}", 'red'))
    print("=======================================================================")
    
train_test_mean_median_std_KNN_LSH_errors = np.array(train_test_mean_median_std_KNN_LSH_errors)
test_KNN_LSH_errors = train_test_mean_median_std_KNN_LSH_errors[:,1]
std_KNN_LSH_errors = train_test_mean_median_std_KNN_LSH_errors[:,3]

print(np.mean(test_KNN_LSH_errors))

test_error = np.round(np.mean(test_KNN_LSH_errors, 0), decimals=4)
std_error = np.round(np.mean(std_KNN_LSH_errors, 0), decimals=4)
print(colored(f'average test error for 4 pairs: {test_error}', 'magenta'))
print(colored(f'average stds of 4 pairs:        {std_error}', 'yellow'))

#path = 'Claculated test errors for correlation/Beijing/KNN_LSH_test_errors_Beijing.csv'
#np.savetxt(path, test_KNN_LSH_errors, delimiter=',')

#path = 'Claculated test errors for correlation/Beijing/train_test_mean_median_std_KNN_LSH_errors.csv'
#np.savetxt(path, train_test_mean_median_std_KNN_LSH_errors, delimiter=',')

"""# Classification with Perceptron-Like Algorithm

## Import Class
"""

from Perceptron_Like_Algo_Class import classification

"""## Vote(MD $v_Q^{\exp}$) with epoch=50, majority=11 and init_num=3"""

m, n = pairs_final[0]

for Model in ['LSVM', 'GSVM', 'PSVM']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=10, gamma=10, 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")
    time.sleep(600)

m, n = pairs_final[0]

for Model in ['DT', 'RF', 'KNN', 'LR']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=10, gamma=10, 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")
    time.sleep(300)

m, n = pairs_final[1]

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, gamma=10, 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")
    time.sleep(300)

m, n = pairs_final[1]

for Model in ['RF', 'KNN', 'LR']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, gamma=10, 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")
    time.sleep(300)

m, n = pairs_final[2]

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, gamma=10, 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")
    time.sleep(300)

m, n = pairs_final[2]

for Model in ['RF', 'KNN', 'LR']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, gamma=10, 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")
    time.sleep(300)

m, n = pairs_final[3]

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")
    time.sleep(300)

m, n = pairs_final[3]

for Model in ['RF', 'KNN', 'LR']:
    classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, gamma='auto',
                         classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")
    time.sleep(300)

# average errors of 5 pairs reported in the paper (epoch=100, maj=1, init_iter=3)
print('LSVM:', np.round((0.2693 +0.3326 +0.2329 +0.2611 )/4, decimals=4), 
      np.round((0.0571 +0.0598 +0.0449 +0.0369 )/4, decimals=4))
print('GSVM:', np.round((0.2422 +0.1502 +0.2218 +0.2219 )/4, decimals=4), 
      np.round((0.0331 +0.0447 +0.0355 + 0.0359)/4, decimals=4))
print('PSVM:', np.round((0.2626 +0.3212 +0.2116 +0.2117 )/4, decimals=4), 
      np.round((0.0319 +0.0511 +0.0248 +0.0348 )/4, decimals=4))
print('DT:', np.round((0.2028 +0.1433 +0.1794 +0.2087 )/4, decimals=4), 
      np.round((0.0367 +0.0317 +0.0314 +0.0359 )/4, decimals=4))
print('RF:', np.round((0.1807 +0.1279 +0.1614 +0.1909 )/4, decimals=4),
      np.round((0.0316 +0.0334 +0.036 +0.0327 )/4, decimals=4))
print('KNN:', np.round((0.2101 +0.1526 +0.1902 +0.2196 )/4, decimals=4), 
      np.round((0.0325 +0.0366 +0.0327 +0.04 )/4, decimals=4))
print('LR:', np.round((0.3144 +0.3579 +0.2967 +0.2128 )/4, decimals=4),
      np.round((0.0536 +0.0257 +0.0461 +0.0279 )/4, decimals=4))

#LSVM: 0.274, 0.0497
#GSVM: 0.209, 0.0373
#PSVM: 0.2518, 0.0356
#DT: 0.1836, 0.0339
#RF: 0.1652, 0.0334
#KNN: 0.1931, 0.0354
#LR: 0.2954, 0.0383

"""## MD $v_Q^{\exp}$ with epoch=50, majority=1 and init_num=3"""

test_errors_ave = []
std_errors_ave = []

for pair in pairs_final: 
    m, n = pair
    print(f"pair: {m, n}")
    
    test_errors_temp = []
    std_errors_temp = []

    for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
        classif = classification(data[m], data[n], Q_size=20, model=Model, C=100, 
                        gamma='auto', classifiers=[], epoch=50, maj_num=1, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

        A = classif.classification_Q()
        print(A[0])
        print(colored(f"mu = {A[1]}", 'blue'))
        test_errors_temp.append(np.array(A[0])[0][2])
        std_errors_temp.append(np.array(A[0])[0][3])
        print(colored('=======================================================================', 'yellow'))
    time.sleep(120)
    print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')
    test_errors_ave.append(test_errors_temp)
    std_errors_ave.append(std_errors_temp)
    print(colored(f'test errors:     {test_errors_temp}', 'magenta'))
    print(colored(f'std test errors: {std_errors_temp}', 'green'))

print(colored('************************************************************************', 'red'))
print(colored('************************************************************************', 'red'))
print(colored('************************************************************************', 'red'))

# average errors of 5 pairs reported in the paper (epoch=50, maj=1, init_iter=3)

test_errors_ave = np.array(test_errors_ave)
std_errors_ave = np.array(std_errors_ave)

print('average test error for 4 pairs:', list(np.round(list(np.mean(test_errors_ave, 0)), decimals=4)))
print()
print('average std error for 4 pairs: ', list(np.round(list(np.mean(std_errors_ave, 0)), decimals=4)))
print()

"""# Classification with feature mappings $v_Q$, $v_Q^{\exp}$, $v_Q^{\varsigma}$ with 20 random landmarks, and endpoints"""

from v_Q_mu_endpoints_classification import binaryClassificationAverageMajority

"""## Classification with $v_Q$ with random $Q$ in each iteration"""

# Voting v_Q
v_Q_maj_errors = []
v_Q_maj_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=11, classifiers=clf, 
                                            version='unsigned', test_size=0.3)
    A = classif.classification_v_Q()
    print(A[0])
    v_Q_maj_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_maj_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_maj_errors = \n', v_Q_maj_errors)
print('v_Q_maj_stds = \n', v_Q_maj_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_maj_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds:   {list(np.round(np.mean(v_Q_maj_stds, 0), decimals=4))}', 'green'))

# Averaging v_Q
v_Q_errors = []
v_Q_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=1, classifiers=clf, 
                                            version='unsigned', test_size=0.3)
    A = classif.classification_v_Q()
    print(A[0])
    v_Q_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_errors = \n', v_Q_errors)
print('v_Q_stds = \n', v_Q_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds:   {list(np.round(np.mean(v_Q_stds, 0), decimals=4))}', 'green'))

"""## Classification with $v_Q^{\varsigma}$ with random $Q$ in each iteration"""

# Voting v_Q^sigma

v_Q_sigma_maj_errors = []
v_Q_sigma_maj_stds = []
i = 0

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=11, classifiers=clf, 
                                            version='signed', sigma=1, test_size=0.3)
    A = classif.classification_v_Q()
    print(A[0])
    v_Q_sigma_maj_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_sigma_maj_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")
    i += 1

print('v_Q_sigma_maj_errors = \n', v_Q_sigma_maj_errors)
print('v_Q_sigma_maj_stds = \n', v_Q_sigma_maj_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_sigma_maj_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds:   {list(np.round(np.mean(v_Q_sigma_maj_stds, 0), decimals=4))}', 'green'))

# Averaging v_Q^sigma
v_Q_sigma_errors = []
v_Q_sigma_stds = []
i = 0

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green')) 
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                        epoch=50, num_trials_maj=1, classifiers=clf, 
                                        version='signed', sigma=1, test_size=0.3)
    A = classif.classification_v_Q()
    print(A[0])
    v_Q_sigma_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_sigma_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")
    i += 1

print('v_Q_sigma_errors = \n', v_Q_sigma_errors)
print('v_Q_sigma_stds = \n', v_Q_sigma_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_sigma_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds:   {list(np.round(np.mean(v_Q_sigma_stds, 0), decimals=4))}', 'green'))

"""## Classification with $v_Q^{\exp}$ with random $Q$ in each iteration"""

# Voting v_Q_mu
v_Q_mu_maj_errors = []
v_Q_mu_maj_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=11, classifiers=clf, 
                                            test_size=0.3)
    A = classif.classification_v_Q_mu()
    print(A[0])
    v_Q_mu_maj_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_mu_maj_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_mu_maj_errors = \n', v_Q_mu_maj_errors)
print('v_Q_mu_maj_stds = \n', v_Q_mu_maj_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_mu_maj_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds:   {list(np.round(np.mean(v_Q_mu_maj_stds, 0), decimals=4))}', 'green'))

# Averaging v_Q_mu
v_Q_mu_errors = []
v_Q_mu_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=1, classifiers=clf, 
                                            test_size=0.3)
    A = classif.classification_v_Q_mu()
    print(A[0])
    v_Q_mu_errors.append(np.round(A[2], decimals=4).tolist())
    v_Q_mu_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_mu_errors = \n', v_Q_mu_errors)
print('v_Q_mu_stds = \n', v_Q_mu_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(v_Q_mu_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds:   {list(np.round(np.mean(v_Q_mu_stds, 0), decimals=4))}', 'green'))

"""## Endpoint classification"""

endpoint_errors = []
endpoint_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    classif = binaryClassificationAverageMajority(data[pair[0]], data[pair[1]], Q_size=20,
                                            epoch=50, num_trials_maj=1, classifiers=clf, 
                                            version='unsigned', test_size=0.3)
    A = classif.endpoint_classification()
    print(A[0])
    endpoint_errors.append(np.round(A[2], decimals=4).tolist())
    endpoint_stds.append(np.round(A[3], decimals=4).tolist())
    print("===========================================================================")

print('endpoint_errors = \n', endpoint_errors)
print('endpoint_stds = \n', endpoint_stds)
print("===========================================================================")
print(colored(f'average errors: {list(np.round(np.mean(endpoint_errors, 0), decimals=4))}', 'blue'))
print(colored(f'average stds:   {list(np.round(np.mean(endpoint_stds, 0), decimals=4))}', 'green'))

"""# Neural Networks (CNN)"""

# Helper functions

from collections import Counter

def find_majority(votes):
    vote_count = Counter(votes)
    top = vote_count.most_common(1)
    return top[0][0]

def find_majority_array(A): # column-wise majority
    return list(map(find_majority, A.T))

def ExpCurve2Vec(points, curves, mu):
    D = tt.distsbase.DistsBase()
    a = np.array([np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves])
    return a

def get_mu(data_1, data_2):
    a = np.mean([np.mean(data_1[i], 0) for i in range(len(data_1))], 0)
    b = np.mean([np.mean(data_2[i], 0) for i in range(len(data_2))], 0)
    c = abs(a-b)
    return max(c)

def get_endpoints(data):
    n = len(data)
    data_endpoints = np.zeros((n, 4))
    for i in range(n):
        data_endpoints[i] = np.concatenate((data[i][0], data[i][-1]), 0)
    return data_endpoints

def train_test(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size = test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size = test_size)

    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
                train_idx_2, test_idx_2, train_label_2, test_label_2

def train_test_mu(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size=test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size=test_size)

    train_1 = data_1[train_idx_1]
    train_2 = data_2[train_idx_2]
    test_1 = data_1[test_idx_1]
    test_2 = data_2[test_idx_2]

    arr1 = np.arange(len(train_1)+len(train_2))
    I_1 = np.random.shuffle(arr1)

    arr2 = np.arange(len(test_1)+len(test_2))
    I_2 = np.random.shuffle(arr2)
    
    train = np.concatenate((train_1, train_2), 0)[arr1[I_1]]
    train_labels = np.concatenate((train_label_1, train_label_2), 0)[arr1[I_1]]
    test = np.concatenate((test_1, test_2), 0)[arr2[I_2]]
    test_labels = np.concatenate((test_label_1, test_label_2), 0)[arr2[I_2]]

    a = np.mean([np.mean(train_1[i], 0) for i in range(len(train_1))], 0)
    b = np.mean([np.mean(train_2[i], 0) for i in range(len(train_2))], 0)
    mu = max(abs(a-b))
    
    return mu, train[0], test[0], train_labels[0], test_labels[0]

def flatten(x):
    N = x.shape[0] 
    return x.view(N, -1)

class Flatten(nn.Module):
    def forward(self, x):
        return flatten(x)

"""## CNN for $v_Q$, $v_Q^{\varsigma}$, $v_Q^{exp}$, endpoints"""

def neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 1e-3, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 10, 
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3):
    
    """ 
    in_channels: the dimension of hidden layer
    D_out: output dimension
    version: 'signed' or 'unsigned' or 'exp' 
    stride: should be fixed to 1
    """

    start_time = time.time()

    train_errors = np.zeros(epoch)
    test_errors = np.zeros(epoch)

    losses = torch.zeros(epoch, num_trials_maj, Num_updates)
    
    for s in range(epoch):

        mu, train, test, train_labels, test_labels = train_test_mu(data_1, data_2, test_size)

        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()
        
        x_preds = torch.zeros(num_trials_maj, len(train))
        y_preds = torch.zeros(num_trials_maj, len(test))

        Min = np.min([np.min(train[i], 0) for i in range(len(train))], 0)
        Max = np.max([np.max(train[i], 0) for i in range(len(train))], 0)
        Mean = np.mean([np.mean(train[i], 0) for i in range(len(train))], 0)
        Std = np.std([np.std(train[i], 0) for i in range(len(train))], 0)
        
        for t in range(num_trials_maj):
            Q = np.ones((Q_size, 2))
            Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], Q_size)
            Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], Q_size)

            if (version == 'unsigned' or version == 'signed'):
                train_data = curve2vec(Q, train, version = version, sigma = sigma)
                test_data = curve2vec(Q, test, version = version, sigma = sigma)
            elif version == 'exp':
                train_data = ExpCurve2Vec(Q, train, mu)
                test_data = ExpCurve2Vec(Q, test, mu)
            elif version == 'endpoints':
                train_data = get_endpoints(train)
                test_data = get_endpoints(test)
            
            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                #nn.LeakyReLU(0.01),
                                #nn.Tanh(),
                                Flatten(),
                                nn.Linear(out_channels * (len(train_data[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )
    
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
            
            train_data = torch.from_numpy(train_data).float()
            test_data = torch.from_numpy(test_data).float()

            train_data = train_data.view(len(train_data), 1, len(train_data[0]))
            
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)
                losses[s, t, k] = loss
                    
                if (k+1) % 1000 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

                optimizer.zero_grad()

                loss.backward() # Backward pass

                optimizer.step()  # Calling the step function on the Optimizer 
                
            x_preds[t] = torch.argmax(model(train_data), axis=1)
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))        
            y_preds[t] = torch.argmax(model(test_data), axis=1)
        
        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))
        
        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    plt.plot((torch.mean(losses, dim=(0,1))).detach().numpy())
    plt.show()

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
               np.round(train_error_mean, decimals=4), 
                np.round(test_error_mean, decimals=4),
                np.round(test_error_std, decimals=4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                columns=['Channel 1', 'Learning Rate', 'Train Error', 
                         'Test Error', 'Std Error'])
    
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf, test_error_mean, test_error_std

"""## num_maj = 1

### Endpoints
"""

endpoint_errors = []
endpoint_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj = 1, out_channels = 10, kernel_size = 2,  
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'endpoints', 
                                       sigma = 0.01, test_size = 0.3)
    print(A[0])
    endpoint_errors.append(np.round(A[1], decimals=4).tolist())
    endpoint_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('endpoint_errors = \n', endpoint_errors)
print('endpoint_stds = \n', endpoint_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(endpoint_errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(endpoint_stds), decimals=4)}', 'green'))

"""### Rand($v_Q$)"""

errors = []
errors_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=1, out_channels= 10, kernel_size= 5,
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'unsigned', 
                                       sigma = 1, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_errors = \n', errors)
print('v_Q_errors_stds = \n', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))

"""### Rand($v_Q^{\varsigma}$)"""

errors = []
errors_stds = []

s_time = time.time()
for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=1, out_channels= 10, kernel_size= 5,
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'signed', 
                                       sigma = 1, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_varsigma_errors = ', errors)
print('v_Q_varsigma_errors_stds = ', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))

print("Total time: ", time.time() - s_time)

"""### Rand($v_Q^{exp}$)"""

errors = []
errors_stds = []

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=1, out_channels= 10, kernel_size= 5,
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'exp', 
                                       sigma = 1, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_exp_errors = \n', errors)
print('v_Q_exp_errors_stds = \n', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))

"""## num_maj = 11

### Vote($v_Q$)
"""

errors = []
errors_stds = []
s_time = time.time()

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=11, out_channels=10, kernel_size=5,
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'unsigned', 
                                       sigma = 1, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_errors = \n', errors)
print('v_Q_errors_stds = \n', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))
print("Total time: ", time.time() - s_time)

"""### Vote($v_Q^{\varsigma}$)"""

errors = []
errors_stds = []
s_time = time.time()

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 2e-2, 
                                       num_trials_maj=11, out_channels=10, kernel_size=5,
                                       padding = 1, bias = True, Num_updates = 500, 
                                       D_out = 2, epoch = 50, version = 'signed', 
                                       sigma = 1, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_varsigma_errors = \n', errors)
print('v_Q_varsigma_errors_stds = \n', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))
print("Total time: ", time.time() - s_time)

"""### Vote($v_Q^{exp}$)"""

errors = []
errors_stds = []
s_time = time.time()

for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = neuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], Q_size = 20, 
                                       lr_decay = 0.9, learning_rate = 1e-2, 
                                       num_trials_maj=11, out_channels=10, kernel_size=5,
                                       padding = 1, bias = True, Num_updates = 1000, 
                                       D_out = 2, epoch = 50, version = 'exp', 
                                       sigma = 1, test_size = 0.3)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('v_Q_exp_errors = \n', errors)
print('v_Q_exp_errors_stds = \n', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))
print("Total time: ", time.time() - s_time)

"""# Neural Network for Mistake Driven Method of Choosing $Q$

## Initialize $Q$
"""

def initialize_Q(train_1, train_2, std_coeff, out_channels, kernel_size, Q_size, 
                 padding, learning_rate = 1e-3, bias = True, D_out=2, lr_decay = 0.9, 
                 Num_updates = 100): 
        
    Q = []
    errors = []
    losses = np.zeros((Q_size - kernel_size, Num_updates))
    
    mu = get_mu(train_1, train_2)
    std = mu * std_coeff

    trajectory_train_data = np.concatenate((train_1, train_2), axis = 0)
    train_labels = np.concatenate(([1] * len(train_1), [0] * len(train_2)), 0)
    train_labels = torch.from_numpy(train_labels).long()
    index = np.random.randint(0, high=len(trajectory_train_data)) 
    k = np.random.randint(0, high=len(trajectory_train_data[index]))
    for i in range(kernel_size):
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)

    for i in range(Q_size - kernel_size):
        train_data = ExpCurve2Vec(np.array(Q), trajectory_train_data, mu)
        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                            out_channels = out_channels, 
                            kernel_size = kernel_size,
                            stride  = 1,
                            padding = padding,
                            bias = bias),
                nn.ReLU(),
                #nn.LeakyReLU(0.01),
                #nn.Tanh(),
                Flatten(),
                nn.Linear(out_channels * (len(train_data[0]) - kernel_size + 1 + 2 * padding), 
                            D_out)
                )
    
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
        train_data = torch.from_numpy(train_data).float()
        train_data = train_data.view(len(train_data), 1, len(train_data[0]))
        
        for k in range(Num_updates):
            x_pred = model(train_data) # of shape (N, D_out)
            loss = loss_fn(x_pred, train_labels)
            losses[i, k] = loss
            
            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
            optimizer.zero_grad()
            loss.backward() 
            optimizer.step() 
        
        train_pred = torch.argmax(model(train_data), axis=1)
        scores = model(train_data)
        I = np.where((train_labels == train_pred) == False)[0]

        temp_labels = 2 * train_labels.numpy().reshape(len(train_labels.numpy()), 1) - 1
        temp = temp_labels * scores.detach().numpy()
        temp = np.max(temp, axis=1)
        index = I[np.argmax(temp[I])]

        error = sum(train_labels != train_pred)/len(train_labels)
        errors.append(error.item())
        
        k = np.random.randint(0, high=len(trajectory_train_data[index]))
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)

    final_error = errors[-1]

    #plt.plot((np.mean(losses, axis=0)))
    #plt.show()

    return np.array(Q), np.array(errors), mu, final_error

def MD_NeuralNetworkClassificationCNN(data_1, data_2, maj_num, epoch, init_iter, 
                                      test_size, std_coeff, out_channels, kernel_size, 
                                      Q_size, padding, learning_rate = 1e-3, 
                                      bias = True, D_out=2, lr_decay = 0.9, 
                                      Num_updates = 100):
        
    start_time = time.time()

    train_errors = np.zeros(epoch) 
    test_errors = np.zeros(epoch)

    n_1 = len(data_1)
    n_2 = len(data_2) 

    for s in range(epoch):
        train_idx_1, test_idx_1, train_label_1, test_label_1, train_idx_2, \
        test_idx_2, train_label_2, test_label_2 = train_test(data_1, data_2, test_size)

        train = np.concatenate((data_1[train_idx_1], data_2[train_idx_2]), 0)
        test = np.concatenate((data_1[test_idx_1], data_2[test_idx_2]), 0)
        train_labels = np.concatenate((train_label_1, train_label_2), axis = 0)
        test_labels = np.concatenate((test_label_1, test_label_2), axis = 0)
        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()

        x_preds = torch.zeros((maj_num, len(train)))
        y_preds = torch.zeros((maj_num, len(test)))
        
        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        train_labels = train_labels[I]
        
        J = np.arange(len(test))
        np.random.shuffle(J)
        test = test[J]
        test_labels = test_labels[J]

        for t in range(maj_num):

            Q_list = []
            temp_errors = []
            mu_temp = []

            for j in range(init_iter):
                B = initialize_Q(data_1[train_idx_1], data_2[train_idx_2], 
                                 std_coeff, out_channels, kernel_size, Q_size, 
                                 padding, learning_rate = 1e-3, bias = True, 
                                 D_out=2, lr_decay = 0.9, Num_updates = 100)

                Q_list.append(B[0])
                mu_temp.append(B[2])
                temp_errors.append(B[-1])

            h = np.argmin(temp_errors)
            Q = Q_list[h]
            mu = mu_temp[h]

            train_data = torch.from_numpy(ExpCurve2Vec(Q, train, mu)).float()
            train_data = train_data.view(len(train_data), 1, len(train_data[0]))

            test_data = torch.from_numpy(ExpCurve2Vec(Q, test, mu)).float()
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))

            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                    nn.ReLU(),
                    #nn.LeakyReLU(0.01),
                    #nn.Tanh(),
                    Flatten(),
                    nn.Linear(out_channels * (len(train_data[0][0]) - kernel_size + 1 + 2 * padding), 
                              D_out)
                    )
        
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)

                if (k+1) % 10 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
                optimizer.zero_grad()
                loss.backward() 
                optimizer.step() 

            scores = model(train_data)
            
            x_preds[t] = torch.argmax(scores, axis=1)
            y_preds[t] = torch.argmax(model(test_data), axis=1)

        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))

        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
                            np.round(train_error_mean, decimals = 4), 
                            np.round(test_error_mean, decimals = 4),
                            np.round(test_error_std, decimals = 4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                                 columns=['Channel 1', 'Learning Rate', 
                                          'Train Error', 'Test Error', 'Std Error'])

    print(colored(f"total time = {time.time() - start_time}", "red"))
    print("mu =", mu)

    return pdf, test_error_mean, test_error_std

"""### num_maj = 1"""

errors = []
errors_stds = []

s_time = time.time()
for pair in pairs_final:
    print(colored(f"pair={pair}", 'green'))
    A = MD_NeuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], maj_num=1, 
                                          epoch=50, init_iter=3, 
                                          test_size=0.3, std_coeff=1, out_channels=10, 
                                          kernel_size = 5, Q_size = 20, padding = 1, 
                                          learning_rate = 1e-2, bias = True, D_out=2, 
                                          lr_decay = 0.9, Num_updates = 1000)
    print(A[0])
    errors.append(np.round(A[1], decimals=4).tolist())
    errors_stds.append(np.round(A[2], decimals=4).tolist())
    print("===========================================================================")

print('errors = ', errors)
print('errors_stds = ', errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(errors_stds), decimals=4)}', 'green'))

print("Total time: ", time.time() - s_time)

"""### num_maj = 11"""

MD_errors = []
MD_errors_stds = []

s_time = time.time()
pair = pairs_final[0]
print(colored(f"pair={pair}", 'green'))
A = MD_NeuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], maj_num=11, epoch=50, 
                                      init_iter=3, test_size=0.3, std_coeff=1, 
                                      out_channels=10, kernel_size = 5, Q_size = 20, 
                                      padding = 1, learning_rate = 1e-2, bias = True, 
                                      D_out=2, lr_decay = 0.9, Num_updates = 1000)
print(A[0])
MD_errors.append(np.round(A[1], decimals=4).tolist())
MD_errors_stds.append(np.round(A[2], decimals=4).tolist())
print("time: ", time.time() - s_time)

s_time = time.time()
pair = pairs_final[1]
print(colored(f"pair={pair}", 'green'))
A = MD_NeuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], maj_num=11, epoch=50, 
                                      init_iter=3, test_size=0.3, std_coeff=1, 
                                      out_channels=10, kernel_size = 5, Q_size = 20, 
                                      padding = 1, learning_rate = 1e-2, bias = True, 
                                      D_out=2, lr_decay = 0.9, Num_updates = 1000)
print(A[0])
MD_errors.append(np.round(A[1], decimals=4).tolist())
MD_errors_stds.append(np.round(A[2], decimals=4).tolist())
print("time: ", time.time() - s_time)

s_time = time.time()
pair = pairs_final[2]
print(colored(f"pair={pair}", 'green'))
A = MD_NeuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], maj_num=11, epoch=50, 
                                      init_iter=3, test_size=0.3, std_coeff=1, 
                                      out_channels=10, kernel_size = 5, Q_size = 20, 
                                      padding = 1, learning_rate = 1e-2, bias = True, 
                                      D_out=2, lr_decay = 0.9, Num_updates = 1000)
print(A[0])
MD_errors.append(np.round(A[1], decimals=4).tolist())
MD_errors_stds.append(np.round(A[2], decimals=4).tolist())
print("time: ", time.time() - s_time)

s_time = time.time()
pair = pairs_final[3]
print(colored(f"pair={pair}", 'green'))
A = MD_NeuralNetworkClassificationCNN(data[pair[0]], data[pair[1]], maj_num=11, epoch=50, 
                                      init_iter=3, test_size=0.3, std_coeff=1, 
                                      out_channels=10, kernel_size = 5, Q_size = 20, 
                                      padding = 1, learning_rate = 1e-2, bias = True, 
                                      D_out=2, lr_decay = 0.9, Num_updates = 1000)
print(A[0])
MD_errors.append(np.round(A[1], decimals=4).tolist())
MD_errors_stds.append(np.round(A[2], decimals=4).tolist())
print("time: ", time.time() - s_time)

print('errors = ', MD_errors)
print('errors_stds = ', MD_errors_stds)
print("===========================================================================")
print(colored(f'average errors: {np.round(np.mean(MD_errors), decimals=4)}', 'blue'))
print(colored(f'average stds:   {np.round(np.mean(MD_errors_stds), decimals=4)}', 'green'))

"""# Plot test errors"""

def lower_limit_error(x, y):
    if y - x < 0:
        return y
    else:
        return x

lower_limit_error = np.vectorize(lower_limit_error)

A_LSVM = [0.2740, 0.3269, 0.4772, 0.3214, 0.2888, 0.3258, 0.4860, 0.3339, 0.4326]
A_GSVM = [0.2090, 0.2788, 0.3414, 0.2854, 0.2447, 0.2744, 0.3496, 0.2886, 0.2810]
A_PSVM = [0.2518, 0.3302, 0.3822, 0.3196, 0.2567, 0.3278, 0.3829, 0.3226, 0.3311]
A_DT = [0.1836, 0.2221, 0.2955, 0.2386, 0.2132, 0.2492, 0.3244, 0.2631, 0.2136]
A_RF = [0.1652, 0.2172, 0.2812, 0.2293, 0.1731, 0.2213, 0.2875, 0.2320, 0.1723]
A_KNN = [0.1931, 0.2303, 0.3201, 0.2298, 0.2067, 0.2306, 0.3406, 0.2388, 0.2156]
A_LR = [0.2954, 0.3496, 0.4175, 0.3293, 0.3022, 0.3469, 0.4168, 0.3318, 0.3461]
A_CNN = [0.2674, 0.2878, 0.3554, 0.2862, 0.2563, 0.2926, 0.3639, 0.2890, 0.3987]

A = np.array([A_LSVM, A_GSVM, A_PSVM, A_DT, A_RF, A_KNN, A_LR, A_CNN])
B = A.T

std_LSVM = [0.0497, 0.0412, 0.0460, 0.0410, 0.0448, 0.0417, 0.0476, 0.0469, 0.0368]
std_GSVM = [0.0373, 0.0356, 0.0320, 0.0339, 0.0364, 0.0338, 0.0355, 0.0350, 0.0300]
std_PSVM = [0.0356, 0.0283, 0.0174, 0.0300, 0.0397, 0.0324, 0.0184, 0.0326, 0.0226]
std_DT = [0.0339, 0.0381, 0.0399, 0.0404, 0.0390, 0.0423, 0.0452, 0.0454, 0.0428]
std_RF = [0.0334, 0.0374, 0.0363, 0.0356, 0.0338, 0.0402, 0.0412, 0.0401, 0.0341]
std_KNN = [0.0354, 0.0395, 0.0374, 0.0360, 0.0395, 0.0370, 0.0431, 0.0400, 0.0384]
std_LR = [0.0383, 0.0219, 0.0121, 0.0347, 0.0359, 0.0238, 0.0121, 0.0331, 0.0170]
std_CNN = [0.0465, 0.0300, 0.0458, 0.0292, 0.0449, 0.0366, 0.0423, 0.0356, 0.0155]

C = np.array([std_LSVM, std_GSVM, std_PSVM, std_DT, std_RF, std_KNN, std_LR, std_CNN]).T

# for KNN
errors = [0.2349, 0.2111, 0.1985, 0.1921, 0.1946, 0.1940, 0.2138, 0.1717, 0.3315, 
          0.4413, 0.3649, 0.2051]

stds = [0.0372, 0.0375, 0.0377, 0.0357, 0.0368, 0.0346, 0.0468, 0.0383, 0.0464,
        0.0422, 0.0869, 0.0352]

classifiers = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

FM = (r'KNN', r'Vote(MD $v_Q^{exp}$)', r'Vote(Rand $v_Q$)', r'Vote($v_Q^{\varsigma}$)', 
      r'Vote(Rand $v_Q^{exp}$)', r'MD $v_Q^{exp}$', r'Rand $v_Q$', 
      r'Rand $v_Q^{\varsigma}$', r'Rand $v_Q^{exp}$', 'Endpoints')

labels = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

dists = [r'$d_F$', r'$d_{dF}$', r'$dH$', r'DTW', r'soft-dtw', r'fastdtw', 
         r'LCSS', r'SSPD', r'EDR', r'ERP', r'LSH', r'$d_Q^{\pi}$']

width=0.1
index = np.arange(len(stds))
ind = np.arange(len(A)) + len(index) * 0.25

plt.subplots(figsize = (20, 6.5), tight_layout=True)
bars = [0] * 10

lower_lim = lower_limit_error(stds, errors)
bars[0] = plt.bar(index * 0.2, errors, width, yerr=[lower_lim, stds], capsize=2)

for i in range(len(FM)-1):
    lower_limit = lower_limit_error(C[i], B[i])
    bars[i+1] = plt.bar(ind+width*i-0.3, B[i], width, yerr=[lower_limit, C[i]], capsize=2)

plt.title('Geolife Trajectory Data', fontsize = 20)
plt.xticks(list(np.arange(len(stds)) * 0.2) + list(ind + 0.5 * width), 
           dists + labels, fontsize = 14)

plt.legend(tuple(bars), FM, loc=0, fontsize = 11)

plt.gca().yaxis.grid(color='gray', linestyle='dotted', linewidth=0.6)
plt.xticks(rotation='vertical', fontsize = 14)
plt.yticks(fontsize = 14)

plt.savefig('/Users/hasan/Desktop/plots/plots Geolife/Geolife bar chart all horizontal.png', 
            bbox_inches='tight', dpi=200)

plt.show()

