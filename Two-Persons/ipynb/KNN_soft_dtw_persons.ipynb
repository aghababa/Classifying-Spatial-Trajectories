{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np \n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from scipy import linalg as LA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from termcolor import colored\n",
    "import similaritymeasures\n",
    "from sdtw import SoftDTW\n",
    "from sdtw.distance import SquaredEuclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    data = []\n",
    "    Data = []\n",
    "    flag = True\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            item = line.strip().split(\"|\")\n",
    "            if flag == True and len(item) == 5:\n",
    "                data.append([float(item[0]), float(item[1])])\n",
    "                Data.append([float(item[3]), float(item[4])])\n",
    "                flag = False\n",
    "            elif flag == True and len(item) == 6:\n",
    "                data.append([float(item[0]), float(item[1])])\n",
    "                Data.append([float(item[3]), float(item[4])])\n",
    "                flag = False\n",
    "            else:\n",
    "                flag = True\n",
    "    return np.array(data), np.array(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = glob.glob('/Users/hasan/Desktop/Anaconda/Research/gpsdata/person1/**/', recursive=True)[1:]\n",
    "P2 = glob.glob('/Users/hasan/Desktop/Anaconda/Research/gpsdata/person2/**/', recursive=True)[1:]\n",
    "I1 = glob.glob('/Users/hasan/Desktop/Anaconda/Research/gpsdata/person1/**/*.txt', recursive=True)\n",
    "I2 = glob.glob('/Users/hasan/Desktop/Anaconda/Research/gpsdata/person2/**/*.txt', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4151.7448, 8739.0343, ' -5:09:12 ', 221.635987, 358.560798],\n",
       "       [4151.7404, 8739.0384, ' -5:09:22 ', 221.632501, 358.555709],\n",
       "       [4151.7418, 8739.0387, ' -5:09:23 ', 221.63223100000002,\n",
       "        358.557317],\n",
       "       ...,\n",
       "       [4147.5155, 8806.6687, ' -5:43:28 ', 197.887199, 353.570877],\n",
       "       [4147.5141, 8806.6688, ' -5:43:29 ', 197.887118,\n",
       "        353.56926699999997],\n",
       "       [4147.5126, 8806.6689, ' -5:43:30 ', 197.88703600000002, 353.5675]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = pd.read_csv(I1[0], header=None, delimiter=\"|\")\n",
    "np.array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 35, 124, 89, 1.3932584269662922)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(P1), len(P2), len(I1), len(I2), len(I1)/len(I2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_lan_long = [0] * len(I1) # trajectories in lan,long-coordinates\n",
    "data1_x_y = [0] * len(I1) # trajectories in projected x,y-coordinate\n",
    "data2_lan_long = [0] * len(I2) # trajectories in lan,long-coordinates\n",
    "data2_x_y = [0] * len(I2) # trajectories in projected x,y-coordinate\n",
    "\n",
    "for i in range(len(I1)):\n",
    "    z =read_file(I1[i])\n",
    "    data1_lan_long[i] = z[0]\n",
    "    data1_x_y[i] = z[1]\n",
    "\n",
    "for i in range(len(I2)):\n",
    "    z =read_file(I2[i])\n",
    "    data2_lan_long[i] = z[0]\n",
    "    data2_x_y[i] = z[1]\n",
    "    \n",
    "data1_lan_long = np.array(data1_lan_long, dtype = 'object')\n",
    "data1_x_y = np.array(data1_x_y, dtype = 'object')\n",
    "data2_lan_long = np.array(data2_lan_long, dtype = 'object')\n",
    "data2_x_y = np.array(data2_x_y, dtype = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_lan_long = data1_x_y\n",
    "data2_lan_long = data2_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_segments(traj): # removes stationary points\n",
    "    p2 = traj[1:]\n",
    "    p1 = traj[:-1]\n",
    "    L = ((p2-p1)*(p2-p1)).sum(axis =1)\n",
    "    I = np.where(L>1e-16)[0]\n",
    "    return traj[I]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/hasan/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(124, 89)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_lan_long = np.array([remove_segments(data1_lan_long[i]) for i in range(len(data1_lan_long))])\n",
    "\n",
    "data2_lan_long = np.array([remove_segments(data2_lan_long[i]) for i in range(len(data2_lan_long))])\n",
    "\n",
    "len(data1_lan_long), len(data2_lan_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 88)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1 = np.array([len(data1_lan_long[i]) for i in range(len(data1_lan_long))])\n",
    "L2 = np.array([len(data2_lan_long[i]) for i in range(len(data2_lan_long))])\n",
    "    \n",
    "len(np.where(L1 < 2500)[0]), len(np.where(L2 < 2000)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  70,   92,  120,  139,  144,  188,  203,  229,  263,  266,  284,\n",
       "         293,  294,  310,  323,  359,  373,  385,  416,  424,  427,  427,\n",
       "         431,  447,  472,  675,  838,  844,  846,  895,  923,  927,  957,\n",
       "         984, 1000, 1009, 1010, 1035, 1057, 1077, 1097, 1156, 1160, 1178,\n",
       "        1179, 1206, 1219, 1226, 1227, 1331, 1357, 1372, 1384, 1459, 1471,\n",
       "        1656, 1729, 1822, 1899, 1902, 1908, 1909, 1924, 1925, 1932, 1942,\n",
       "        1948, 1949, 1957, 1994, 2006, 2045, 2065, 2065, 2083, 2098, 2098,\n",
       "        2117, 2146, 2181, 2206, 2234, 2259, 2290, 2325, 2374, 2398, 2419,\n",
       "        2422, 2431, 2464, 2475, 2486, 2505, 2505, 2506, 2527, 2573, 2576,\n",
       "        2594, 2609, 2616, 2663, 2672, 2673, 2683, 2781, 2870, 3013, 3121,\n",
       "        3128, 3150, 3197, 3239, 3307, 3383, 3417, 3424, 3807, 3864, 3965,\n",
       "        4053, 4414, 5493]),\n",
       " array([  72,   92,   93,   95,  131,  139,  143,  145,  153,  164,  165,\n",
       "         179,  179,  180,  186,  188,  188,  192,  205,  207,  213,  214,\n",
       "         216,  219,  225,  248,  248,  254,  255,  257,  258,  260,  260,\n",
       "         263,  263,  275,  276,  276,  278,  281,  285,  286,  290,  290,\n",
       "         293,  295,  299,  310,  315,  317,  334,  339,  339,  349,  349,\n",
       "         357,  371,  376,  381,  382,  388,  388,  394,  395,  409,  440,\n",
       "         446,  447,  458,  465,  468,  473,  476,  481,  483,  483,  496,\n",
       "         502,  513,  515,  605,  610,  631,  634,  644,  645,  752, 1465,\n",
       "        2686]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(L1), np.sort(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1757.1693548387098, 364.9887640449438)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(L1), np.mean(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mu(data_1, data_2):\n",
    "    a = np.mean([np.mean(data_1[i], 0) for i in range(len(data_1))], 0)\n",
    "    b = np.mean([np.mean(data_2[i], 0) for i in range(len(data_2))], 0)\n",
    "    c = abs(a-b)\n",
    "    return max(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN with Soft-DTW with matrix saving method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dists_soft_dtw(data1, data2, gamma, path): \n",
    "    start_time = time.time() \n",
    "    data = np.concatenate((data1, data2), 0)\n",
    "    n = len(data)\n",
    "    A = []\n",
    "    for i in range(n-1):\n",
    "        for j in range(i+1, n):\n",
    "            D = SquaredEuclidean(data[i], data[j])\n",
    "            sdtw = SoftDTW(D, gamma=gamma)\n",
    "            A.append(sdtw.compute())\n",
    "    A = np.array(A)\n",
    "    tri = np.zeros((n, n))\n",
    "    tri[np.triu_indices(n, 1)] = A\n",
    "    for i in range(1, n):\n",
    "        for j in range(i):\n",
    "            tri[i][j] = tri[j][i]\n",
    "    np.savetxt(path, tri, delimiter=',')\n",
    "    total_time = time.time() - start_time\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1422.896469116211"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/hasan/Desktop/Anaconda/Research/Calculated Distance Matrices for KNN/persons-soft-dtw.csv'\n",
    "calculate_dists_soft_dtw(data1_lan_long, data2_lan_long, gamma=1e-10, path=path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_with_dists_soft_dtw(n_1, n_2, path_to_dists):\n",
    "    '''path example: '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/sspd.csv'\n",
    "       path_to_dists: the path to the corresponding distance matrix\n",
    "       n_1: len(data_1)\n",
    "       n_2: len(data_2)'''\n",
    "\n",
    "    I_1, J_1, y_train_1, y_test_1 = train_test_split(np.arange(n_1), \n",
    "                                                np.ones(n_1), test_size=0.3)\n",
    "    I_2, J_2, y_train_2, y_test_2 = train_test_split(np.arange(n_1, n_1+n_2), \n",
    "                                                np.ones(n_2), test_size=0.3)\n",
    "    labels = np.array([1] * n_1 + [0] * n_2)\n",
    "    I = np.concatenate((I_1, I_2), 0)\n",
    "    np.random.shuffle(I)\n",
    "    J = np.concatenate((J_1, J_2), 0)\n",
    "    np.random.shuffle(J)\n",
    "\n",
    "    dist_matrix = np.array(pd.read_csv(path_to_dists,  header=None))\n",
    "\n",
    "    D_train = dist_matrix[I][:, I]\n",
    "    D_test = dist_matrix[J][:,I]\n",
    "    train_labels = labels[I]\n",
    "    test_labels = labels[J]\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5, metric='precomputed')\n",
    "    \n",
    "    #Train the model using the training sets\n",
    "    clf.fit(D_train, list(train_labels))\n",
    "\n",
    "    #Predict labels for train dataset\n",
    "    train_pred = clf.predict(D_train)\n",
    "    train_error = sum(train_labels != train_pred)/len(I)\n",
    "    \n",
    "    #Predict labels for test dataset\n",
    "    test_pred = clf.predict(D_test)\n",
    "    test_error = sum((test_labels != test_pred))/len(J)\n",
    "        \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_average_error_soft_dtw(data1, data2, num_trials, path_to_dists):\n",
    "\n",
    "    '''path_to_dists: the path to the corresponding distance matrix'''\n",
    "\n",
    "    Start_time = time.time()\n",
    "\n",
    "    train_errors = np.zeros(num_trials)\n",
    "    test_errors = np.zeros(num_trials)\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        train_errors[i], test_errors[i] = KNN_with_dists_soft_dtw(len(data1), len(data2), path_to_dists)\n",
    "\n",
    "    Dict = {}\n",
    "    Dict[1] = [f\"KNN with soft dtw\", \n",
    "                    np.round(np.mean(train_errors), decimals = 4), \n",
    "                    np.round(np.mean(test_errors), decimals = 4), \n",
    "                    np.round(np.std(test_errors), decimals = 4)]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(Dict, orient='index', columns=['Classifier',\n",
    "                                'Train Error', 'Test Error', 'std'])\n",
    "    print(colored(f\"num_trials = {num_trials}\", \"blue\"))\n",
    "    print(colored(f'total time = {time.time() - Start_time}', 'green'))\n",
    "\n",
    "    return (df, np.mean(train_errors), np.mean(test_errors), np.std(test_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mnum_trials = 50\u001b[0m\n",
      "\u001b[32mtotal time = 1.273284912109375\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Train Error</th>\n",
       "      <th>Test Error</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>KNN with soft dtw</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.0212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Classifier  Train Error  Test Error     std\n",
       "1  KNN with soft dtw       0.0459      0.0591  0.0212"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/hasan/Desktop/Anaconda/Research/Calculated Distance Matrices for KNN/persons-soft-dtw.csv'\n",
    "A = KNN_average_error_soft_dtw(data1_lan_long, data2_lan_long, num_trials=50, \n",
    "                               path_to_dists=path)\n",
    "A[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This class handles all the metrics in \"metrics array bellow\" and is appropriate for using in Anaconda \n",
    "   for example, but not on Google Colab.'''\n",
    "\n",
    "'''Requirements: (These are already installed in my computer)\n",
    "        1. We need \"pip install trjtrypy\" in order to be able to use d_Q_pi\n",
    "        2. We need \"pip install tslearn\" in order to be able to use dtw\n",
    "        3. We need \"pip install fastdtw\" in order to be able to use fastdtw\n",
    "        4. We need \"pip install traj_dist\" in order to be able to use the rest of metrics'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "import traj_dist.distance as tdist\n",
    "import pickle\n",
    "import tslearn\n",
    "from tslearn.metrics import dtw as dtw_tslearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from trjtrypy.distances import d_Q_pi\n",
    "from termcolor import colored\n",
    "\n",
    "#metrics = ['discret_frechet', 'hausdorff', 'dtw', SoftDTW, 'sspd', 'erp', 'edr', 'lcss',  \n",
    "#           fastdtw, dtw, d_Q_pi]\n",
    "\n",
    "# path example: \n",
    "#'Calculated Distance Matrices for KNN/Beijing-Pairs['+str(pairs_final[i])+']-d_Q_pi.csv'\n",
    "\n",
    "\n",
    "class KNN_runTime:\n",
    "    \n",
    "    def __init__(self, data1, data2, metric, gamma=None, eps_edr=None, eps_lcss=None, \n",
    "                 Q_size=None, Q=None, p=2, n_neighbors=5, pair=None):\n",
    "        '''data1 = data[pair[0]]\n",
    "           data2 = data[pair[1]]'''\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.metric = metric\n",
    "        self.gamma = gamma\n",
    "        self.eps_edr = eps_edr\n",
    "        self.eps_lcss = eps_lcss\n",
    "        self.Q_size = Q_size\n",
    "        self.Q = Q\n",
    "        self.p = p\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.pair = pair\n",
    "\n",
    "\n",
    "    def calculate_dists_matrix(self):\n",
    "\n",
    "        data = np.concatenate((self.data1, self.data2), 0)\n",
    "        n = len(data)\n",
    "\n",
    "        if self.metric == 'lcss':\n",
    "            A = tdist.pdist(data, self.metric, type_d=\"euclidean\", eps=self.eps_lcss)\n",
    "        if self.metric == 'edr':\n",
    "            A = tdist.pdist(data, self.metric, type_d=\"euclidean\", eps=self.eps_edr)\n",
    "        if self.metric in ['discret_frechet', 'hausdorff', 'dtw', 'sspd', 'erp']: \n",
    "            A = tdist.pdist(data, str(self.metric))\n",
    "        if str(self.metric) == str(SoftDTW): \n",
    "            A = []\n",
    "            for i in range(n-1):\n",
    "                for j in range(i+1, n):\n",
    "                    D = SquaredEuclidean(data[i], data[j])\n",
    "                    sdtw = self.metric(D, gamma=self.gamma)\n",
    "                    A.append(sdtw.compute())\n",
    "        if self.metric == fastdtw: \n",
    "            A = []\n",
    "            for i in range(n-1):\n",
    "                for j in range(i+1, n):\n",
    "                    A.append(self.metric(data[i], data[j])[0])\n",
    "        if self.metric == dtw_tslearn: \n",
    "            A = []\n",
    "            for i in range(n-1):\n",
    "                for j in range(i+1, n):\n",
    "                    A.append(self.metric(data[i], data[j]))\n",
    "        if self.metric == 'd_Q_pi':\n",
    "            A = []\n",
    "            if self.Q_size:\n",
    "                Q = self.generate_random_Q()\n",
    "                for i in range(n-1):\n",
    "                    for j in range(i+1, n):\n",
    "                        A.append(d_Q_pi(Q, data[i], data[j], p=self.p))\n",
    "            elif len(self.Q):\n",
    "                for i in range(n-1):\n",
    "                    for j in range(i+1, n):\n",
    "                        A.append(d_Q_pi(self.Q, data[i], data[j], p=self.p))\n",
    "\n",
    "        tri = np.zeros((n, n))\n",
    "        tri[np.triu_indices(n, 1)] = np.array(A)\n",
    "        for i in range(1, n):\n",
    "            for j in range(i):\n",
    "                tri[i][j] = tri[j][i]\n",
    "                \n",
    "        return tri\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''The following function is only used for d_Q_pi distance in order to \n",
    "       generate random landmarks\n",
    "       Notice: in this pattern of coding we cannot use train1 and train2 to \n",
    "       get Q in the following function.'''\n",
    "    def generate_random_Q(self):\n",
    "        Q = np.zeros((self.Q_size, 2))\n",
    "        data = np.concatenate((self.data1, self.data2), 0)\n",
    "        Mean = np.mean([np.mean(data[i], 0) for i in range(len(data))], 0)\n",
    "        Std = np.std([np.std(data[i], 0) for i in range(len(data))], 0)\n",
    "        Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], self.Q_size)\n",
    "        Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], self.Q_size)\n",
    "        return Q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def KNN_runtime(self):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        n_1 = len(self.data1)\n",
    "        n_2 = len(self.data2) \n",
    "        \n",
    "        labels = np.array([1] * n_1 + [-1] * n_2)\n",
    "        data = np.concatenate((self.data1, self.data2), 0)\n",
    "        dist_matrix = self.calculate_dists_matrix()\n",
    "\n",
    "        clf = KNeighborsClassifier(n_neighbors=self.n_neighbors, metric='precomputed')\n",
    "        clf.fit(dist_matrix, list(labels))\n",
    "        stop_time = time.time()\n",
    "        runtime = stop_time - start_time\n",
    "\n",
    "        return runtime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "969 ns ± 53 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
      "\u001b[35mruntimes for <class 'sdtw.soft_dtw.SoftDTW'>: 1378.758239030838\u001b[0m\n",
      "\u001b[31m===============================================================\u001b[0m\n",
      "\u001b[34m[1378.758239030838]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "metrics = ['discret_frechet', 'hausdorff', dtw_tslearn, fastdtw, 'lcss', 'sspd',\n",
    "           'edr', 'erp', 'd_Q_pi']\n",
    "metrics = [SoftDTW]\n",
    "runtimes = []\n",
    "for metric in metrics:\n",
    "    %timeit KNN_runTime(data1_lan_long, data2_lan_long, metric=metric, gamma=1e-10, \\\n",
    "                        eps_edr=0.1, eps_lcss=0.1, Q_size=20, Q=None, p=2, n_neighbors=5, \\\n",
    "                        pair=[0,1])\n",
    "\n",
    "    Runtime = KNN_runTime(data1_lan_long, data2_lan_long, metric=metric, gamma=1e-10, \n",
    "                          eps_edr=0.02, eps_lcss=0.02, Q_size=20, Q=None, p=2, \n",
    "                          n_neighbors=5, pair=[0,1])\n",
    "    a = Runtime.KNN_runtime()\n",
    "    runtimes.append(a)\n",
    "    print(colored(f'runtimes for {metric}: {a}', 'magenta'))\n",
    "    print(colored(\"===============================================================\", 'red'))\n",
    "\n",
    "print(colored(runtimes, 'blue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
