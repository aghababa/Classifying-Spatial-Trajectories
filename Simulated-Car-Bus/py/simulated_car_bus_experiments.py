# -*- coding: utf-8 -*-
"""Simulated_Car_Bus_Experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wiinUEVFj_ILT4yVaPqCcp1CHspyYhIE
"""

#pip install similaritymeasures

#pip install tslearn

#pip install frechetdist

pip install trjtrypy

"""# Libraries"""

import glob
import numpy as np 
import time
import math
import random
from scipy import linalg as LA
import pandas as pd
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.svm import NuSVC
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import Perceptron
import statsmodels.api as sm
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
from termcolor import colored
import matplotlib as mpl
from scipy import optimize
from sklearn.svm import LinearSVC
from termcolor import colored
from scipy.stats import entropy
import os
import ast
import csv
import json 
import scipy.io
import trjtrypy as tt
from trjtrypy.featureMappings import curve2vec
from trjtrypy.distances import d_Q_pi
#import similaritymeasures
#import tslearn
#from tslearn.metrics import dtw
#from scipy.spatial.distance import directed_hausdorff
#from frechetdist import frdist

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import sampler
import torchvision.transforms as T

from google.colab import drive
drive.mount("/content/gdrive")

"""# Read car-bus data"""

def read_file(file_name):
    data = []
    with open(file_name, "r") as f:
        for line in f:
            item = line.strip().split(",")
            data.append(np.array(item))
    return np.array(data)

data1 = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_tracks.csv')[1:,:-1]
data2 = read_file('/content/gdrive/My Drive/Colab Notebooks/go_track_trackspoints.csv')[1:,1:4]

float1 = np.vectorize(float)
int1 = np.vectorize(int)
track_id = int1(data1[:,0])
labels = np.where(int1(data1[:,-1]) < 1.5, int1(data1[:,-1]), -1) 
traj = float1(data2)

sum(labels==1), sum(labels==-1)

trajec = [0] * 163

for i in range(163):
    trajec[i] = []
    I = np.where(traj[:,2] == track_id[i])
    trajec[i] = np.array([labels[i], traj[I]], dtype = 'object')

trajec = np.array(trajec)

trajectory = [0] * 163
trajectory_label_id = [0] * 163

for i in range(163):
    trajectory[i] = trajec[i][1][:,:2]
    trajectory_label_id[i] = np.array([trajec[i][1][:,:2], trajec[i][0], 
                                       trajec[i][1][:,2][0]], dtype = 'object')
    
trajectory_label_id = np.array(trajectory_label_id, dtype = 'object')
trajectory = np.array(trajectory, dtype = 'object')

min_length = 10
max_length = 1000 #160 for balance data
l = 0
index = [] 
for i in range(163):
    if len(trajectory[i]) < min_length or len(trajectory[i]) > max_length:
        l = l + 1
    else:
        index.append(i)
        
l, 163-l

trajectories = [0] * (163-l)
trajectories_label_id = [0] * (163-l)

j = 0
for i in range(163):
    if len(trajectory[i]) >= min_length and len(trajectory[i]) <= max_length:
        trajectories[j] = np.array(trajectory[i])
        trajectories_label_id[j] = trajectory_label_id[i]
        j = j + 1

trajectories_label_id = np.array(trajectories_label_id, dtype = 'object')
trajectories = np.array(trajectories, dtype = 'object')

cars = trajectories_label_id[np.where(trajectories_label_id[:,1] == 1)][:,:2][:,0]
buses = trajectories_label_id[np.where(trajectories_label_id[:,1] == -1)][:,:2][:,0]

cars_copy = cars.copy()
buses_copy = buses.copy()
len(cars), len(buses)

def remove_segments(traj): # removes stationary points
    p2 = traj[1:]
    p1 = traj[:-1]
    L = ((p2-p1)*(p2-p1)).sum(axis =1)
    I = np.where(L>1e-16)[0]
    return traj[I]

cars = np.array(list(map(remove_segments, cars)), dtype='object')
buses = np.array(list(map(remove_segments, buses)), dtype='object')

I = np.where(np.array([len(cars[i]) for i in range(len(cars))]) > 1)
J = np.where(np.array([len(buses[i]) for i in range(len(buses))]) > 1)

cars = cars[I]
buses = buses[J]

cars_copy = cars.copy()
buses_copy = buses.copy()
print("len(cars), len(buses)=", len(cars), len(buses))

a = np.arange(len(cars))
I = np.where((a != 28) & (a != 29))
cars = cars[I]
buses = buses[:-1]
len(cars), len(buses)

a, c = np.min((np.min([np.min(cars[i], axis=0) for i in range(len(cars))], axis=0), 
       np.min([np.min(buses[i], axis=0) for i in range(len(buses))], axis=0)), axis=0)
  
b, d = np.max((np.max([np.max(cars[i], axis=0) for i in range(len(cars))], axis=0), 
               np.max([np.max(buses[i], axis=0) for i in range(len(buses))], axis=0)), axis=0)

m = 20
Q = np.ones((m,2))

Q[:,0] = (b - a + 0.001) * np.random.random_sample(m) + a - 0.02
Q[:,1] = (d - c + 0.001) * np.random.random_sample(m) + c - 0.01

for i in range(len(cars)):
    plt.plot(cars[i][:,0], cars[i][:,1], color = "steelblue");
for i in range(len(buses)):
    plt.plot(buses[i][:,0], buses[i][:,1], color = "r");
plt.scatter(Q[:,0], Q[:,1], color = "black")
print(colored(f'Original car-bus', 'yellow'))
plt.show()

"""# Classifiers and get_mu function"""

CC = [100, 100, 10]
number_estimators = [50, 50]


clf0 = [make_pipeline(LinearSVC(dual=False, C=CC[0], tol=1e-5, 
                               class_weight ='balanced', max_iter=1000)), 
        "SVM, LinearSVC, C = "+str(CC[0])]
clf1 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[1], kernel='rbf', gamma='auto', max_iter=200000)),
        "Gaussian SVM, C="+str(CC[1])+", gamma=auto"]
clf2 = [make_pipeline(StandardScaler(), svm.SVC(C=CC[2], kernel='poly', degree=3, max_iter=400000)),
        "Poly kernel SVM, C="+str(CC[2])+", deg=auto"]
clf3 = [DecisionTreeClassifier(), "Decision Tree"]
clf4 = [RandomForestClassifier(n_estimators=number_estimators[0]), 
         "RandomForestClassifier, n="+str(number_estimators[0])]
clf5 = [KNeighborsClassifier(n_neighbors=5), "KNN"]
clf6 = [LogisticRegression(solver='newton-cg'), "Logistic Regression"]

clf = [clf0, clf1, clf2, clf3, clf4, clf5, clf6]
classifs = [item[0] for item in clf]
keys = [item[1] for item in clf]

def get_mu(data_1, data_2):
    a = np.mean([np.mean(data_1[i], 0) for i in range(len(data_1))], 0)
    b = np.mean([np.mean(data_2[i], 0) for i in range(len(data_2))], 0)
    c = abs(a-b)
    return max(c)

"""# The following 3 blocks are run and stored and fixed. We should not run it again.

# Read the fixed datasets data_1 and data_2 from gdrive
"""

data_1 = []
data_2 = []
for i in range(226):
    path = '/Users/hasan/Desktop/Classifying-Spatial-Trajectories-main/Simulated_Car_Bus_data/data_1/'+str(i)+'.csv'
    #path = '/content/gdrive/My Drive/car-bus-generated/data_1/'+str(i)+'.csv'
    data_1.append(np.array(pd.read_csv(path, header=None)))

for i in range(220):
    path = '/Users/hasan/Desktop/Classifying-Spatial-Trajectories-main/Simulated_Car_Bus_data/data_2/'+str(i)+'.csv'
    #path = '/content/gdrive/My Drive/car-bus-generated/data_2/'+str(i)+'.csv'
    data_2.append(np.array(pd.read_csv(path, header=None)))

data_1 = np.array(data_1, dtype = "object")
data_2 = np.array(data_2, dtype = "object")
print("data_1.shape, data_2.shape=", data_1.shape, data_2.shape)

"""## Plot data"""

for i in range(len(data_1)):
    if data_1[i][0][0] < -10.7:
        plt.plot(data_1[i][:,0], data_1[i][:,1], color='blue')
for i in range(len(data_2)):
    plt.plot(data_2[i][:,0], data_2[i][:,1], color='red')
plt.title('Simulated Car-Bus')
plt.savefig(f'/Users/hasan/Desktop/Anaconda/Research/Pictures for 2ed paper/Simulated Car-Bus data.png', 
            bbox_inches='tight', dpi=200)
plt.show()

"""# Classification with feature mappings $v_Q$, $v_Q^{\exp}$, $v_Q^{\varsigma}$ with 20 random landmarks, and endpoints"""

from google.colab import files
files.upload()

import v_Q_mu_endpoints_classification
from v_Q_mu_endpoints_classification import binaryClassificationAverageMajority

"""### Boost($v_Q$), Boost($v_Q^{\varsigma}$), Boost($v_Q^{\exp}$) with epoch=50 and num_trials_maj=11"""

classifs = binaryClassificationAverageMajority(data_1, data_2, 
            Q_size=20, epoch=50, num_trials_maj=11, classifiers=clf,
            version='unsigned', test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(data_1, data_2, 
            Q_size=20, epoch=50, num_trials_maj=11, classifiers=clf,
            version='signed', sigma=1, test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(data_1, data_2, Q_size=20, epoch=50, 
                                num_trials_maj=11, classifiers=clf, test_size=0.3)
A = classifs.classification_v_Q_mu()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

"""### Rand $v_Q$, Rand $v_Q^{\varsigma}$, Rand $v_Q^{\exp}$ with epoch=50 and num_trials_maj=1"""

classifs = binaryClassificationAverageMajority(data_1, data_2, Q_size=20, 
                                    epoch=50, num_trials_maj=1, classifiers=clf, 
                                    version='unsigned', test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(data_1, data_2, Q_size=20, 
                                    epoch=50, num_trials_maj=1, classifiers=clf, 
                                    version='signed', sigma=1, test_size=0.3)
A = classifs.classification_v_Q()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(data_1, data_2, Q_size=20, 
                    epoch=50, num_trials_maj=1, classifiers=clf, test_size=0.3)
A = classifs.classification_v_Q_mu()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

classifs = binaryClassificationAverageMajority(data_1, data_2, Q_size=20, 
                                               epoch=50, num_trials_maj=1, 
                                               classifiers=clf, test_size=0.3)
A = classifs.endpoint_classification()
print(A[0])
print('test_errors: \n', list(np.round(A[2], decimals=4)))
print('std_test_errors: \n', list(np.round(A[3], decimals=4)))

"""# Classification with Perceptron-Like algorithm"""

from google.colab import files
files.upload()

import Perceptron_Like_Algo_Class
from Perceptron_Like_Algo_Class import classification

classif = classification(data_1, data_2, Q_size=20, model="GSVM", C=50, gamma='auto', 
                        classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

A = classif.classification_Q()
print(A[0])
print(colored(f"mu = {A[1]}", 'blue'))

for Model in ['GSVM']:
    classif = classification(data_1, data_2, Q_size=20, model=Model, C=50, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=1, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

"""## Boost(MD $v_Q^{\exp}$) with epoch=50, maj_num=11 and init_iter=3 (done)"""

classif = classification(data_1, data_2, Q_size=20, model="LSVM", C=100, gamma='auto', 
                        classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

A = classif.classification_Q()
print(A[0])
print(colored(f"mu = {A[1]}", 'blue'))

classif = classification(data_1, data_2, Q_size=20, model="GSVM", C=100, gamma='auto', 
                        classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

A = classif.classification_Q()
print(A[0])
print(colored(f"mu = {A[1]}", 'blue'))

classif = classification(data_1, data_2, Q_size=20, model="PSVM", C=100, gamma='auto', 
                        classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

A = classif.classification_Q()
print(A[0])
print(colored(f"mu = {A[1]}", 'blue'))

classif = classification(data_1, data_2, Q_size=20, model="DT", C=100, gamma='auto', 
                        classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

A = classif.classification_Q()
print(A[0])
print(colored(f"mu = {A[1]}", 'blue'))

classif = classification(data_1, data_2, Q_size=20, model="RF", C=100, gamma='auto', 
                        classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

A = classif.classification_Q()
print(A[0])
print(colored(f"mu = {A[1]}", 'blue'))

classif = classification(data_1, data_2, Q_size=20, model="KNN", C=100, gamma='auto', 
                        classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

A = classif.classification_Q()
print(A[0])
print(colored(f"mu = {A[1]}", 'blue'))

classif = classification(data_1, data_2, Q_size=20, model="LR", C=100, gamma='auto', 
                        classifiers=[], epoch=50, maj_num=11, init_iter=3, 
                        std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

A = classif.classification_Q()
print(A[0])
print(colored(f"mu = {A[1]}", 'blue'))

"""## MD $v_Q^{\exp}$ with epoch=50, maj_num=1 and init_iter=3 (done)"""

for Model in ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR']:
    classif = classification(data_1, data_2, Q_size=20, model=Model, C=100, gamma='auto', 
                         classifiers=[], epoch=50, maj_num=1, init_iter=3, 
                         std_coeff=1, test_size=0.3, n_neighbors=5, n_estimators=50)

    A = classif.classification_Q()
    print(A[0])
    print(colored(f"mu = {A[1]}", 'blue'))
    print("===========================================================================")

"""# KNN with $d_Q^{\pi}$ and DTW

## KNN with $d_Q^{\pi}$ distance with matrix storing method

### Calculate distance matrix
"""

def calculate_dists_d_Q_pi(data1, data2, p, path): 
    start_time = time.time() 
    data = np.concatenate((data1, data2), 0)
    n = len(data)
    A = []
    for i in range(n-1):
        for j in range(i+1, n):
            A.append(d_Q_pi(Q, data[i], data[j], p=p))
    A = np.array(A)
    tri = np.zeros((n, n))
    tri[np.triu_indices(n, 1)] = A
    for i in range(1, n):
        for j in range(i):
            tri[i][j] = tri[j][i]
    np.savetxt(path, tri, delimiter=',')

    total_time = time.time() - start_time
    return total_time

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (simulated-car-bus)/d_Q_pi.csv'
calculate_dists_d_Q_pi(data_1, data_2, p=1, path=path)

def KNN_with_dists_d_Q_pi(n_1, n_2, path_to_dists):
    '''path example: '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/d_Q_pi.csv'
       path_to_dists: the path to the corresponding distance matrix
       n_1: len(data_1)
       n_2: len(data_2)'''

    I_1, J_1, y_train_1, y_test_1 = train_test_split(np.arange(n_1), 
                                                np.ones(n_1), test_size=0.3)
    I_2, J_2, y_train_2, y_test_2 = train_test_split(np.arange(n_1, n_1+n_2), 
                                                np.ones(n_2), test_size=0.3)
    labels = np.array([1] * n_1 + [0] * n_2)
    I = np.concatenate((I_1, I_2), 0)
    np.random.shuffle(I)
    J = np.concatenate((J_1, J_2), 0)
    np.random.shuffle(J)

    dist_matrix = np.array(pd.read_csv(path_to_dists,  header=None))

    D_train = dist_matrix[I][:, I]
    D_test = dist_matrix[J][:,I]
    train_labels = labels[I]
    test_labels = labels[J]

    clf = KNeighborsClassifier(n_neighbors=5, metric='precomputed')
    
    #Train the model using the training sets
    clf.fit(D_train, list(train_labels))

    #Predict labels for train dataset
    train_pred = clf.predict(D_train)
    train_error = sum(train_labels != train_pred)/len(I)
    
    #Predict labels for test dataset
    test_pred = clf.predict(D_test)
    test_error = sum((test_labels != test_pred))/len(J)
        
    return train_error, test_error

def KNN_average_error_d_Q_pi(data1, data2, num_trials, path_to_dists):

    '''path_to_dists: the path to the corresponding distance matrix'''

    Start_time = time.time()

    train_errors = np.zeros(num_trials)
    test_errors = np.zeros(num_trials)

    for i in range(num_trials):
        train_errors[i], test_errors[i] = KNN_with_dists_d_Q_pi(len(data1), len(data2), path_to_dists)

    Dict = {}
    Dict[1] = [f"KNN with d_Q_pi", 
                    np.round(np.mean(train_errors), decimals = 4), 
                    np.round(np.mean(test_errors), decimals = 4), 
                    np.round(np.std(test_errors), decimals = 4)]

    df = pd.DataFrame.from_dict(Dict, orient='index', columns=['Classifier',
                                'Train Error', 'Test Error', 'std'])
    print(colored(f"num_trials = {num_trials}", "blue"))
    print(colored(f'total time = {time.time() - Start_time}', 'green'))

    return (df, np.mean(train_errors), np.mean(test_errors), np.std(test_errors))

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (simulated-car-bus)/d_Q_pi.csv'
KNN_average_error_d_Q_pi(data_1, data_2, num_trials=50, path_to_dists=path)

"""## KNN with DTW from tslearn by saving matrx method

### Calculate distance matrix
"""

def calculate_dists_dtw_tslearn(data1, data2, path): 
    start_time = time.time() 
    data = np.concatenate((data1, data2), 0)
    n = len(data)
    A = []
    for i in range(n-1):
        for j in range(i+1, n):
            A.append(tslearn.metrics.dtw(data[i], data[j]))
    A = np.array(A)
    tri = np.zeros((n, n))
    tri[np.triu_indices(n, 1)] = A
    for i in range(1, n):
        for j in range(i):
            tri[i][j] = tri[j][i]
    np.savetxt(path, tri, delimiter=',')

    total_time = time.time() - start_time
    return total_time

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (simulated-car-bus)/dtw-tslearn.csv'
calculate_dists_dtw_tslearn(data_1, data_2, path=path)

def KNN_with_dists_dtw_tslearn(n_1, n_2, path_to_dists):
    '''path example: '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (car-bus)/sspd.csv'
       path_to_dists: the path to the corresponding distance matrix
       n_1: len(data_1)
       n_2: len(data_2)'''

    I_1, J_1, y_train_1, y_test_1 = train_test_split(np.arange(n_1), 
                                                np.ones(n_1), test_size=0.3)
    I_2, J_2, y_train_2, y_test_2 = train_test_split(np.arange(n_1, n_1+n_2), 
                                                np.ones(n_2), test_size=0.3)
    labels = np.array([1] * n_1 + [0] * n_2)
    I = np.concatenate((I_1, I_2), 0)
    np.random.shuffle(I)
    J = np.concatenate((J_1, J_2), 0)
    np.random.shuffle(J)

    dist_matrix = np.array(pd.read_csv(path_to_dists,  header=None))

    D_train = dist_matrix[I][:, I]
    D_test = dist_matrix[J][:,I]
    train_labels = labels[I]
    test_labels = labels[J]

    clf = KNeighborsClassifier(n_neighbors=5, metric='precomputed')
    
    #Train the model using the training sets
    clf.fit(D_train, list(train_labels))

    #Predict labels for train dataset
    train_pred = clf.predict(D_train)
    train_error = sum(train_labels != train_pred)/len(I)
    
    #Predict labels for test dataset
    test_pred = clf.predict(D_test)
    test_error = sum((test_labels != test_pred))/len(J)
        
    return train_error, test_error

def KNN_average_error_dtw_tslearn(data1, data2, num_trials, path_to_dists):

    '''path_to_dists: the path to the corresponding distance matrix'''

    Start_time = time.time()

    train_errors = np.zeros(num_trials)
    test_errors = np.zeros(num_trials)

    for i in range(num_trials):
        train_errors[i], test_errors[i] = KNN_with_dists_dtw_tslearn(len(data1), len(data2), path_to_dists)

    Dict = {}
    Dict[1] = [f"KNN with dtw from tslearn", 
                    np.round(np.mean(train_errors), decimals = 4), 
                    np.round(np.mean(test_errors), decimals = 4), 
                    np.round(np.std(test_errors), decimals = 4)]

    df = pd.DataFrame.from_dict(Dict, orient='index', columns=['Classifier',
                                'Train Error', 'Test Error', 'std'])
    print(colored(f"num_trials = {num_trials}", "blue"))
    print(colored(f'total time = {time.time() - Start_time}', 'green'))

    return (df, np.mean(train_errors), np.mean(test_errors), np.std(test_errors))

path = '/content/gdrive/My Drive/traj-dist/Calculated Distance Matrices (simulated-car-bus)/dtw-tslearn.csv'

G = KNN_average_error_dtw_tslearn(data_1, data_2, num_trials=50, path_to_dists=path)
G[0]

"""# Neural Networks (CNN)

## Helper functions
"""

from collections import Counter

def find_majority(votes):
    vote_count = Counter(votes)
    top = vote_count.most_common(1)
    return top[0][0]

def find_majority_array(A): # column-wise majority
    return list(map(find_majority, A.T))

def ExpCurve2Vec(points, curves, mu):
    D = tt.distsbase.DistsBase()
    a = np.array([np.exp(-1*np.power(D.APntSetDistACrv(points,curve),2)/(mu)**2) for curve in curves])
    return a

def get_mu(data_1, data_2):
    a = np.mean([np.mean(data_1[i], 0) for i in range(len(data_1))], 0)
    b = np.mean([np.mean(data_2[i], 0) for i in range(len(data_2))], 0)
    c = abs(a-b)
    return max(c)

def get_endpoints(data):
    n = len(data)
    data_endpoints = np.zeros((n, 4))
    for i in range(n):
        data_endpoints[i] = np.concatenate((data[i][0], data[i][-1]), 0)
    return data_endpoints

def train_test(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size = test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size = test_size)

    return train_idx_1, test_idx_1, train_label_1, test_label_1, \
                train_idx_2, test_idx_2, train_label_2, test_label_2

def train_test_mu(data_1, data_2, test_size=0.3):
    
    n_1 = len(data_1)
    n_2 = len(data_2) 
    
    train_idx_1, test_idx_1, train_label_1, test_label_1 \
        = train_test_split(np.arange(n_1), [1] * n_1, test_size=test_size) 
    train_idx_2, test_idx_2, train_label_2, test_label_2 \
        = train_test_split(np.arange(n_2), [0] * n_2, test_size=test_size)

    train_1 = data_1[train_idx_1]
    train_2 = data_2[train_idx_2]
    test_1 = data_1[test_idx_1]
    test_2 = data_2[test_idx_2]

    arr1 = np.arange(len(train_1)+len(train_2))
    I_1 = np.random.shuffle(arr1)

    arr2 = np.arange(len(test_1)+len(test_2))
    I_2 = np.random.shuffle(arr2)
    
    train = np.concatenate((train_1, train_2), 0)[arr1[I_1]]
    train_labels = np.concatenate((train_label_1, train_label_2), 0)[arr1[I_1]]
    test = np.concatenate((test_1, test_2), 0)[arr2[I_2]]
    test_labels = np.concatenate((test_label_1, test_label_2), 0)[arr2[I_2]]

    a = np.mean([np.mean(train_1[i], 0) for i in range(len(train_1))], 0)
    b = np.mean([np.mean(train_2[i], 0) for i in range(len(train_2))], 0)
    mu = max(abs(a-b))
    
    return mu, train[0], test[0], train_labels[0], test_labels[0]

def flatten(x):
    N = x.shape[0] 
    return x.view(N, -1)

class Flatten(nn.Module):
    def forward(self, x):
        return flatten(x)

"""## CNN for $v_Q$, $v_Q^{\varsigma}$, $v_Q^{exp}$, endpoints"""

def neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                                   learning_rate = 1e-3, num_trials_maj = 11,
                                   out_channels = 10, kernel_size = 10, 
                                   padding = 1, bias = True, Num_updates = 1000, 
                                   D_out = 2, epoch = 50, version = 'unsigned', 
                                   sigma = 1, test_size = 0.3):
    
    """ 
    in_channels: the dimension of hidden layer
    D_out: output dimension
    version: 'signed' or 'unsigned' or 'exp' 
    stride: should be fixed to 1
    """

    start_time = time.time()

    train_errors = np.zeros(epoch)
    test_errors = np.zeros(epoch)

    losses = torch.zeros(epoch, num_trials_maj, Num_updates)
    
    for s in range(epoch):

        mu, train, test, train_labels, test_labels = train_test_mu(data_1, data_2, test_size)

        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()
        
        x_preds = torch.zeros(num_trials_maj, len(train))
        y_preds = torch.zeros(num_trials_maj, len(test))

        Min = np.min([np.min(train[i], 0) for i in range(len(train))], 0)
        Max = np.max([np.max(train[i], 0) for i in range(len(train))], 0)
        Mean = np.mean([np.mean(train[i], 0) for i in range(len(train))], 0)
        Std = np.std([np.std(train[i], 0) for i in range(len(train))], 0)
        
        for t in range(num_trials_maj):
            Q = np.ones((Q_size, 2))
            Q[:,0] = np.random.normal(Mean[0], 4 * Std[0], Q_size)
            Q[:,1] = np.random.normal(Mean[1], 4 * Std[1], Q_size)

            if (version == 'unsigned' or version == 'signed'):
                train_data = curve2vec(Q, train, version = version, sigma = sigma)
                test_data = curve2vec(Q, test, version = version, sigma = sigma)
            elif version == 'exp':
                train_data = ExpCurve2Vec(Q, train, mu)
                test_data = ExpCurve2Vec(Q, test, mu)
            elif version == 'endpoints':
                train_data = get_endpoints(train)
                test_data = get_endpoints(test)
            
            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                                nn.ReLU(),
                                #nn.LeakyReLU(0.01),
                                #nn.Tanh(),
                                Flatten(),
                                nn.Linear(out_channels * (len(train_data[0]) - kernel_size + 1 + 2 * padding), 
                                            D_out)
                                )
    
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
            
            train_data = torch.from_numpy(train_data).float()
            test_data = torch.from_numpy(test_data).float()

            train_data = train_data.view(len(train_data), 1, len(train_data[0]))
            
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)
                losses[s, t, k] = loss
                    
                if (k+1) % 1000 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate

                optimizer.zero_grad()

                loss.backward() # Backward pass

                optimizer.step()  # Calling the step function on the Optimizer 
                
            x_preds[t] = torch.argmax(model(train_data), axis=1)
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))        
            y_preds[t] = torch.argmax(model(test_data), axis=1)
        
        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))
        
        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    plt.plot((torch.mean(losses, dim=(0,1))).detach().numpy())
    plt.show()

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
               np.round(train_error_mean, decimals=4), 
                np.round(test_error_mean, decimals=4),
                np.round(test_error_std, decimals=4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                columns=['Channel 1', 'Learning Rate', 'Train Error', 
                         'Test Error', 'Std Error'])
    
    print(colored(f"total time = {time.time() - start_time}", "red"))

    return pdf

"""## num_maj = 1

### Endpoints
"""

neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                               learning_rate = 1e-2, num_trials_maj = 1, 
                               out_channels = 10, kernel_size = 2, padding = 1, 
                               bias = True, Num_updates = 1000, D_out = 2, epoch = 50,
                               version = 'endpoints', sigma = 1, test_size = 0.3)

"""### Rand($v_Q$)"""

neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                               learning_rate = 2e-2, num_trials_maj = 1, 
                               out_channels = 10, kernel_size = 5, padding = 1, 
                               bias = True, Num_updates = 1000, D_out = 2, epoch = 50,
                               version = 'unsigned', sigma = 1, test_size = 0.3)

"""### Rand($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                               learning_rate = 2e-2, num_trials_maj = 1, 
                               out_channels = 10, kernel_size = 5, padding = 1, 
                               bias = True, Num_updates = 1000, D_out = 2, epoch = 50,
                               version = 'signed', sigma = 1, test_size = 0.3)

"""### Rand($v_Q^{exp}$)"""

neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                               learning_rate = 1e-2, num_trials_maj = 1, 
                               out_channels = 10, kernel_size = 5, padding = 1, 
                               bias = True, Num_updates = 1000, D_out = 2, epoch = 50,
                               version = 'exp', sigma = 1, test_size = 0.3)

"""## maj_num = 11

### Vote($v_Q$)
"""

neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                               learning_rate = 2e-2, num_trials_maj = 11, 
                               out_channels = 10, kernel_size = 5, padding = 1, 
                               bias = True, Num_updates = 500, D_out = 2, epoch = 50,
                               version = 'unsigned', sigma = 1, test_size = 0.3)

"""### Vote($v_Q^{\varsigma}$)"""

neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                               learning_rate = 2e-2, num_trials_maj = 11, 
                               out_channels = 10, kernel_size = 5, padding = 1, 
                               bias = True, Num_updates = 500, D_out = 2, epoch = 50,
                               version = 'signed', sigma = 1, test_size = 0.3)

"""### Vote($v_Q^{exp}$)"""

neuralNetworkClassificationCNN(data_1, data_2, Q_size = 20, lr_decay = 0.9, 
                               learning_rate = 1e-2, num_trials_maj = 11, 
                               out_channels = 10, kernel_size = 5, padding = 1, 
                               bias = True, Num_updates = 500, D_out = 2, epoch = 50,
                               version = 'exp', sigma = 1, test_size = 0.3)

"""# Neural Network for Mistake Driven Method of Choosing $Q$

## Initialize $Q$
"""

def initialize_Q(train_1, train_2, std_coeff, out_channels, kernel_size, Q_size, 
                 padding, learning_rate = 1e-3, bias = True, D_out=2, lr_decay = 0.9, 
                 Num_updates = 100): 
        
    Q = []
    errors = []
    losses = np.zeros((Q_size - kernel_size, Num_updates))
    
    mu = get_mu(train_1, train_2)
    std = mu * std_coeff

    trajectory_train_data = np.concatenate((train_1, train_2), axis = 0)
    train_labels = np.concatenate(([1] * len(train_1), [0] * len(train_2)), 0)
    train_labels = torch.from_numpy(train_labels).long()
    index = np.random.randint(0, high=len(trajectory_train_data)) 
    k = np.random.randint(0, high=len(trajectory_train_data[index]))
    for i in range(kernel_size):
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)

    for i in range(Q_size - kernel_size):
        train_data = ExpCurve2Vec(np.array(Q), trajectory_train_data, mu)
        model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                            out_channels = out_channels, 
                            kernel_size = kernel_size,
                            stride  = 1,
                            padding = padding,
                            bias = bias),
                nn.ReLU(),
                #nn.LeakyReLU(0.01),
                #nn.Tanh(),
                Flatten(),
                nn.Linear(out_channels * (len(train_data[0]) - kernel_size + 1 + 2 * padding), 
                            D_out)
                )
    
        loss_fn = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
        train_data = torch.from_numpy(train_data).float()
        train_data = train_data.view(len(train_data), 1, len(train_data[0]))
        
        for k in range(Num_updates):
            x_pred = model(train_data) # of shape (N, D_out)
            loss = loss_fn(x_pred, train_labels)
            losses[i, k] = loss
            
            if (k+1) % 100 == 0:
                optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
            optimizer.zero_grad()
            loss.backward() 
            optimizer.step() 
        
        train_pred = torch.argmax(model(train_data), axis=1)
        scores = model(train_data)
        I = np.where((train_labels == train_pred) == False)[0]

        temp_labels = 2 * train_labels.numpy().reshape(len(train_labels.numpy()), 1) - 1
        temp = temp_labels * scores.detach().numpy()
        temp = np.max(temp, axis=1)
        index = I[np.argmax(temp[I])]

        error = sum(train_labels != train_pred)/len(train_labels)
        errors.append(error.item())
        
        k = np.random.randint(0, high=len(trajectory_train_data[index]))
        q = trajectory_train_data[index][k] + np.random.normal(0, std, 2)
        Q.append(q)

    final_error = errors[-1]

    #plt.plot((np.mean(losses, axis=0)))
    #plt.show()

    return np.array(Q), np.array(errors), mu, final_error

def MD_NeuralNetworkClassificationCNN(data_1, data_2, maj_num, epoch, init_iter, 
                                      test_size, std_coeff, out_channels, kernel_size, 
                                      Q_size, padding, learning_rate = 1e-3, 
                                      bias = True, D_out=2, lr_decay = 0.9, 
                                      Num_updates = 100):
        
    start_time = time.time()

    train_errors = np.zeros(epoch) 
    test_errors = np.zeros(epoch)

    n_1 = len(data_1)
    n_2 = len(data_2) 

    for s in range(epoch):
        train_idx_1, test_idx_1, train_label_1, test_label_1, train_idx_2, \
        test_idx_2, train_label_2, test_label_2 = train_test(data_1, data_2, test_size)

        train = np.concatenate((data_1[train_idx_1], data_2[train_idx_2]), 0)
        test = np.concatenate((data_1[test_idx_1], data_2[test_idx_2]), 0)
        train_labels = np.concatenate((train_label_1, train_label_2), axis = 0)
        test_labels = np.concatenate((test_label_1, test_label_2), axis = 0)
        train_labels = torch.from_numpy(train_labels).long()
        test_labels = torch.from_numpy(test_labels).long()

        x_preds = torch.zeros((maj_num, len(train)))
        y_preds = torch.zeros((maj_num, len(test)))
        
        I = np.arange(len(train))
        np.random.shuffle(I)
        train = train[I]
        train_labels = train_labels[I]
        
        J = np.arange(len(test))
        np.random.shuffle(J)
        test = test[J]
        test_labels = test_labels[J]

        for t in range(maj_num):

            Q_list = []
            temp_errors = []
            mu_temp = []

            for j in range(init_iter):
                B = initialize_Q(data_1[train_idx_1], data_2[train_idx_2], 
                                 std_coeff, out_channels, kernel_size, Q_size, 
                                 padding, learning_rate = 1e-3, bias = True, 
                                 D_out=2, lr_decay = 0.9, Num_updates = 100)

                Q_list.append(B[0])
                mu_temp.append(B[2])
                temp_errors.append(B[-1])

            h = np.argmin(temp_errors)
            Q = Q_list[h]
            mu = mu_temp[h]

            train_data = torch.from_numpy(ExpCurve2Vec(Q, train, mu)).float()
            train_data = train_data.view(len(train_data), 1, len(train_data[0]))

            test_data = torch.from_numpy(ExpCurve2Vec(Q, test, mu)).float()
            test_data = test_data.view(len(test_data), 1, len(test_data[0]))

            model = nn.Sequential(nn.Conv1d(in_channels = 1, 
                                    out_channels = out_channels, 
                                    kernel_size = kernel_size,
                                    stride  = 1,
                                    padding = padding,
                                    bias = bias),
                    nn.ReLU(),
                    #nn.LeakyReLU(0.01),
                    #nn.Tanh(),
                    Flatten(),
                    nn.Linear(out_channels * (len(train_data[0][0]) - kernel_size + 1 + 2 * padding), 
                              D_out)
                    )
        
            loss_fn = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
        
            for k in range(Num_updates):
                x_pred = model(train_data) # of shape (N, D_out)
                loss = loss_fn(x_pred, train_labels)

                if (k+1) % 10 == 0:
                    optimizer.param_groups[0]['lr'] = lr_decay * learning_rate
                optimizer.zero_grad()
                loss.backward() 
                optimizer.step() 

            scores = model(train_data)
            
            x_preds[t] = torch.argmax(scores, axis=1)
            y_preds[t] = torch.argmax(model(test_data), axis=1)

        x_preds = torch.tensor(find_majority_array(x_preds))
        y_preds = torch.tensor(find_majority_array(y_preds))

        train_errors[s] = sum(abs(train_labels - x_preds))/len(train_labels)
        test_errors[s] = sum(abs(test_labels - y_preds))/len(test_labels)

    train_error_mean = np.mean(train_errors)
    test_error_mean = np.mean(test_errors)
    test_error_std = np.std(test_errors)

    Dict = {"CNN 1-Layer": [out_channels, learning_rate, 
                            np.round(train_error_mean, decimals = 4), 
                            np.round(test_error_mean, decimals = 4),
                            np.round(test_error_std, decimals = 4)]}

    pdf = pd.DataFrame.from_dict(Dict, orient='index', 
                                 columns=['Channel 1', 'Learning Rate', 
                                          'Train Error', 'Test Error', 'Std Error'])

    print(colored(f"total time = {time.time() - start_time}", "red"))
    print("mu =", mu)

    return pdf

"""## num_maj = 1"""

MD_NeuralNetworkClassificationCNN(data_1, data_2, maj_num = 1, epoch = 50, init_iter = 3,
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 20, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 1000)

"""## num_maj = 11"""

MD_NeuralNetworkClassificationCNN(data_1, data_2, maj_num = 11, epoch=50, init_iter=3, 
                                  test_size=0.3, std_coeff=1, out_channels=10, 
                                  kernel_size = 5, Q_size = 20, padding = 1, 
                                  learning_rate = 1e-2, bias = True, D_out=2, 
                                  lr_decay = 0.9, Num_updates = 500)

"""# Plot test errors"""

A_LSVM = [0.1573, 0.1716, 0.3733, 0.0887, 0.1993, 0.1960, 0.3991, 0.2649, 0.4013]
A_GSVM = [0.0464, 0.0675, 0.1142, 0.1384, 0.0682, 0.0851, 0.1555, 0.1443, 0.0697]
A_PSVM = [0.0530, 0.3109, 0.2836, 0.2119, 0.0658, 0.3219, 0.3143, 0.2158, 0.2075]
A_DT = [0.0888, 0.0678, 0.1027, 0.0781, 0.1266, 0.1128, 0.1503, 0.1257, 0.0967]
A_RF = [0.0578, 0.0536, 0.0875, 0.0622, 0.0752, 0.0660, 0.0975, 0.0918, 0.0649]
A_KNN = [0.0733, 0.0858, 0.2175, 0.0888, 0.1069, 0.0861, 0.2488, 0.1519, 0.0916]
A_LR = [0.2207, 0.4452, 0.4324, 0.2321, 0.2673, 0.4252, 0.4182, 0.2864, 0.5096]
A_CNN = [0.1370, 0.0906, 0.1918, 0.1549, 0.1243, 0.0767, 0.1809, 0.1507, 0.4949]

A = np.array([A_LSVM, A_GSVM, A_PSVM, A_DT, A_RF, A_KNN, A_LR, A_CNN])
B = A.T

std_LSVM = [0.0435, 0.0252, 0.0423, 0.0395, 0.0390, 0.0418, 0.0433, 0.0567, 0.0378]
std_GSVM = [0.0186, 0.0224, 0.0300, 0.0388, 0.0338, 0.0237, 0.0303, 0.0752, 0.0229]
std_PSVM = [0.0323, 0.1131, 0.0379, 0.0555, 0.0388, 0.1057, 0.0456, 0.0789, 0.0422]
std_DT = [0.0237, 0.0196, 0.0251, 0.0230, 0.0300, 0.0237, 0.0312, 0.0500, 0.0278]
std_RF = [0.0225, 0.0186, 0.0222, 0.0213, 0.0277, 0.0207, 0.0263, 0.0483, 0.0224]
std_KNN = [0.0292, 0.0271, 0.0318, 0.0276, 0.0398, 0.0231, 0.0448, 0.0556, 0.0231]
std_LR = [0.0365, 0.0522, 0.0398, 0.0386, 0.0523, 0.0482, 0.0452, 0.0628, 0.0312]
std_CNN = [0.0692, 0.0287, 0.0359, 0.0561, 0.0664, 0.0299, 0.0331, 0.0743, 0.0075]

C = np.array([std_LSVM, std_GSVM, std_PSVM, std_DT, std_RF, std_KNN, std_LR, std_CNN]).T

# for KNN
errors = [0.0949, 0.1037, 0.0903, 0.1130, 0.1240, 0.1131, 0.3013, 0.0828, 0.2084, 
          0.1652, 0.2891, 0.1070]

stds = [0.0344, 0.0329, 0.0306, 0.0360, 0.0315, 0.0283, 0.0508, 0.0256, 0.0448, 
        0.0332, 0.0732, 0.0271]

classifiers = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

FM = (r'KNN', r'Vote(MD $v_Q^{exp}$)', r'Vote(Rand $v_Q$)', r'Vote($v_Q^{\varsigma}$)', 
      r'Vote(Rand $v_Q^{exp}$)', r'MD $v_Q^{exp}$', r'Rand $v_Q$', 
      r'Rand $v_Q^{\varsigma}$', r'Rand $v_Q^{exp}$', 'Endpoints')

labels = ['LSVM', 'GSVM', 'PSVM', 'DT', 'RF', 'KNN', 'LR', 'CNN']

dists = [r'$d_F$', r'$d_{dF}$', r'$dH$', r'DTW', r'soft-dtw', r'fastdtw', 
         r'LCSS', r'SSPD', r'EDR', r'ERP', r'LSH', r'$d_Q^{\pi}$']

def lower_limit_error(x, y):
    if y - x < 0:
        return y
    else:
        return x

lower_limit_error = np.vectorize(lower_limit_error)

width=0.1
index = np.arange(len(stds))
ind = np.arange(len(A)) + len(index) * 0.25

plt.subplots(figsize = (18, 5.5), tight_layout=True)
bars = [0] * 10

lower_lim = lower_limit_error(stds, errors)
bars[0] = plt.bar(index * 0.2, errors, width, yerr=[lower_lim, stds], capsize=2)

for i in range(len(FM)-1):
    lower_limit = lower_limit_error(C[i], B[i])
    bars[i+1] = plt.bar(ind+width*i-0.3, B[i], width, yerr=[lower_limit, C[i]], capsize=2)

plt.title('Simulated Car-Bus Data', fontsize = 20)
plt.xticks(list(np.arange(len(stds)) * 0.2) + list(ind + 0.5 * width), 
           dists + labels, fontsize = 14)

plt.legend(tuple(bars), FM, loc=0, fontsize = 12)

plt.gca().yaxis.grid(color='gray', linestyle='dotted', linewidth=0.6)
plt.xticks(rotation='vertical', fontsize = 14)
plt.yticks(fontsize = 14)

path = '/content/gdrive/My Drive/Colab Notebooks/plots/plots simulated car-bus/simulated car-bus bar chart all horizontal.png'

plt.savefig(path, bbox_inches='tight', dpi=200)

plt.show()

